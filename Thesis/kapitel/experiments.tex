% experiments.tex

\chapter{Experiments}
\label{chapter:ch5}
Empirical evaluation is necessary to verify the theoretical properties of the aggregation mechanics and sampling methods.
In this chapter we will discuss the experimental setup, talk about the data sets used and will evaluate the empirical results obtained from applying the aggregation techniques.

We are specifically going to investigate the following questions:
\begin{itemize}
    \item Do local models benefit from aggregation?
    \item Does any aggregation method outperform the others? 
    \item Do additional parameter samples affect the aggregate?
    \item How do aggregates perform when compared to the baseline?
\end{itemize}
First, we compared the performance of local models, aggregates, and baseline based on likelihood and performance. 
Moreover, we continuously monitored the performance of local and aggregate model to record accuracy and likelihood in every round, where we increased the amount of data available each round.
We ran experiments with five different aggregation methods and three covariance matrix sampling variants, while also testing the aggregation without additional sampling.
Additionally, models where trained either without regularization or with $l2$ regularization.
Operations that involve random events such as sampling or index shuffling were performed using a seeded randomizer to improve reproducibility.

Directly comparing our results is difficult as not much work specifically on aggregating canonical exponential family models has been done. Han and Liu\cite{han2016bootstrap} evaluate aggregation on gaussian mixture models, while Kamp et. al. \cite{kamp2017effective} consider classifier ensembles.
Moreover, the metrics used to compare and evaluate results vary greatly with accuracy, likelihood AUC/ROC measures or mean squared errors all being used to evaluate model performance.
Here, we compare our results to integer exponential family models investiaged by Piatkowsiks~\cite{piatkowski2018exponential}obtained by computing the arithmetic mean. 
Integer models restrict the solution space to being integer only, which is then an approximation of the real-valued solution.

We start by introducing  the experimental setup in  \sect~\ref{sec:setup}, where we provide an overview of the data sets used.
We then present the experiment framework and evaluation metrics in \sect~\ref{sec:experiments} and \sect~\ref{sec:evaluation}.
Finally, in \sect~\ref{sec:results}, we present and evaluate the results.

\section{Data}
\label{sec:setup}

We used three benchmark datasets from the UCI machine learning repository: SUSY~\cite{baldi2014searching}, Covertype~\cite{blackard2000comparison} and Dota2~\cite{tridgell2016dota2} to evaluate the aggregation methods.


We discretize the features into their $q=10$ quantiles to obtain comparable results.
Each feature that contains more that $q$ unique values into its $q=10$ quantiles, all other features are not discretized.
Ideally, we determine the interval ranges and number of buckets prior to the discretization, where the intervals are then based on the total range of each feature.
However, this is usually not the case, i.e., newly observed data from some feature $\mathcal{D}_{\cdot i}$ may contain values outside of the feature range observed in the training data. 
We then observe values exceeding the original range, that is, $\sup(\hat{\vect{X}}_i) > \sup(\vect{X}_i) $ or  $\inf(\hat{\vect{X}}_i) < \inf(\vect{X}_i) $.
In such a case we add two additional bins, where samples outside the original range are assigned to.
Samples above or below the original range of $\mathcal{D}_{\cdot i}$ are then  mapped to buckets $0$ or $q+1$ respectively.

Additionally we used cross-validation for the global models and for the distributed learners we partition the training portion of each split further in $k$ equal sized splits, where $k$ is the number of learners.
While increasing or decreasing the number of local models is always possible, again we chose this according to the experiments already conducted by Piatkowski.

The independence structures are estimated the Chow-Liu algorithm and can be found in \sect~\ref{sec:apdx:struc}.
Since we reestimate the structure for each experiment the structure varies slightly.
The structures were generated on a holdout set of $n=10000$ sampled prior to creating the cross-validation splits.
The holdout set was discretized separately , based on the ranges observed in the holdout set only.
We then discretized the training data and based on the training data quantiles, the test data.

We used three real-world data sets for the experiments and evaluation. 
Each task associated with the data set is either a binary or multi-label classification task.

\paragraph*{Dota2}
The Dota2 data set contains game results of roughly $n=100.000$ matches from the team-based video game Dota2, where five players on each side each pick one character from a growing pool of characters (114 at the time of recording the data).
Each character can only be chosen by one team and only once.
The data set contains one feature for each character, the game type, region id, and which of the result.
Each character feature can take one of three values $\{-1,0,1\}$, where zero indicates that the character was not picked and -1, 1 that the character was picked by either team.
The match result is a binary variable as there are no draws.
We can then predict, which of the two teams won or for example choose a character based on already chosen ones, that best fits the team composition.

\paragraph*{Covertype}
The Covertype data set contains cartographic features for  $n=581012$ forest cells of size $30m^2$.
Here, the task is to determine one out of 7 possible forest cover types given a set of cartographic features such as elevation, slope or shade. 
Some of these features are binary and thus need not to be discretized, while others such as elevation are discretized into 10 quantiles.
This is a standard multi-label classification task with unbalanced classes.
Two classes appear with higher frequency, while the other classes are rarely observed.

\paragraph*{SUSY}
The SUSY data set contains eight kinematic properties measured by particle detectors in a particle accelerator. 
The features were obtained using Monte Carlo simulations, while the additional ten features are high level features derived from the original measurements.
Here, we aim to distinguish between two signal processes one of which does produce supersymmetric particles, while the other does not. 
Thus, the task is a binary classification task containing $n=5000000$ already labeled samples.

\section{Experiments}
\label{sec:experiments}
We employ a set of parameters in order to control and evaluate experiments in different settings.
For each experiment we employ all aggregation methods (if possible) and additionally use one type of covariance matrix to sample more parameter vectors. 
This lets us effectively simulate more models, while also eliminating the need to train additional models, which is also useful when there is not much data to work with.

We train models in a cross-validation fashion with $c = 10$ splits on $k=10$ distributed learners.
Each cross-validation training split is further divided into $k$ splits and made available to a learner.
Then, we train each learner on a subset of the split, increasing the split size every round to simulate local data collection.
Once the maximum number of iterations(i=5000) has been reached or the stopping criterion (\eq~\ref{eq:stopcrit}) is satisfied we terminate the training and send the local parameters to the coordinator.

\subsection{Aggregators}
\subsection{Parameter Sampling}
We sample additional parameter vectors such that we are able to employ the radon machines with $h=1$, which requires $(d+2)$ models.
From each local model $m^i$ we sample $\lceil d+2/k \rceil$ parameter vectors., which are then used for all aggregators.
Radon machines are to used in case we do not sample additional parameter vectors.

\subsection{Evaluation}
\label{sec:evaluation}
We evaluate the different aggregation and sampling methods by comparing likelihood, accuracy and f1-score in terms of the training and test data. 
First we assess the general quality of the aggregate by computing the likelihood on the full training data $\mathcal{D}$.
\begin{equation}
    \begin{split}
        \ell(\vect{\theta} ; \mathcal{D}) &= -\frac{1}{\abs{\mathcal{D}}} \sum_{\vect{x} \in \mathcal{D}} \inner{\phi(\vect{x})}{\vect{\theta}} - A(\vect{\theta}) \\    
        &= A(\vect{\theta}) - \inner{\vect{\mu}}{\vect{\theta}}
    \end{split}
\end{equation}
Additionally, we compare the aggregate likelihood with the local 
\paragraph*{Relative Log-Likelihood}
We measure the likelihood in terms of a quality function, which is the difference between the global model $\vect{\theta}^*$, which has had access to the full data set and any other local or aggregate model on the full data. 
Since the likelihood is convex the global model is a lower bound for the likelihood, i.e., $\ell(\vect{\theta}^* ; \mathcal{D}) \leq   \ell(\vect{\theta} ; \mathcal{D})$ without additional sampling.
The likelihood is convex, thus for 
\begin{equation}
    \frac{\partial}{\partial \vect{\theta^*}} \ell(\vect{\theta}^* ; \mathcal{D})  = 0,
\end{equation}
the parameters $\vect{\theta}^*$ are optmial \wrt $\mathcal{D}$ and for any other $\vect{\theta} \neq \vect{\theta}^*$ we have 

\begin{equation}
    \frac{\partial}{\partial \vect{\theta}} \ell(\vect{\theta} ; \mathcal{D})  > 0,
\end{equation}
and thus obtaining a distance function (\eq~\ref{eq:regret})
\begin{equation}
d_{\ell}(\vect{\theta}, \vect{\theta}^*; \mathcal{D}) = \ell(\vect{\theta} ; \mathcal{D}) -    \ell(\vect{\theta}^* ; \mathcal{D})
\end{equation}

\paragraph*{Accuracy}
Classification tasks require the prediction of a label from a given set of possible outcomes given some input data. The data is usually divided into features and labels $\mathcal{D} = \{\vect{\mathcal{X}, \vect{\mathcal{Y}}}\} \subset \mathbb{R}^m \times \mathbb{N}$ , with features $\vect{\mathcal{X}}$ and labels or classes $\vect{\mathcal{Y}}$.

Predictions with probabilistic graphical models are realized by conditional probabilities, i.e.
\begin{equation}
    \hat{y} = \arg\max_{y \in \mathcal{Y}} \mathbb{P}_{\vect{\theta}}(\vect{Y} = y \lvert \vect{X} = \vect{x}) ,
\end{equation}
where the class $y$ with the highest probability condition on the observed $\vect{x}$ is chosen.

The accuracy is then just the quotient of correct predictions and the cardinality of the true labes $\vect{y}^* \in \mathbb{R}^n$

\begin{equation}
    d_{acc}(\vect{\hat{y}}, \vect{y}^*) = \frac{\abs{\{\hat{y}_i \lvert \hat{y}_i = y^*_i, 0 \leq i \leq n\}}}{n}, 
\end{equation}
which provides the ratio of successfully predicted labels. 

\paragraph*{F1-Score}
Classification tasks, especially for multi-label classification, might contain a heavily unbalanced class distribution. 
Such problems often arise in biology\cite{sonnenburg2007accurate} and even though classifiers tend to achieve high accuracy the actual performance is considered subpar.

Scores, such as the F1-Score can take class inbalance into account by computing the weighted average based on class frequencyin order to provide a better measure of performance compared to accuracy alone.
The F1-Score \wrt to some label $i$ is obtained via 
\begin{equation}
    f1(\hat{\vect{y}},\vect{y}^*)_i = \frac{\text{precision}(\hat{\vect{y}},\vect{y}^*)_i \cdot \text{recall}(\hat{\vect{y}},\vect{y}^*)_i}{\text{precision}(\hat{\vect{y}},\vect{y}^*)_i + \text{recall}(\hat{\vect{y}},\vect{y}^*)_i},
\end{equation}
where precision and recall are measures based on true and false positive rate as well as true and false negative rate.\cite{friedman2001elements} \cite{hossin2015review}
The F1-Score is the weighted average between precision and recall and in a multi-class scenario it can be computed as the arithmetic mean for each label
\begin{equation}
    d_{f1}(\hat{\vect{y}},\vect{y}^*) = \frac{1}{N} \sum_{i=1}^N f1_i(\hat{\vect{y}},\vect{y}^*)
\end{equation}


to achieve this we evaluate:

\begin{itemize}
    \item Likelihood of the local and aggregated weight vectors when plugged into the baseline model 
    \item Performance scores such as accuracy, f1-score, which is especially useful for unbalanced classes 
    \item Other stuff 
\end{itemize}
\section{Discussion}
\label{sec:results}
\begin{table}
    \centering
    \caption{Relative Likelihood when compared to the baseline likelihood on the test split. This table shows the results for covertype using  fish sampling with epsilon  0.1 and  default regularization.}
    \label{tab:0}
    \begin{tabular}{|l||r|rrr|rr|}
    \toprule
    {} &  Local Average &  Average &  RadonMachine &  LL-Weighted &  Bootstrap &  Acc. Weighted \\
    n\_data &                &         &               &              &            &                \\
    \midrule
    9      &          65.63 &      --- &           --- &          --- &      54.91 &          28.23 \\
    33     &          48.57 &   714.98 &        854.45 &          --- &      20.18 &          20.11 \\
    73     &          29.05 &   109.37 &        155.06 &       325.66 &       8.32 &          13.58 \\
    130    &          16.33 &    72.31 &        106.21 &       151.95 &       3.64 &           8.04 \\
    203    &           9.25 &    25.15 &         43.97 &        36.82 &       1.96 &           4.52 \\
    292    &           5.19 &    20.46 &         24.78 &        55.31 &       1.49 &           2.49 \\
    397    &           3.19 &     5.02 &          6.92 &        11.00 &       1.69 &           1.49 \\
    519    &           2.01 &     4.53 &         15.58 &        13.80 &       1.60 &           0.88 \\
    657    &           1.37 &     1.45 &          2.08 &         2.11 &       1.03 &           0.57 \\
    810    &           1.02 &     0.98 &          3.44 &         1.93 &       1.08 &           0.41 \\
    981    &           0.78 &     1.57 &          3.47 &         3.15 &       1.13 &           0.31 \\
    1167   &           0.62 &     0.25 &          1.68 &         0.29 &       0.78 &           0.23 \\
    1369   &           0.53 &     0.21 &          0.22 &         0.21 &       0.62 &           0.20 \\
    1588   &           0.43 &     0.17 &          0.17 &         0.17 &       0.56 &           0.16 \\
    1823   &           0.37 &     0.14 &          0.15 &         0.14 &       0.53 &           0.14 \\
    \bottomrule
    \end{tabular}
    \end{table}
    
    \begin{figure}
        \center
        \begin{tikzpicture}
            \clip (0,0) rectangle (17.0,15.0);
            \Vertex[x=12.637,y=6.951,size=0.5,color=tuorange,opacity=0.7]{0}
            \Vertex[x=13.224,y=8.670,size=0.5,color=tugreen,opacity=0.7]{3}
            \Vertex[x=14.502,y=6.725,size=0.5,color=tugreen,opacity=0.7]{5}
            \Vertex[x=12.634,y=8.090,size=0.5,color=tugreen,opacity=0.7]{9}
            \Vertex[x=10.487,y=5.452,size=0.5,color=tublue,opacity=0.7]{54}
            \Vertex[x=8.715,y=11.565,size=0.5,color=tugreen,opacity=0.7]{1}
            \Vertex[x=7.132,y=12.694,size=0.5,color=tuorange,opacity=0.7]{6}
            \Vertex[x=10.448,y=11.497,size=0.5,color=tuorange,opacity=0.7]{7}
            \Vertex[x=8.610,y=10.130,size=0.5,color=tuorange,opacity=0.7]{8}
            \Vertex[x=12.001,y=11.238,size=0.5,color=tugreen,opacity=0.7]{2}
            \Vertex[x=13.029,y=10.213,size=0.5,color=tuorange,opacity=0.7]{4}
            \Vertex[x=15.279,y=6.087,size=0.5,color=tuorange,opacity=0.7]{14}
            \Vertex[x=15.747,y=6.864,size=0.5,color=tuorange,opacity=0.7]{15}
            \Vertex[x=5.924,y=13.037,size=0.5,color=tugreen,opacity=0.7]{10}
            \Vertex[x=7.538,y=13.901,size=0.5,color=tugreen,opacity=0.7]{11}
            \Vertex[x=6.827,y=14.000,size=0.5,color=tugreen,opacity=0.7]{12}
            \Vertex[x=5.999,y=11.852,size=0.5,color=tugreen,opacity=0.7]{13}
            \Vertex[x=6.440,y=12.705,size=0.5,color=tugreen,opacity=0.7]{16}
            \Vertex[x=5.108,y=11.775,size=0.5,color=tuorange,opacity=0.7]{43}
            \Vertex[x=10.085,y=7.026,size=0.5,color=tuorange,opacity=0.7]{17}
            \Vertex[x=2.368,y=11.619,size=0.5,color=tuorange,opacity=0.7]{18}
            \Vertex[x=1.253,y=12.024,size=0.5,color=tugreen,opacity=0.7]{19}
            \Vertex[x=1.757,y=12.689,size=0.5,color=tugreen,opacity=0.7]{20}
            \Vertex[x=3.959,y=10.329,size=0.5,color=tugreen,opacity=0.7]{27}
            \Vertex[x=10.642,y=6.917,size=0.5,color=tuorange,opacity=0.7]{21}
            \Vertex[x=5.790,y=4.399,size=0.5,color=tuorange,opacity=0.7]{22}
            \Vertex[x=6.961,y=3.509,size=0.5,color=tugreen,opacity=0.7]{32}
            \Vertex[x=8.659,y=2.745,size=0.5,color=tuorange,opacity=0.7]{23}
            \Vertex[x=6.917,y=5.374,size=0.5,color=tugreen,opacity=0.7]{33}
            \Vertex[x=8.551,y=1.000,size=0.5,color=tugreen,opacity=0.7]{34}
            \Vertex[x=9.886,y=3.161,size=0.5,color=tugreen,opacity=0.7]{35}
            \Vertex[x=7.809,y=2.199,size=0.5,color=tugreen,opacity=0.7]{36}
            \Vertex[x=7.463,y=2.745,size=0.5,color=tugreen,opacity=0.7]{37}
            \Vertex[x=9.674,y=1.333,size=0.5,color=tugreen,opacity=0.7]{38}
            \Vertex[x=10.054,y=1.846,size=0.5,color=tugreen,opacity=0.7]{39}
            \Vertex[x=9.150,y=1.055,size=0.5,color=tugreen,opacity=0.7]{40}
            \Vertex[x=9.626,y=2.552,size=0.5,color=tugreen,opacity=0.7]{41}
            \Vertex[x=7.220,y=2.063,size=0.5,color=tugreen,opacity=0.7]{45}
            \Vertex[x=7.524,y=1.524,size=0.5,color=tugreen,opacity=0.7]{46}
            \Vertex[x=10.202,y=2.467,size=0.5,color=tugreen,opacity=0.7]{47}
            \Vertex[x=9.216,y=3.550,size=0.5,color=tugreen,opacity=0.7]{48}
            \Vertex[x=7.974,y=1.135,size=0.5,color=tugreen,opacity=0.7]{49}
            \Vertex[x=9.432,y=1.871,size=0.5,color=tugreen,opacity=0.7]{50}
            \Vertex[x=8.865,y=1.565,size=0.5,color=tugreen,opacity=0.7]{51}
            \Vertex[x=8.226,y=1.652,size=0.5,color=tugreen,opacity=0.7]{52}
            \Vertex[x=8.174,y=3.472,size=0.5,color=tugreen,opacity=0.7]{53}
            \Vertex[x=5.308,y=8.004,size=0.5,color=tuorange,opacity=0.7]{24}
            \Vertex[x=6.057,y=8.782,size=0.5,color=tugreen,opacity=0.7]{25}
            \Vertex[x=6.176,y=7.988,size=0.5,color=tugreen,opacity=0.7]{26}
            \Vertex[x=4.566,y=8.365,size=0.5,color=tugreen,opacity=0.7]{28}
            \Vertex[x=4.210,y=7.479,size=0.5,color=tugreen,opacity=0.7]{29}
            \Vertex[x=4.782,y=8.969,size=0.5,color=tugreen,opacity=0.7]{30}
            \Vertex[x=5.359,y=8.909,size=0.5,color=tugreen,opacity=0.7]{31}
            \Vertex[x=3.691,y=9.682,size=0.5,color=tuorange,opacity=0.7]{42}
            \Vertex[x=2.783,y=10.701,size=0.5,color=tuorange,opacity=0.7]{44}
            \Edge[](0)(3)
            \Edge[](0)(5)
            \Edge[](0)(9)
            \Edge[](0)(54)
            \Edge[](3)(4)
            \Edge[](5)(14)
            \Edge[](5)(15)
            \Edge[](54)(17)
            \Edge[](54)(21)
            \Edge[](54)(23)
            \Edge[](1)(6)
            \Edge[](1)(7)
            \Edge[](1)(8)
            \Edge[](6)(10)
            \Edge[](6)(11)
            \Edge[](6)(12)
            \Edge[](6)(13)
            \Edge[](7)(2)
            \Edge[](2)(4)
            \Edge[](16)(43)
            \Edge[](43)(27)
            \Edge[](18)(19)
            \Edge[](18)(20)
            \Edge[](18)(27)
            \Edge[](27)(24)
            \Edge[](27)(42)
            \Edge[](27)(44)
            \Edge[](22)(32)
            \Edge[](32)(23)
            \Edge[](23)(33)
            \Edge[](23)(34)
            \Edge[](23)(35)
            \Edge[](23)(36)
            \Edge[](23)(37)
            \Edge[](23)(38)
            \Edge[](23)(39)
            \Edge[](23)(40)
            \Edge[](23)(41)
            \Edge[](23)(45)
            \Edge[](23)(46)
            \Edge[](23)(47)
            \Edge[](23)(48)
            \Edge[](23)(49)
            \Edge[](23)(50)
            \Edge[](23)(51)
            \Edge[](23)(52)
            \Edge[](23)(53)
            \Edge[](33)(24)
            \Edge[](24)(25)
            \Edge[](24)(26)
            \Edge[](24)(28)
            \Edge[](24)(29)
            \Edge[](24)(30)
            \Edge[](24)(31)
            \end{tikzpicture}
            \caption[test]{test}
            \label{fig:test}
    \end{figure}