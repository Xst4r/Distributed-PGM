% einleitung.tex
\chapter{Introduction}
With an increasing number of devices available and data collected on these devices, the task of machine learning becomes increasingly challenging.
Not only do we have to consider machine learning tasks in a distributed environment, we now also have to consider communication and network structure.
While the best case, in terms of a machine learning, remains having all observed data available at some central coordinator, this may not be guaranteed for all applications.
Hence, creating a single model, which has access to all data may not be feasible anymore, which requires us to bring machine learning to the edge, that is , onto individual devices in a distributed fashion.

Leading factors, that make this a necessity are the communication cost incurred by sending the data to a central node.
While new technologies in the communication sector with an ever increasing bandwith , such as 5G with up to 10GBit/s \cite{nordrum2017ieee}, are still being developed and deployed around the globe, it is still not possible to make use of this technology in all applications. 
Furthermore, attempting to communicate all data for each task may still be too much as projects like IceCube\todo{Cite} produce a total of x TB data each day.
Additionally, user privacy is a major concern in a setting where we learn from data on user devices, as the data we use for learning tasks may contain sensitive information.
While some users may allow us to retrieve their data, this is not generally the case and as such it is advantegeous to treat the data as fixed on each device.   

However, even anonymized data can be profiled and tracked back to certain users, which requires us to be even more careful about the data that is indeed communicated between devices.
De-anonymization schemes have been well studied in terms of user data by Narayanan et al. \cite{narayanan2008robust} or in terms of geolocation(GPS) data by Gambs et al. \cite{gambs2014anonymization}.
The need for privacy leads to another, directly related challenge, which is the limited amount of memory and cpu power available on each device.
Moreover, we do not aim to strain the hardware of users too much as this would impair the devices' functionality as well as strain battery life in case of mobile devices such as smartphones.

This work deals with statistical models, more specifically exponential families, on distributed devices as these have a few advantages, that can be exploited in order to reduce the resource footprint.
Briefly, we are looking for a probability distribution $\mathbb{P}_{\vect{\theta}}$ conditioned on some parameters $\vect{\theta}$ that best describes the observed data $\mathcal{D}$ on each device.
The data is assumed to be collected on each device and that it cannot be exchanged in any way due to reasons such as privacy preservation.
Usually we estimate the parameters by optimizing a function such as the likelihood.
After finding a suitable distribution and the desired model parameters we can then perform generative tasks on unknown data, which includes sampling new data, completing missing data on partially observed samples or classification, which is a special case of the former, since only one observation is missing.

All in all there are several limitations in a distributed setting, which requires us to choose our methods and machine learning algorithms carefully in order to keep our footprint as small as possible, while trying to obtain the best possible results.


\section{Motivation and Related Work}

%Introduction, ensembling vs aggregation
These restrictions give rise to the formulation of machine learning on the edge. 
Our goal is to directly apply the same machine learning task on each device in order to create a set of local experts based on the individually observed data.
Afterwards, these experts would be grouped or combined, obtaining an overall better performing expert 
that has had access to all distributed devices.
Approaches for combining models can be roughly separated into two categories, ensembling and aggregation. 
Ensembles usually predict each newly observed sample individually, while aggregates predict only once.
The final prediction is then for example decided by majority vote of all local predictors.
This procedure is indirectly encoded into aggregates, as instead of combining the prediction we combine the models instead, obtaining a single model and thus a single prediction.

% Distributed vs Federated Intro
In a distributed environment we first have to consider where and how the data is collected. 
If the data is collected a central node, we can employ the typical distributed approaches, i.e., dividing the data into chunks and sending each chunk onto a single device.
Each device then would perform the same operation on a subset of the data and report the results to the coordinator.
However, the data may also be collected on an array of devices, i.e., the data itself is distributed as well.
Then, we either have the option to simply collect the data from each device onto a single coordinator or server, or to leave the data on the devices. 
Hence, the machine learning task may have to be moved towards the data.

%Federated Learning
While many applications allow for data to be stored on some central system, some applications especially user devices or resource constrained systems, require a more strict handling of communicating data.
This can either be due to privacy concerns, storage limitations or simply bandwidth restrictions.
Especially in wirless networks such as wireless sensor networks, communicating a large amount of data would be prohibitive to the task.

%TODO Federated vs. Centralized

% Problem Definition
Briefly, given a solution space $\Theta$ containing all feaisble solutions to some machine learning task, we are trying to find a solution $\bar{\theta} \in \Theta$, from a set of local solutions $\Psi = \{\theta^{(1)}, \ldots \theta^{(n)}\} \subset \Theta$ on n devices.
In this case we want to find a suitable probability distribution $\mathbb{P}_{\vect{\theta}}$ that is optimal in $\vect{\theta}$ \wrt an optimziation criterium such as the likelihood.

%Todo Why Exp Families, Statistical Models
%Just Notes
Communication between devices is inefficient.
Local Models usually have a large variance between each other on the same data/distribution.
Two staple methods, ensembling and aggregation.
Ensembling aims to decide the label of a class by accounting for each decision on the local models e.g. majority vote.
Aggregation aims to aggregate the approximated function, i.e., its parameters to create a new aggregated model, which then predicts the labels.
After aggregation we only need to predict once and not on every local model.
Models introduce non-linearity, which in turn might introduce an error in the aggregation as e.g. averaging is a linear operation.
Model aggregation has been studied by several people.
Aggregation requires communication between local models in a non-centralized approach or communication between local nodes and a coordinator (Federated learning).

Several problems are associated with distributed learning:
Amount of communication required.
Stopping criteria for local models, i.e., when are models close enough to stop training and this includes the question of sample complexity.
Model aggregation itself.

%Related Work
Distributed Learning and Model Aggregation is a large research area that has been a subject to a variety of reserach.
Applications such as \todo{Application A}, \todo{Application B} or \todo{Application C} arise frequently in different areas of reserach and industry. 

Therefore studying different tasks associated with distributed learning such as model aggregation, thresholding and communication optimization is vital for improving and advancing the this research area.

Wolff \cite{wolff2013local} has proposed a way to approach thresholding in distributed system for a network without structural restrictions.
The proposed algorithm passes the local information about some defined threshold to neighbouring nodes, which is then propagated thorugh the network structure. Nodes/Devices that are within the defined threshold stop sending messages until they receive new messages from neighbours that would lead to the local information not satisfying the conditions.
This approach can be used to monitor local conditions such as sample size or distances between models in order to find local models that do not satisfy the threshold. These would then continue sending messages, while also continuing to gather data and training the local model. 
Once all devices fulfill the conditions we have successfully found the model all devices agree on to a certain extent. 
This is closely related, in terms of sample complexity, to a bound on the number of samples required for models to be sufficiently similar, such as the Hoefding Bound \todo{cite}.

Similar approaches, with a geometric interpretation have been discussed by Keren et al. \cite{sharfman2007geometric}, \cite{keren2011shape}, which interprets the threshold as convex hull. Local information that is outside of the convex hull is assumed to be active and continues sending messages until all local information is inside the same space.
\todo{this needs more research and work}

Model aggregation has also been studied.
For one we have the general approach to model aggregation and specifically in the distributed setting.
Then we have to consider the sample complexity, i.e., having a general idea bout how much data we need on each device.
This can be done by using a general network approach or using existing bounds such as the hoefding bound to guarentee the models being somewhat close.

What as already been done ?
Naive Kullback Leibler Aggregation with Bootstrapping, Radon Machines for Model Ensembling, simple averaging.

What guarentees do we have ? 
hoefding Bound, Regret etc.

Transitioning from a central computing device to a set of distributed devices requires careful planning and execution, even moreso when considering mobile devices, as limited processing power and power conservation have to be taken into account.
This task has also been explored using the term \textit{federated learning}.
McMahan et al. \cite{mcmahan2016communication} introduced federated learning for dealing with distributed data.
Here, the data is usually distributed on several devices and can not be communicated between these or a central node. 
Therefore, the models have to be learned on each device and then sent to a central node to compute an aggregate.
Depending on the data distribution type, either horizontally or vertically, we need to approach this task differently. 
Consider for example a distributed network of sensors.
Here we would assume the data to be vertically distributed as each sensor gathers data associated with a certain subset of features.
Instead, for this work we assume the former, which implies that all features are available on each device, with each device only containing a small subset of the data.
For each device we perform the intended machine learning task with respect to the underlying problem.
We will then aggregate these models using different mechanisms to create a more accurate and robust local or global model, which performs as well as or as close as possible to the model obtained by a central computing node that has access to the complete data.
Here local model refers to an individual model for each device.
We measure model performance by defining suitable statistics such as classification accuracy, robustness, generalisation capability or some error function.
While one can argue, that with a sufficient amount of data on each device, we are able to create a reasonably well performing model, we usually do not have the required amount of data available to achieve a sufficiently well performing model.

Performing the task on a set of devices allows us to significantly reduce the communication cost as we are not required to transfer data.
Instead, we are only required to send the model parameters or certain statistics, which are then aggregated to create a composite model.
Depending on the overall structure we may only be able create these composite models on the local device, i.e. we are limited to aggregating models individually by receiving information from connected (neighbouring) devices.
For this work we will focus on distributed probabilistic graphical models as our structure of choice for the distributed environment.
\section{Thesis Structure}

We will start with the necessary Background in Probability Theory, probabilistic graphical models and exponential families.
Then we will discuss Model Aggregation and different approaches to model aggregation. 
Afterwards we will have a detailed look at the architecture and its implementation, dependencies and structure.
Followed by performing experiments on data sets using the different aggregation mechanics and evaluate the results based on standard criteria for likelihood optimization as objective value (likelihood) or accuracy and f1 score when dealing with classification. 
Furthermore, we evaluate the number of samples required to obtain a stable maximum likelihood estimator and how close the models are in terms of distance between parameters and statistics, while also considering the amount of communication required for each aggregation mechanic.
Finally we will provide a conclusion to the experiments and the thesis, while also providing a brief outlook on possibly future work and areas that require more research.
