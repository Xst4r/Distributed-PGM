% einleitung.tex
\chapter{Introduction}
With an increasing number of devices and data collected on these devices, the task of machine learning becomes increasingly challenging.
Not only do we have to account for the machine learning task itself, we now also have to consider communication and network structure.
While in the best case in terms of a machine learning remains having all data available at some central device this may not be guaranteed for all applications.
Hence, creating a model, which utilizes all data available may not be feasible anymore, which requires us to bring machine learning tasks to the edge, i.e., onto individual devices in a distributed setting.

Leading factors, that make this a necessity are the communication cost incurred by sending the data to a central node.
While developement new technologies in the communication sector with an ever increasing bandwith , such as 5G with up to 1Gbit\todo{Cite}, are still being developed and deployed around the globe, it is still not possible to make use of this technology in all application. 
Furthermore, attempting to communicate all data for each task may still be too much as projects like IceCube\todo{Cite} produce a total of x TB data each day.
Additionally, user privacy is a major concern in a setting where we learn from data on user devices, as the data we use for learning tasks may contain sensitive information.
While some users may allow us to retrieve their data, this is not generally the case and as such it is advantegeous to treat the data as fixed on each device.   
However, even anonymized data can be profiled and tracked back to certain users [TODO CITE], which requires us to be even more careful about the data that is indeed communicated between devices.
The need for privacy leads to another, directly related challenge, which is the limited amount of memory and cpu power available on each device.
Moreover, we do not aim to strain the hardware of users too much as this would impair the devices' functionality as well as strain battery life in case of mobile devices such as smartphones.

All in all there are several limitations in a distributed setting, which requires us to choose our methods and machine learning algorithms carefully in order to keep our footprint as small as possible, while trying to obtain the best possible results.

These restrictions lead to the formulation of machine learning on the edge, that is, applying the machine learning tasks directly on each device and then building and aggregate model out of all available models.
Using the devices for machine learning and then convergecasting some metadata to a central node such a model parameters is an approach known as Federated Learning \todo{cite}.
This requires several careful considerations, such as energy efficient computing, preservation of privacy and the availability of limited ressources.


\section{Motivation and Background}
Communication between devices is inefficient.
Local Models usually have a large variance between each other on the same data/distribution.
Two staple methods, ensembling and aggregation.
Ensembling aims to decide the label of a class by accounting for each decision on the local models e.g. majority vote.
Aggregation aims to aggregate the approximated function, i.e., its parameters to create a new aggregated model, which then predicts the labels.
After aggregation we only need to predict once and not on every local model.
Models introduce non-linearity, which in turn might introduce an error in the aggregation as e.g. averaging is a linear operation.
Model aggregation has been studied by several people.
Aggregation requires communication between local models in a non-centralized approach or communication between local nodes and a coordinator (Federated learning).

Several problems are associated with distributed learning:
Amount of communication required.
Stopping criteria for local models, i.e., when are models close enough to stop training and this includes the question of sample complexity.
Model aggregation itself.


\section{Related Work}
Related Work for Model Aggregation, Local Thresholding(Ran Wolff, Assaf Schuster, Michael Kamp), Sample Complexity , Communication effieciency.


Motivation for this topic, why do we need distributed learning or even resource constrained ?

The related work to this thesis divides itself upon several reserach areas. 
For one we have the general approach to model aggregation and specifically in the distributed setting.
Then we have to consider the sample complexity, i.e., having a general idea bout how much data we need on each device.
This can be done by using a general network approach or using existing bounds such as the hoefding bound to guarentee the models being somewhat close.

What as already been done ?
Naive Kullback Leibler Aggregation with Bootstrapping, Radon Machines for Model Ensembling, simple averaging.

What guarentees do we have ? 
hoefding Bound, Regret etc.

Transitioning from a central computing device to a set of distributed devices requires careful planning and execution, even moreso when considering mobile devices, as limited processing power and power conservation have to be taken into account.
This task has also been explored using the term \textit{federated learning}.
McMahan et al. \cite{mcmahan2016communication} introduced federated learning for dealing with distributed data.
Here, the data is usually distributed on several devices and can not be communicated between these or a central node. 
Therefore, the models have to be learned on each device and then sent to a central node to compute an aggregate.
Depending on the data distribution type, either horizontally or vertically, we need to approach this task differently. 
Consider for example a distributed network of sensors.
Here we would assume the data to be vertically distributed as each sensor gathers data associated with a certain subset of features.
Instead, for this work we assume the former, which implies that all features are available on each device, with each device only containing a small subset of the data.
For each device we perform the intended machine learning task with respect to the underlying problem.
We will then aggregate these models using different mechanisms to create a more accurate and robust local or global model, which performs as well as or as close as possible to the model obtained by a central computing node that has access to the complete data.
Here local model refers to an individual model for each device.
We measure model performance by defining suitable statistics such as classification accuracy, robustness, generalisation capability or some error function.
While one can argue, that with a sufficient amount of data on each device, we are able to create a reasonably well performing model, we usually do not have the required amount of data available to achieve a sufficiently well performing model.

Performing the task on a set of devices allows us to significantly reduce the communication cost as we are not required to transfer data.
Instead, we are only required to send the model parameters or certain statistics, which are then aggregated to create a composite model.
Depending on the overall structure we may only be able create these composite models on the local device, i.e. we are limited to aggregating models individually by receiving information from connected (neighbouring) devices.
For this work we will focus on distributed probabilistic graphical models as our structure of choice for the distributed environment.
\section{Thesis Structure}