% einleitung.tex
\chapter{Introduction}
With an increasing number of devices and data collected on these devices, the task of machine learning becomes increasingly challenging.
Not only do we have to account for the machine learning task itself, we now also have to consider communication and network structure.
While in the best case in terms of a machine learning remains having all data available at some central device this may not be guaranteed for all applications.
Hence, creating a model, which utilizes all data available may not be feasible anymore, which requires us to bring machine learning tasks to the edge, i.e., onto individual devices in a distributed setting.

Leading factors, that make this a necessity are the communication cost incurred by sending the data to a central node.
While developement new technologies in the communication sector with an ever increasing bandwith , such as 5G with up to 10GBit/s \cite{nordrum2017ieee}, are still being developed and deployed around the globe, it is still not possible to make use of this technology in all application. 
Furthermore, attempting to communicate all data for each task may still be too much as projects like IceCube\todo{Cite} produce a total of x TB data each day.
Additionally, user privacy is a major concern in a setting where we learn from data on user devices, as the data we use for learning tasks may contain sensitive information.
While some users may allow us to retrieve their data, this is not generally the case and as such it is advantegeous to treat the data as fixed on each device.   
However, even anonymized data can be profiled and tracked back to certain users, which requires us to be even more careful about the data that is indeed communicated between devices.
These types of de-anonymization schemes have been well studied in terms of user data by Narayanan et al. \cite{narayanan2008robust} or in terms of geolocation(GPS) data by Gambs et al. \cite{gambs2014anonymization}.
The need for privacy leads to another, directly related challenge, which is the limited amount of memory and cpu power available on each device.
Moreover, we do not aim to strain the hardware of users too much as this would impair the devices' functionality as well as strain battery life in case of mobile devices such as smartphones.

All in all there are several limitations in a distributed setting, which requires us to choose our methods and machine learning algorithms carefully in order to keep our footprint as small as possible, while trying to obtain the best possible results.

These restrictions lead to the formulation of machine learning on the edge, that is, applying the machine learning tasks directly on each device and then building and aggregate model out of all available models.
Using the devices for machine learning and then convergecasting some metadata to a central node such a model parameters is an approach known as Federated Learning \todo{cite}.
This requires several careful considerations, such as energy efficient computing, preservation of privacy and the availability of limited ressources.

In distributed systems there exist two major approaches to solving certain tasks. 
While many applications allow for data to be stored on some central system, some applications especially involving end users, require a more strict handling of communicating data.
Possibly the most widely known approach for the centralized system is Map\&Reduce, which takes some data and a task, then distributes the data onto several nodes.
These then compute the given task on a subset of the data.
The result is then aggregated into the final result by some manner of mapping function.

The other approach involves the data being directly generated on the nodes.
This is usally the case when talking about end-user devices such as smartphones, personal computers or in sensor networks, where each sensor locally gathers data.
Even in such networks in might be beneficial to compute some intermediate results directly to reduce communication overhead, while other apporaches that would allow for data transmission may be restricted due to privacy reasons.


Briefly, given a solution space $\Theta$ containing all possible solutions to some machine learning task, we are trying to find a solution $\bar{\theta} \in \Theta$, from a set of local solutions $\Psi = \{\theta^{(1)}, \ldots \theta^{(n)}\} \subset \Theta$ on n devices.



\section{Motivation and Background}
Communication between devices is inefficient.
Local Models usually have a large variance between each other on the same data/distribution.
Two staple methods, ensembling and aggregation.
Ensembling aims to decide the label of a class by accounting for each decision on the local models e.g. majority vote.
Aggregation aims to aggregate the approximated function, i.e., its parameters to create a new aggregated model, which then predicts the labels.
After aggregation we only need to predict once and not on every local model.
Models introduce non-linearity, which in turn might introduce an error in the aggregation as e.g. averaging is a linear operation.
Model aggregation has been studied by several people.
Aggregation requires communication between local models in a non-centralized approach or communication between local nodes and a coordinator (Federated learning).

Several problems are associated with distributed learning:
Amount of communication required.
Stopping criteria for local models, i.e., when are models close enough to stop training and this includes the question of sample complexity.
Model aggregation itself.


\section{Related Work}

Distributed Learning and Model Aggregation is a large research area that has been a subject to a variety of reserach.
Applications such as \todo{Application A}, \todo{Application B} or \todo{Application C} arise frequently in different areas of reserach and industry. 

Therefore studying different tasks associated with distributed learning such as model aggregation, thresholding and communication optimization is vital for improving and advancing the this research area.

Wolff \cite{wolff2013local} has proposed a way to approach thresholding in distributed system for a network without structural restrictions.
The proposed algorithm passes the local information about some defined threshold to neighbouring nodes, which is then propagated thorugh the network structure. Nodes/Devices that are within the defined threshold stop sending messages until they receive new messages from neighbours that would lead to the local information not satisfying the conditions.
This approach can be used to monitor local conditions such as sample size or distances between models in order to find local models that do not satisfy the threshold. These would then continue sending messages, while also continuing to gather data and training the local model. 
Once all devices fulfill the conditions we have successfully found the model all devices agree on to a certain extent. 
This is closely related, in terms of sample complexity, to a bound on the number of samples required for models to be sufficiently similar, such as the Hoefding Bound \todo{cite}.

Similar approaches, with a geometric interpretation have been discussed by Keren et al. \cite{sharfman2007geometric}, \cite{keren2011shape}, which interprets the threshold as convex hull. Local information that is outside of the convex hull is assumed to be active and continues sending messages until all local information is inside the same space.
\todo{this needs more research and work}

Model aggregation has also been studied.
For one we have the general approach to model aggregation and specifically in the distributed setting.
Then we have to consider the sample complexity, i.e., having a general idea bout how much data we need on each device.
This can be done by using a general network approach or using existing bounds such as the hoefding bound to guarentee the models being somewhat close.

What as already been done ?
Naive Kullback Leibler Aggregation with Bootstrapping, Radon Machines for Model Ensembling, simple averaging.

What guarentees do we have ? 
hoefding Bound, Regret etc.

Transitioning from a central computing device to a set of distributed devices requires careful planning and execution, even moreso when considering mobile devices, as limited processing power and power conservation have to be taken into account.
This task has also been explored using the term \textit{federated learning}.
McMahan et al. \cite{mcmahan2016communication} introduced federated learning for dealing with distributed data.
Here, the data is usually distributed on several devices and can not be communicated between these or a central node. 
Therefore, the models have to be learned on each device and then sent to a central node to compute an aggregate.
Depending on the data distribution type, either horizontally or vertically, we need to approach this task differently. 
Consider for example a distributed network of sensors.
Here we would assume the data to be vertically distributed as each sensor gathers data associated with a certain subset of features.
Instead, for this work we assume the former, which implies that all features are available on each device, with each device only containing a small subset of the data.
For each device we perform the intended machine learning task with respect to the underlying problem.
We will then aggregate these models using different mechanisms to create a more accurate and robust local or global model, which performs as well as or as close as possible to the model obtained by a central computing node that has access to the complete data.
Here local model refers to an individual model for each device.
We measure model performance by defining suitable statistics such as classification accuracy, robustness, generalisation capability or some error function.
While one can argue, that with a sufficient amount of data on each device, we are able to create a reasonably well performing model, we usually do not have the required amount of data available to achieve a sufficiently well performing model.

Performing the task on a set of devices allows us to significantly reduce the communication cost as we are not required to transfer data.
Instead, we are only required to send the model parameters or certain statistics, which are then aggregated to create a composite model.
Depending on the overall structure we may only be able create these composite models on the local device, i.e. we are limited to aggregating models individually by receiving information from connected (neighbouring) devices.
For this work we will focus on distributed probabilistic graphical models as our structure of choice for the distributed environment.
\section{Thesis Structure}

We will start with the necessary Background in Probability Theory, probabilistic graphical models and exponential families.
Then we will discuss Model Aggregation and different approaches to model aggregation. 
Afterwards we will have a detailed look at the architecture and its implementation, dependencies and structure.
Followed by performing experiments on data sets using the different aggregation mechanics and evaluate the results based on standard criteria for likelihood optimization as objective value (likelihood) or accuracy and f1 score when dealing with classification. 
Furthermore, we evaluate the number of samples required to obtain a stable maximum likelihood estimator and how close the models are in terms of distance between parameters and statistics, while also considering the amount of communication required for each aggregation mechanic.
Finally we will provide a conclusion to the experiments and the thesis, while also providing a brief outlook on possibly future work and areas that require more research.
