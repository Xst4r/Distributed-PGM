% einleitung.tex
\chapter{Introduction}
With an increasing number of devices available and being deployed for a variety of tasks more and more data collected is available in a distributed fashion.
Such devices include mobile phones, sensor networks or embedded systems with and without user interaction.
Not only do we have to consider the machine learning task a hand we now have to take a distributed environment into account, which involves communication from and between devices and the underlying  network architecture.
Ideally, in terms of machine learning, we are able to transmit all observed data to some  central server in order to utilize the full capabilities of the chosen approach.
However, this may not be possible for all applications.
Creating a single model, which has access to all data may not be feasible anymore, which leads to alternative ways we are able fully benefit from the collected and readily available data.
One possibility is creating an array of distributed learners, each computing a model with locally available data, then transmitting the resulting models to a central server, where they may be aggregated into a better performing model.

One of the main reasons forcing us to include this intermediate step are bandwidth or storage limitations incurred by continuously transmitting all observed data to a central server.
While research on increasing the bandwidth of such applications progresses steadily, e.g. the new telecommunication standard 5G with a transmission rate of up to 10GBit/s \cite{nordrum2017ieee}, in some areas restrictions to bandwidth and communication still apply which render new technologies as impractical. 
Especially when considering energy efficiency, constant transmission of data may not be feasible and thus the data may need to be processed locally.
Additionally, user privacy is a major concern in a setting where we learn from data on user devices, as the data we use for learning tasks may contain sensitive information.
While some users may allow us to retrieve their data, this is not generally the case and as such it is advantegeous to treat the data as fixed on each device.   

However, even anonymized data can be profiled and tracked back to the users that the data originated from, which requires us to be even more attentive about what kind of information is communicated and transmitted.
De-anonymization schemes with regards to anonymized user data have been well studied  by Narayanan et al. \cite{narayanan2008robust} or in terms of geolocation(GPS) data by Gambs et al. \cite{gambs2014anonymization}.
Even when having permission to perform machine learning on user devices we still want to be as energy efficient as possible, not straining resources up to a noticeable amount \wrt cpu and memory usage as well as battery life.
Similarly, on sensor networks with remote servers our goal is to be as efficient as possible, respecting the local constraints of processing power and energy consumption, if applicable.

This work in particular deals with statistical models, more specifically exponential families, on distributed devices as these have a few advantages, that can be exploited in order to reduce the resource footprint.
Briefly, we are looking for a probability distribution $\mathbb{P}_{\vect{\theta}}$ conditioned on some parameters $\vect{\theta}$ that best describes the observed data $\mathcal{D}$ on each device.
The data is assumed to be collected on each device and in addition that it cannot be exchanged in any way due to reasons such as privacy preservation.
Usually we estimate the parameters by optimizing a function such as the likelihood.
After finding a suitable distribution and the desired model parameters we can then perform generative tasks on unknown data, which includes sampling new data, completing missing data on partially observed samples or classification, which is a special case of the former, since only one observation is missing.

All in all there are several limitations in a distributed setting, which requires us to choose our methods and machine learning algorithms carefully in order to be able to trainb models on the devices, while trying to obtain the best possible results.

\section{Motivation and Related Work}

%Introduction, ensembling vs aggregation
These restrictions give rise to the formulation of machine learning on the edge. 
Our goal is to directly apply the same machine learning task on each device in order to create a set of local experts based on the individually observed data.
Afterwards, these experts would be grouped or combined, obtaining an overall better performing expert 
that has had access to all distributed devices.
Approaches for combining models can be roughly separated into two categories, ensembling and aggregation. 
Ensembles usually predict each newly observed sample individually, while aggregates predict only once.
The final prediction is then for example decided by majority vote of all local predictors.
This procedure is indirectly encoded into aggregates, as instead of combining the prediction we combine the models instead, obtaining a single model and thus a single prediction.

% Distributed vs Federated Intro
In a distributed environment we first have to consider where and how the data is collected. 
If the data is collected a central node, we can employ the typical distributed approaches, i.e., dividing the data into chunks and sending each chunk onto a single device.
Each device then would perform the same operation on a subset of the data and report the results to the coordinator.
However, the data may also be collected on an array of devices, i.e., the data itself is distributed as well.
Then, we either have the option to simply collect the data from each device onto a single coordinator or server, or to leave the data on the devices. 
Hence, the machine learning task may have to be moved towards the data.

%Federated Learning
While many applications allow for data to be stored on some central system, some applications especially user devices or resource constrained systems, require a more strict handling of communicating data.
This can either be due to privacy concerns, storage limitations or simply bandwidth restrictions.
Especially in wirless networks such as wireless sensor networks, communicating a large amount of data would be prohibitive to the task.

%TODO Federated vs. Centralized

% Problem Definition
Briefly, given a solution space $\Theta$ containing all feaisble solutions to some machine learning task, we are trying to find a solution $\bar{\theta} \in \Theta$, from a set of local solutions $\Psi = \{\theta^{(1)}, \ldots \theta^{(n)}\} \subset \Theta$ on n devices.
In this case we want to find a suitable probability distribution $\mathbb{P}_{\vect{\theta}}$ that is optimal in $\vect{\theta}$ \wrt an optimziation criterium such as the likelihood.

%Todo Why Exp Families, Statistical Models
%Just Notes
Communication between devices is inefficient.
Local Models usually have a large variance between each other on the same data/distribution.
Two staple methods, ensembling and aggregation.
Ensembling aims to decide the label of a class by accounting for each decision on the local models e.g. majority vote.
Aggregation aims to aggregate the approximated function, i.e., its parameters to create a new aggregated model, which then predicts the labels.
After aggregation we only need to predict once and not on every local model.
Models introduce non-linearity, which in turn might introduce an error in the aggregation as e.g. averaging is a linear operation.
Model aggregation has been studied by several people.
Aggregation requires communication between local models in a non-centralized approach or communication between local nodes and a coordinator (Federated learning).

Several problems are associated with distributed learning:
Amount of communication required.
Stopping criteria for local models, i.e., when are models close enough to stop training and this includes the question of sample complexity.
Model aggregation itself.

%Related Work
Distributed Learning and Model Aggregation is a large research area that has been a subject to a variety of reserach.
Applications such as \todo{Application A}, \todo{Application B} or \todo{Application C} arise frequently in different areas of reserach and industry. 

Therefore studying different tasks associated with distributed learning such as model aggregation, thresholding and communication optimization is vital for improving and advancing the this research area.

Wolff \cite{wolff2013local} has proposed a way to approach thresholding in distributed system for a network without structural restrictions.
The proposed algorithm passes the local information about some defined threshold to neighbouring nodes, which is then propagated thorugh the network structure. Nodes/Devices that are within the defined threshold stop sending messages until they receive new messages from neighbours that would lead to the local information not satisfying the conditions.
This approach can be used to monitor local conditions such as sample size or distances between models in order to find local models that do not satisfy the threshold. These would then continue sending messages, while also continuing to gather data and training the local model. 
Once all devices fulfill the conditions we have successfully found the model all devices agree on to a certain extent. 
This is closely related, in terms of sample complexity, to a bound on the number of samples required for models to be sufficiently similar, such as the Hoefding Bound \todo{cite}.

Similar approaches, with a geometric interpretation have been discussed by Keren et al. \cite{sharfman2007geometric}, \cite{keren2011shape}, which interprets the threshold as convex hull. Local information that is outside of the convex hull is assumed to be active and continues sending messages until all local information is inside the same space.
\todo{this needs more research and work}

Model aggregation has also been studied.
For one we have the general approach to model aggregation and specifically in the distributed setting.
Then we have to consider the sample complexity, i.e., having a general idea bout how much data we need on each device.
This can be done by using a general network approach or using existing bounds such as the hoefding bound to guarentee the models being somewhat close.

What as already been done ?
Naive Kullback Leibler Aggregation with Bootstrapping, Radon Machines for Model Ensembling, simple averaging.

What guarentees do we have ? 
hoefding Bound, Regret etc.

Transitioning from a central computing device to a set of distributed devices requires careful planning and execution, even moreso when considering mobile devices, as limited processing power and power conservation have to be taken into account.
This task has also been explored using the term \textit{federated learning}.
McMahan et al. \cite{mcmahan2016communication} introduced federated learning for dealing with distributed data.
Here, the data is usually distributed on several devices and can not be communicated between these or a central node. 
Therefore, the models have to be learned on each device and then sent to a central node to compute an aggregate.
Depending on the data distribution type, either horizontally or vertically, we need to approach this task differently. 
Consider for example a distributed network of sensors.
Here we would assume the data to be vertically distributed as each sensor gathers data associated with a certain subset of features.
Instead, for this work we assume the former, which implies that all features are available on each device, with each device only containing a small subset of the data.
For each device we perform the intended machine learning task with respect to the underlying problem.
We will then aggregate these models using different mechanisms to create a more accurate and robust local or global model, which performs as well as or as close as possible to the model obtained by a central computing node that has access to the complete data.
Here local model refers to an individual model for each device.
We measure model performance by defining suitable statistics such as classification accuracy, robustness, generalisation capability or some error function.
While one can argue, that with a sufficient amount of data on each device, we are able to create a reasonably well performing model, we usually do not have the required amount of data available to achieve a sufficiently well performing model.

Performing the task on a set of devices allows us to significantly reduce the communication cost as we are not required to transfer data.
Instead, we are only required to send the model parameters or certain statistics, which are then aggregated to create a composite model.
Depending on the overall structure we may only be able create these composite models on the local device, i.e. we are limited to aggregating models individually by receiving information from connected (neighbouring) devices.
For this work we will focus on distributed probabilistic graphical models as our structure of choice for the distributed environment.
\section{Thesis Structure}

We will start with the necessary Background in Probability Theory, probabilistic graphical models and exponential families.
Then we will discuss Model Aggregation and different approaches to model aggregation. 
Afterwards we will have a detailed look at the architecture and its implementation, dependencies and structure.
Followed by performing experiments on data sets using the different aggregation mechanics and evaluate the results based on standard criteria for likelihood optimization as objective value (likelihood) or accuracy and f1 score when dealing with classification. 
Furthermore, we evaluate the number of samples required to obtain a stable maximum likelihood estimator and how close the models are in terms of distance between parameters and statistics, while also considering the amount of communication required for each aggregation mechanic.
Finally we will provide a conclusion to the experiments and the thesis, while also providing a brief outlook on possibly future work and areas that require more research.
