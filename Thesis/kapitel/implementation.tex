% implementation.tex

\chapter{Implementation}
\label{chapter:ch4}
In this chapter, we will discuss the implementation, featuring the theoretical results from \autoref{chapter:kap2} and aggregation methods presented in \autoref{chapter:ch3} for probabilistic graphical models and exponential families.
The implementation features a simulated environment with distributed learners and the coordinator.
The learners have shared access to some data structures used for simulating the message exchange between both parties.
We are going to discuss implemented features, design choices as well as providing a general idea about data processing, model initialization, and code architecture.

The framework is implemented with Python as it is highly accessible and features C-based packages for efficient linear algebra operations and data processing.
With Python, we furthermore preserve the option to interface to C and C++ libraries if necessary for additional computational speedup.


We mainly use well-known python modules such as NumPy\footnote[1]{\hypersetup{pdfborder=1 1 1}\href{https://numpy.org/}{https://numpy.org/}\hypersetup{hidelinks}}, SciPy\footnote[2]{\hypersetup{pdfborder=1 1 1}\href{https://www.scipy.org/}{https://www.scipy.org/}\hypersetup{hidelinks}}, Pandas\footnote[3]{\hypersetup{pdfborder=1 1 1}\href{https://pandas.pydata.org/}{https://pandas.pydata.org/}\hypersetup{hidelinks}}, and Scikit-Learn~\cite{scikit-learn} for data processing, matrix management, and evaluation.
Moreover, we use the \texttt{PXPY}\footnote[4]{\hypersetup{pdfborder=1 1 1}\href{https://randomfields.org/}{https://randomfields.org/}\hypersetup{hidelinks}} C++ module for probabilistic graphical models and probabilistic inference.

Our goal in this work is to present model aggregation techniques; therefore, distributed learning is implemented in a simulated fashion.
The simulation pattern consists of two objects, coordinator and learners, exchanging data on shared memory. 
The coordinator object has access to a model object, which holds the results for all distributed learners.
Depending on the aggregation method, the coordinator may have access to additional data, such as the total number of distributed learners, parameter vectors, or likelihood. 
However, we strictly enforce the coordinator not accessing the local data or sufficient statistics.

Moreover, in a real-world setting, the devices may terminate optimization at different times and send the parameters to the coordinator asynchronously. 
Here, we enforce the training to be synchronized, such that we only send the data to the coordinator once all models have finished training.

The implementation can be roughly divided into three phases, the initialization, run, and aggregation phase.
While the initialization phase is invoked once during startup, run and aggregation phase alternate every round starting with the run phase.
Let us now have a closer look at the code structure, data processing, and initialization process, which is executed at the program's start and before training the models. 

\section{Initialization Phase}
\input{kapitel/figures/flowchart.tex}
The initialization phase is executed at the program's start, loading the data, parameters, and creating mandatory objects such as the coordinator, models, and aggregators.
\autoref{fig:flowchart} shows the initial setup.
First, for a data set $\mathcal{D}$, we create a data set object, which discretizes the data and creates the desired number of cross-validation splits. 
For easier accessibility, the implementation features a basic framework for directly downloading and extracting datasets when providing a hyperlink to the data server.
After that, we create a coordinator object, which has access to the data set, a sampler object, and the aggregators.
The sampler is responsible for creating random index-based splits, accessible by the models, providing them access to the data set via index operations.
Finally, we create the graph-based independence structure $G=(V,E)$  on a holdout set, where the number of vertices is the number of features in $\mathcal{D}$ and generating the conditional independencies using the Chow-Liu algorithm.
Afterward, we create distributed models from the individual splits and graph, which then start alternating between run and aggregation phase (cf. \autoref{fig:dist}).

We discretize the data before determining the independence structure. Then, the number of states for each feature is the maximum among all observations from the corresponding discretized feature. 
After the discretization, each random variable $X_i$ with discretized state-space $\hat{\mathcal{X}}_i$ has at most $\{0, \ldots, \max(\hat{\mathcal{D}}_{\cdot i})\}$ observable states.

Having prior information about structure and state-space is vital as the aggregation methods used for this work rely on having an identical structure and known state-space across the distributed learners.

\subsection{Cross-Validation}
We generate cross-validation splits by a fixed seed to increase the reproducibility of our results.
Furthermore, we add an index over the data, which is then shuffled and split into the desired number of cross-validation runs.
For each run, we assign one split to represent the test data, while the remaining folds are used for training.
The training data is again shuffled and distributed by a secondary index set to all distributed learners.

\subsection{Discretization}
Most datasets contain features with a real-valued range, which either leads to non-discrete problems or dramatically increases the model complexity.
By discretizing the data, we can group observations into a fixed number of bins, which in turn also reduces the amount of parameters.
We discretize data by mapping each sample to a bin index.
Consider, for example, a sample $x=5$ and two bins consisting of an index and limits $(1,[0,10)),(2,[10,20))$, implicitly assigning x to the first bin and thus mapping x to the first index. 
Observations outside the original interval range are assigned to the 0-th and q-th bin, respectively.

We discretize the data using the $q$ quantiles on each feature, which are computed based on the training data. 
The number of quantiles $q$ is either chosen to be the same across all features or for each feature individually.
Furthermore, if the number of quantiles is higher than the state-space for any random variable $X_i$, we do not compute the discretized feature as the number of possible observations is less than $q$.
Afterward, we use the same intervals to discretize the training data.
The intervals are a mapping function, which assigns observations of a single feature to the index of interval borders. 
Choosing the correct number of quantiles is difficult and requires prior knowledge about the data.
We may choose the number of quantiles based on the total range of each feature or based on some prior information about a feature, such as what physical measurement or unit it represents.

\section{Run Phase}
After creating all necessary objects, the run, and aggregation phases begin alternating, where the available data on the learners increases each round.
\autoref{fig:dist} illustrates the sequence of operations.
First, each of the distributed learners trains a probabilistic graphical model with the locally available data.  
Then, the coordinator receives and aggregates parameters from the distributed learners $\vect{m}$. 
Optionally we return the aggregate to the distributed learners, using them as initial parameters for the upcoming round.

In practice, we only have a fixed amount of data available for each learner.
We simulate this by restricting access to the full local data, i.e., at each round t on the i-th learner  $n^{t}_{m^i}$ samples are available with $n^{t+1}_{m^i} = n^{t}_{m^i} + f(t+1)$, where $f(t)$ is a positive function or constant.
After each round we extend the local data set, i.e., given the full local data $\abs{\mathcal{D}_i} = n_{m^i}$  we choose the first  $ n^{t}_{m^i} \leq n_{m^i}$ elements from the data index $I^t =\{1, \ldots, n^{t}_{m^i}\} \subseteq I = \{1, \ldots, n_{m^i}\}$. 
Note, we do not allow duplicates, i.e., each index added is unique, and only adding indices that are not yet in the index.

\input{kapitel/figures/distributed_schema.tex}

Since preprocessing standardizes the data in such a way that it can be used as direct input to the model, a single model class is sufficient for distributing and training most data sets.
In case we need to perform additional modifications to the training, extending the model class to adapt to some situations is also an option.
However, in our case, a single model class for all data sets was sufficient.
Each model, when created, uses an instantiation of a data set class and features optional parameters to control the simulated, distributed environment. 
Setting these parameters entails, for example, fixing the maximum number of iterations for each training pass and the number of distributed learners.
Each distributed learner is implemented by a \texttt{PXPY} model for training and inference at its core.

\subsection{\texttt{PXPY}}
\texttt{PXPY}~\cite{piatkowski2018exponential} (\texttt{PX}) is a toolkit for performing probabilistic inference and parameter estimation using graphical models using exponential family distributions.
Initially implemented as a C++ module, PX also features a Python front-end for easier access to core functionalities.
PX offers a variety of model- and inference types such as Markov random fields with real value or integer parameters, while also providing different parallelized inference algorithms such as belief-, loopy belief propagation or the junction tree algorithm.

Core features include the model- training, evaluation, and inference while providing means to influence the optimization with user-defined callbacks or functions hooks that allow adding regularization terms manually or to check progress and convergence of the training.

Options are passed via register entries using a virtual machine, which is then accessed by the C++ core to execute the algorithms with the provided options.

The graph's independence structure can be directly passed as an edge list or be inferred directly from the data using structure learning algorithms such as the Chow-Liu algorithm.
Additionally, we may choose between several default graph structures such as chain, grid, or star graphs.

Optimization is done through negative average likelihood minimization using first-order proximal gradient methods with the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) (see Piatkowski \cite{piatkowski2018exponential}).
Functionality for gradient manipulation is provided by explicitly passing function hooks to \texttt{PX}, which are then called at the corresponding optimization step and execute the user-defined function.
Hooks may be used, for example, to add a regularization term to the gradient before updating the objective function. 
Here, \texttt{PX} calls available hooks for regularization, providing an optimization-state object containing the current gradient, weights, step size, and the current number of iterations, which can then be directly manipulated with in-memory operations.
Likewise, we may also check if the objective value is close to converging between two gradient descent iterations and stop the optimization altogether if desired.

\subsection{Training}
At the start of each training iteration, we first increment the number of samples available on each distributed learner. 
We may then either update the previous model by extending the sufficient statistics with the new data or train a new model. 
In either case we  have to keep the graph $G$, state-space $\vect{\mathcal{X}}$, average sufficient statistics $\tilde{\mathbb{E}}_{\mathcal{D}^i}[\phi(x)] = \tilde{\vect{\mu}}^i$ and the previous parameters $\vect{\theta}^i$ in memory in order to update the i-th learner.

When creating a new \texttt{PX} model, the state-space is usually inferred directly from the data, while the graph has to be passed either as an edge list or as a \texttt{PX} graph object. 
Here we run into the issue of the state-spaces for distributed learners not being identical, especially when the number of samples is low. 
When having only a small number of samples, some states are with high probability not observed, which leads to an incomplete state-space and thus to a varying number of parameters. 
This issue is solved by providing \texttt{PX} with an additional sample $\mathcal{D}'^i = \mathcal{D}^i \cup \vect{x}_{max}$ with $\vect{x}_{max} = (max(\mathcal{D}_{\cdot 1}), \ldots, max(\mathcal{D}_{\cdot m}))$, which contains the maximum observed (or discretized) value for each feature.
Since the state-space is created by taking the maximum value of each feature, this leads to all distributed learners having the same number of parameters.
Afterwards, we remove the additional sample from the sufficient statistics such that
\begin{equation}
    \tilde{\vect{\mu}}^t_{m^i} = \frac{(n^t_{m^i} + 1) \cdot \tilde{\vect{\mu}}^{'i}_{m^i} - \phi(\vect{x}_{max})}{n^t_{m^{i}}},
\end{equation}
while keeping the dimensionality of parameters equal across all learners.
This way, we ensure all models having the same structure and number of parameters, even when some configurations have not been observed yet.

We start the parameter estimation by invoking the train method of \texttt{PX}, passing the graph and data along with a maximum number of iterations.
Optionally, we provide the progress hooks to be called from within \texttt{PX}.
On each gradient descent iteration, we monitor the difference between current and previous likelihood. 
Once the difference is sufficiently small, we start a grace period of, for example, $t=100$ iterations, after which we terminate training, given that the difference remains below that threshold.

When using a regulariser on the likelihood, we provide \texttt{PX} with an additional regularization hook. 
The hook then copies the sum of gradient and derivative regulariser to the memory reserved for the gradient.
For the $\ell_2$ regularization we, for example, have 
\begin{equation}
    \begin{split}
    \nabla \ell_R( \vect{\theta}; \mathcal{D}) &= \nabla_{\vect{\theta}} \ell( \vect{\theta}; \mathcal{D}) +  \nabla_{\vect{\theta}}\lambda R(\vect{\theta}) \\
    &=\nabla_{\vect{\theta}} \ell( \vect{\theta}; \mathcal{D}) +  2\lambda \vect{\theta},
    \end{split}
\end{equation} 
where $\lambda$ controls the impact of the regularization term, with $\lambda=0$ equating to an unregularised task.

Once the training terminates by either exceeding the maximum of iterations or by triggering the stop criterion, we store the local model parameters in a shared list, which is accessible by the coordinator. 
The coordinator then accesses the parameters for the aggregation phase.


\section{Aggregation Phase}
Upon completion of the run phase, the coordinator receives the updated local model parameters and uses these to create an aggregator instance.
The parameters for an arbitrary aggregator object are typically passed as $\mathbb{R}^{d \times k}$ matrix, where, again, $d$ is the number of parameters for each model, and $k$ is the number of models.

Before the aggregation, we may optionally choose to sample additional models, i.e., parameter vectors from existing ones taking advantage of the asymptotic normality of the MLE (cf. \autoref{ssec:asymp}).
This is especially useful for radon machines when we have close to $ r^h << k < r^{h+1}$ models, but strictly less than required for aggregating $r^{h+1}$ models for an arbitrary $h$.

Due to the asymptotic normality, we can center the sample distribution around $\vect{\theta}^i$ if we assume this to be the asymptotic result.
In case, where it is not feasible to compute and store the Fisher information, we have to determine the covariance matrix for $\hat{\vect{\theta}}^i \sim \mathcal{N}(\vect{\theta}^i, \cdot)$ the asymptotic distribution.
The different choices for correlation matrices are derived from the results in \autoref{ssec:asymp} and \autoref{ssec:bounds}.
%We could use Wasserstein-2 Distance to measure the distance between the local distribution and the true distribution

\subsection{Covariance Generation}
\label{ssec:covgen}

Recall, that we want to sample from $\vect{\theta} \sim \mathcal{N}(\vect{\theta}^*, \cdot)$ for some covariance matrix $\vect{\Sigma}$ under the assumption that $\vect{\theta}^*$ are the true parameters.
In this case we can use either the full covariance matrix $\vect{\Sigma}$, the diagonal $diag(\Sigma)$ or a single value $\sigma^2 I$.
Ideally, we want to compute the fisher information \wrt $\vect{\theta}^*$, but this is not always possible.

Instead, we determine the covariance matrix for each distribution in three different ways, while also having the option to skip sampling altogether.
While the uniform variance and the diagonal are only required to contain positive entries only, the full covariance matrix needs to satisfy some additional properties.

Covariance matrices are always positive semidefinite, symmetric, and non-singular. 
Hence sampling all entries at random may almost definitely result in a matrix violating at least one of these constraints.
However, there exist algorithms and techniques that generate full covariance matrices.
First, we can sample a matrix $A\in \mathbb{R}^{d \times d}$ over an arbitrary distribution and perform a matrix multiplication with its transpose
\begin{equation}
    \Sigma = AA^T, \quad A_{ij}\sim\mathcal{U}(a,b), \; \forall \;0 < i,j \leq n,
\end{equation}
where $\mathcal{U}$ is a uniform distribution with minimum $a$ and maximum $b$.

The result is a positive semidefinite and symmetric matrix, while the matrix may still contain linear dependencies for some rows or columns.

Another approach originally introduced by Davies and Higham \cite{davies2000numerically} uses a set of positive eigenvalues to sample a correlation matrix.
Given a set of positive eigenvalues $\vect{\lambda}\in \mathbb{R}^d$ where $\sum_{i=1}^n \lambda_i = d$ we can sample a random correlation $\rho$ with these eigenvalues
This can be used to randomly generate a set of positive eigenvalues, then normalizing them such that the sum is equal to the number of dimensions $d$ and use the resulting normalized eigenvalues to sample a fitting correlation matrix. 
We then obtain the covariance matrix by rescaling the components of the correlation matrix with the product of standard deviations from two components 
\begin{equation}
    \begin{split}
            \rho_{ij} &= \frac{\Sigma_{ij}}{\sqrt{\Sigma_{ii}\Sigma_{jj}}} \\
            \Sigma_{ij} &=  \rho_{ij}\sqrt{\Sigma_{ii}\Sigma_{jj}}.
    \end{split}
\end{equation}

While both variants mentioned above result in proper covariance matrices, they do not have an inherent relationship with the actual covariance matrix for the asymptotic distribution.
Recall that the actual covariance matrix is the inverse Fisher information matrix of the likelihood, i.e., the second-order gradient $\nabla^2 A(\vect{\theta})$ of the likelihood \wrt $\vect{\theta}$.
While the computation of the full fisher information matrix may not be feasible on the devices or even the coordinator, we can partially compute the components. 
For example the diagonal of the Fisher information for some parameters $\vect{\theta}^j$ on a distributed learner is 
\begin{equation}
    diag(i(\vect{\theta}^j)) = \mathbb{E}[\phi(X)] - \mathbb{E}[\phi(X)]^2,
\end{equation}
which for binary random variables is just the difference between the marginal probability of observing a state and its squared value.
For distributions on local models, we add an error to the diagonal of the variance, i.e.,
\begin{equation}
    \tilde{\Sigma}=  \frac{1}{\abs{\mathcal{D}} \cdot diag(i(\vect{\theta}^j))} + \epsilon,
\end{equation}
where $\epsilon$ is based on the bounds derived in \autoref{ssec:bounds}.

Moreover, we may compute entries on the diagonal blocks, that is, entries between sufficient statistics from the same clique:
\begin{equation}
    i(\vect{\theta})_{C_{ij}} = 0- \sum_{\vect{x} \in \vect{\mathcal{X}}} \mathbb{P}_{\vect{\theta}}(X= \vect{x}) \phi_C(\vect{x})_i \sum_{\vect{x} \in \vect{\mathcal{X}}} \mathbb{P}_{\vect{\theta}}(X= \vect{x}) \phi_C(\vect{x})_j= - p_i p_j,
\end{equation}
where $ \mathbb{E}[\phi(\vect{X})_i \phi(\vect{X})_j]$ (cf. \autoref{eq:sparsefish}) is always zero as the probability of observing two events simultaneously in a single clique is zero.

Since we are introducing additional variance between parameters of distributed learners, we can quantify this error and add it to the variance terms.
As the addition of positive values to the diagonal preserves the properties of a covariance matrix, we may add an error term to the diagonal while preserving positive semidefiniteness, symmetry, and non-singularity.

Diagonal matrices such as $diag(\Sigma)$ for some $\Sigma_{ii} > 0, \; \forall 1 \leq i \leq d$ are always positive semidefinite and invertible unless $det(\Sigma) = \prod_{i=1}^n \Sigma_{ii} \approx 0$ becomes numerically too small to represent on the available hardware architecture. 
If this is the case we can apply a matrix regularization $diag(\Sigma) + \lambda I$ for a tradeoff in precision with for example , where $I$ is the identity matrix and $\lambda > 0$.

In summary, we present three different ways of generating full covariance matrices $\Sigma$ or simplifications thereof using only the diagonal $diag(\Sigma)$ or a single variance term $\sigma^2$ for all distribution components.
\subsection{Aggregators}

The aggregator interface provides the predefined functions for implementing additional aggregators.
Aggregators receive the models alongside additional, for the aggregation mandatory, user-defined parameters.
We expect the input models to be either in matrix form or a list of \texttt{PXPY} model objects, which contain the parameters to be aggregated.
Then, calling the aggregate function starts the aggregation process for the provided models. 
Each aggregator stores the results in their instance, while also showing whether the aggregation was successful.
Additional aggregators only require the implementation of the constructor and aggregate function with the predetermined input format.

\paragraph*{Arithmetic Mean}
We provide the models for the arithmetic mean by bringing the model parameters into matrix form, i.e., $\mathbb{R}^{d \times k}$ for d-dimensional parameters and k models.
The aggregate is computed along the row-axis of the matrix according to \autoref{eq:arithmean}

\paragraph*{Weighted Average}
The weighted average is computed as the weighted sum over the matrix rows.
We showcase two approaches to determine the set of weights $\vect{\lambda}$.
First we compute the likelihood $\log \mathcal{N}(\vect{\theta}^i; \hat{\vect{\theta}}, \vect{\Sigma})$, based on the maximum likelihood estimator for mean and covariance computed from all models and compute the normalized weights based on \autoref{eq:normll}.
Computing the likelihood is especially useful when we have additional information about $\mathcal{N}$, such as an improved estimate for the mean. 
Given this prior, we can then better assess the learner's models.

The second approach is based on dynamically computing the mean and variance of some scoring metric using Welford's algorithm.
Distributed learners exchange models and compute the classification score or likelihood on their observed data given external parameters.
\autoref{alg:welford} outlines this routine, where each vertex first sends its parameters to neighboring nodes and then computes and sends messages based on incoming parameters. 
Each node then updates the average score based on incoming messages.

\begin{algo}{Welford's Algorithm~\cite{welford1962note}}
\begin{algorithm}[H]
    \begin{algorithmic}[1]
        \REQUIRE Network Graph $N=(V_N,E_N)$ with models $m^v, v \in V_N$ \\
        \ENSURE Running Average $\tilde{\eta}^i$ and Variance $\tilde{\sigma}^i$ \\
        \COMMENT{Initialization}\\
        \FORALL{$v \in V_N$}
        \STATE{Send $\vect{\theta}^v$ to neighbors $\mathcal{N}(v)$}\\
        \STATE{$\tilde{\eta}^v \leftarrow 0$ }\\
        \STATE{$\tilde{\sigma}^v \leftarrow 0$}\\
        \STATE{$i^v \leftarrow 1$}\\
        \ENDFOR \\
        \COMMENT{Process score and messages for all nodes}
        \FORALL{$\vect{\theta}^{s \rightarrow t},\: s \in \mathcal{N}(t), \: s,t \in V_N$}
        \STATE{$S^{t \rightarrow s} = f(\vect{\theta}^s, \mathcal{D}^t)$}\\
        \ENDFOR
        \FORALL{incoming messages $S^{t \rightarrow s}$}
        \STATE{$i^s \leftarrow i^s + 1$}
        \STATE{$\tilde{\eta}^s,  \tilde{\sigma}^s \leftarrow$ Update($\tilde{\eta}^s, \tilde{\sigma}^s, i^s, S^{t \rightarrow s}$)}\\   
        \ENDFOR\\
        \COMMENT{All nodes send their average and variance score to the coordinator.}\\
        \RETURN{$\tilde{\eta}^s, \:\tilde{\sigma}^s /i^s$}
    \end{algorithmic}
    \caption[Decentralized Weight Estimation]{Weight determination for distributed learners using Welford's Algorithm. All nodes send their parameters to the set of neighbors(ll. 1-6). Then each node computes a score based on the received parameters and the local data (ll. 7-9) and returns the score to the original sender.
    Finally, the nodes monitor incoming messages and dynamically update mean and variance based on Welford's algorithm(ll. 10-13).}
    \label{alg:welford}
\end{algorithm}
\end{algo}

\begin{algo}{Update}
    \begin{algorithm}[H]
        \begin{algorithmic}[1]
    \REQUIRE $\tilde{\eta}^s, \tilde{\sigma}^s, i^s, S^{t \rightarrow s}$ \\
    \ENSURE  Updated average $\tilde{\eta}^i$ and variance $\tilde{\sigma}^i$ \\
    \STATE{$\Delta_{m} \leftarrow  S^{t \rightarrow s} - \tilde{\eta}^s$}\\
    \STATE{$\tilde{\eta}^s \leftarrow \tilde{\eta}^s  + \Delta_{m} / i^s$}\\
    \STATE{$\Delta_{v} =  S^{t \rightarrow s} - \tilde{\eta}^s  $}\\
    \STATE{$\tilde{\sigma}^i \leftarrow \tilde{\sigma}^i + \Delta_{m} \cdot \Delta_{v}$}\\
    \RETURN{$\tilde{\eta}^s, \: \tilde{\sigma}^i$}
\end{algorithmic}
\caption[Update procedure for the online mean and variance computation]{For an incoming message $S$ we compute the difference before and after updating the mean. The unnormalized variance is given by the sum of the previous variance and the product of both differences $\Delta_m, \Delta_v$.}
\label{alg:updWelford}
\end{algorithm}
\end{algo}

The coordinator then normalizes the weights (\autoref{eq:scorenorm}) and computes the weighted average as usual.

\paragraph*{Radon Machines}
While the concept of Radon Points allows us to compute the center point for a given set of points solving a system of linear equations, recall that we can not obtain a unique solution if the coefficient matrix is not full rank, i.e., is singular.
This problem is caused by two or more linear dependent columns or rows in the matrix.

As the number of samples seen by each device increases, the parameters approach each other to some extent, which in turn increases the likelihood of introducing linear dependencies between parameter vectors.
Since we assume the data to be identically and independently distributed (i.i.d.), all samples are from the same distribution. 
As the sample size increases, the local parameters vectors are going to approach each other.
Identical parameters vectors become an issue when solving systems of linear equations as linear dependencies cause the system to be overdetermined, which then has no unique solution anymore.
The probability of introducing such dependencies increases with the amount of data, such that the solution may not be unique anymore.
Instead of solving a system of linear equations, we can transform the problem into a least-squares minimization to find any non-unique solution, or the unique if it exists.

In that case, we check if the coefficient matrix from \autoref{eq:radonopt} is singular, and if it is, we switch to a least-squares optimization approach to obtain a feasible solution. 

In the non-singular case the system of linear equations with coefficients $A \in \mathbb{R}^{d+2 \times d+2}$, variables and constants $\vect{x}, \vect{b} \in \mathbb{R}^{d+2}$

\begin{equation}
    \label{eq:syseq}
    \begin{split}
        A\vect{x} &= \vect{b} \\
        \vect{x} &= (A^TA)^{-1}A^T \vect{b}
    \end{split}
\end{equation}
has a unique solution.
Instead of directly solving \autoref{eq:syseq} we obtain a solution by solving 

\begin{equation}
    \hat{\vect{x}} = \arg\min_{x} \norm{A\vect{x} - \vect{b}}^2_2,
\end{equation}
, which is a least squares minimization \wrt to the local model parameters, which are the columns of $A$ and constants $b$ from constraints introduced in \autoref{eq:radonopt}.

We then arrive at a unique solution for $\vect{x}$ on non-singular matrices using either approach or, if the matrix is singular, one solution to the problem using the least-squares minimization.
We will now move on to the experiments, setup, and empirical results evaluating the different aggregation methods.


\paragraph*{Bootstrap Aggregation}

The bootstrap aggregation takes advantage of generative modeling, which allows us to generate new samples from a distribution.
Here, we use Gibbs sampling (\autoref{alg:gibbs}) with 20 iterations to generate new samples.
We sample a total of $\sum_{i=1}^k \abs{\mathcal{D}^i} /2$ from each model, compute the sufficient statistics, and train a new model on the coordinator with the combined sample data.

\hypersetup{pdfborder=1 1 1}
Having discussed the implementation of all aggregation methods, we conclude the implementation chapter.
The implementation is available on \href{https://github.com/Xst4r/Distributed-PGM/tree/master/Code}{GitHub}.
We will now move on to the experiments and results and, finally, summarize our findings and discuss future work.
\hypersetup{hidelinks}