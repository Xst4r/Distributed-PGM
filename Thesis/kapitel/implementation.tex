% implementation.tex

\chapter{Implementation}
\label{chapter:ch4}
In this chapter, we will discuss the framework that was implemented, featuring the theoretical results from \chap~\ref{chapter:kap2} and aggregation methods presented in \chap~\ref{chapter:ch3} for probabilistic graphical models and exponential families.
The implementation features a simulated environment with distributed learners and the coordinator, who have shared access to some data structures, which are used for simulating the message exchange between both parties.
We are going to discuss implemented features, design choices as well as providing a general idea about data processing, model initialization, and code architecture.

The framework is implemented with Python as it is highly accessible and features C-based packages for efficient linear algebra operations and data processing.
This enables fast and efficient execution and evaluation or most standard operations, while also preserving the option to interface to C and C++ libraries if necessary for additional computational speedup.

We mainly use well-known python modules such as NumPy, SciPy, Pandas, and Scikit-Learn for data processing, matrix management, and evaluation.
Moreover, we use the PXPY C++ module for probabilistic graphical models and probabilistic inference.

Distributed learning is implemented in a simulated fashion, as the goal of this work is to present model aggregation techniques.
The simulation pattern consists of two objects, coordinator and learners, exchanging data on shared memory. 
The coordinator object has access to a model object, which holds the results for all distributed learners.
Depending on the aggregation method the coordinator may have access to additional data, such as the total number of distributed learners, parameter vectors, or likelihood. 
However, we strictly enforce that the local data or the average sufficient statistics are never used by the coordinator.

Moreover, in a real-world setting, the devices may terminate optimization at different times and send the parameters to the coordinator asynchronously. 
Here, we enforce the training to be synchronized, such that we only send the data to the coordinator once all models have finished training.

The implementation can be roughly divided into three phases, the initialization, run, and aggregation phase.
While the initialization phase is invoked once during startup, run and aggregation phase alternate every round starting with the run phase.
Let us now have a closer look at the code structure, data processing, and initialization process, which is executed at the program's start and before training the models. 

\section{Initialization Phase}
\input{kapitel/figures/flowchart.tex}
The initialization phase is executed at the program's start, loading the data, parameters, and creating mandatory objects such as the coordinator, models and aggregators.
\fig \ref{fig:flowchart} shows the initial setup.
First, for a data set $\mathcal{D}$ we create a data set object, which discretizes the data and creates the desired number of cross-validation splits. 
For easier accessibility, the implementation features a basic framework for directly downloading and extracting datasets when providing a hyperlink to the data server.
Thereafter, we create a coordinator object, which has access to the data set, a sampler object, and the aggregators.
The sampler is responsible for creating random index-based splits, which are then sent to the models providing the models access to the data set via index operations.
Finally, the graph-based independency structure $G=(V,E)$ is created on a holdout set, where the number of vertices is the number of features in $\mathcal{D}$ and the conditional independencies are generated using the Chow-Liu algorithm.
Both, individual splits and the graph are then used to create distributed models, which then begin to alternate between run and aggregation phase (cf. \fig~\ref{fig:dist}).

After the discretization, each random variable $X_i$ with state-space $\mathcal{X}_i$ has at most $\{0, \ldots, \max(\mathcal{D}_{\cdot i})\}$ observable states.
Since we discretize the data before determining the independency structure we obtain the number of states for each feature by simply taking the maximum among all observations from the corresponding feature. 

Having prior information about structure and state-space is vital as the aggregation methods used for this work rely on having an identical structure and known state-space across the distributed learners.

\subsection{Cross-Validation}
The cross validation sets are created along the data set object by a seeded randomization.
An index is created over the data, which is then shuffled and split into the desired number of cross validation runs.
For each run one of the sets is considered the test data, while the other remaining sets are used for training.
The training data is again shuffled and distributed by a secondary index set to all distributed learners.

\subsection{Discretization}
Most datasets contain features with a real valued range, which either leads to non discrete problems or greatly increases the model complexity.
By discretizing the data we can group observations into a fixed number of bins, which in turn also reduces the number of parameters.
Each sample that falls into the range of a bin is then mapped to the index of that bin, e.g. for two bins having ranges of $[0,10),[10,20)$, a sample $x=5$ would be assigned to the first bin and thus be mapped to $1$. Observations outside the original intervals are assigned to the 0-th and q-th bin respectively.
We discretize the data using the $q$ quantiles on each feature, which are computed based on the training data. 
The number of quantiles $q$ can either be chosen to be the same across all features or for each feature individually.
Furthermore, if the number of quantiles is larger than the state-space for any random variable $X_i$ we do not compute the discretized feature as the number of possible observations is less than $q$.
Afterwards, we use the same intervals to discretize the training data.
This is essentially a mapping function, which maps observations of a single feature to interval borders, i.e., given a set of intervals we assign each observation to either the left or right border of the interval it is contained in.
Choosing the correct number of quantiles is difficult and requires prior knowledge about the data.
We may choose the number of quantiles based on the total range of each feature or based on some prior information about a feature such as what physical measurement or unit it represents.

\section{Run Phase}
After creating all necessary objects, run and aggregation phase start alternating, where the available data on the learners increases each round.
\fig~\ref{fig:dist} illustrates the sequence of operations.
First each of the distributed learners trains a probabilistic graphical model with the locally available data.  
Then, the coordinator receives parameters from the distributed learners $\vect{m}$, which are then aggregated. 
Optionally we send return the aggregate to the distributed learners, which can then be used as initial parameters for the upcoming round.

In practice we  only have a fixed amount of data available for each learner.
We simulate this by restricting access to the full local data, i.e., at each round t on the i-th learner  $n^{t}_{m^i}$ samples are available with $n^{t+1}_{m^i} = n^{t}_{m^i} + f(t+1)$, where $f(t)$ is a positive function or constant.
After each round we extend the local data set, i.e., given the full local data $\abs{\mathcal{D}_i} = n_{m^i}$  we choose the first  $ n^{t}_{m^i} \leq n_{m^i}$ elements from the data index $I^t =\{1, \ldots, n^{t}_{m^i}\} \subseteq I = \{1, \ldots, n_{m^i}\}$. 
Note, we do not allow duplicates, i.e. each index added is unique and only indices that are not yet in $I$ can be added each round.

\input{kapitel/figures/distributed_schema.tex}

Since preprocessing standardizes the data in such way that it can be used as direct input to the model, a single model class is sufficient for distributing and training most data sets.
In case that there we need to perform additional modifications to the training, the model class can be freely extended to suit the specific needs.
However, in our case a single model class for all data sets was sufficient.
Each model, when created, uses an instantiation of a data set class and features optional parameters to control the simulated distributed environment. 
This includes for example setting maximum number of iterations for each training pass and the number of distributed learner.
Each distributed learner has at its core an instance of a PXPY model, which is used for training and inference.

\subsection{PXPY}
PXPY\cite{piatkowski2018exponential} (PX) is a toolkit for performing probabilistic inference and parameter estimation using graphical models using exponential family distributions.
Originally implemented as a C++ module, PX also features a Python front-end easier access to core functionalities.
PX offers a variety of  model- and inference types such as markov random fields with real value or integer parameters , while also providing different parallelized inference algorithms such as belief-, loopy belief propagation or the junction tree algorithm.

Core features include the model- training, evaluation and inference, while providing means to influence the optimization with user-defined callbacks or functions hooks, that allow to manually add regularization terms or to check progress and convergence of the training.

Options are passed to via register using a virtual machine, which are then accessed by the C++ core to execute the algorithms with the provided options.

The independency structure can either be manually passed to PX as an edgelist or be inferred directly from the data with algorithms such as the Chow-Liu algorithm. 
Additionally, we may choose between several default graph structures such as chain, grid or star graphs.

Optimization is done via negative average likelihood minimization using first order proximal gradient methods (cf. Piatkowski \cite{piatkowski2018exponential} with Fast Iterative Shrinking-Thresholding Algorithm (FISTA)).
Functionality for gradient manipulation is provided by explicitly passing function hooks to PX, which are then called at the corresponding optimization step and execute the user defined function.
This may be used for example to add a regularization term to the gradient before updating the objective function. 
Here, PX calls the function that has been passed for regularization with a content structure, containing the current gradient, weights, step size and number of iterations which can then be directly manipulated with in-memory operations.
Likewise, we may also check if the objective value is close to converging between two gradient descent iterations and stop the optimization altogether, if desired.




\subsection{Training}
At the start of each training iteration we first increment the number of samples available on each distributed learner. 
We may then either update the previous model by extending the sufficient statistics with the new data or train a new model. 
In either case we  have to keep the graph $G$, state-space $\vect{\mathcal{X}}$, average sufficient statistics $\tilde{\mathbb{E}}_{\mathcal{D}^i}[\phi(x)] = \tilde{\vect{\mu}}^i$ and the previous parameters $\vect{\theta}^i$ in memory in order to update the i-th learner.

When creating a new PX model the state-space is usually inferred directly from the data, while the graph has to be passed either as an edgelist or as a PX graph object. 
Here we run into the issue of the state-spaces for distributed learners not being the same especially when the number of samples is low. 
When having only a small number of samples some states might not have been observed yet, which leads to an incomplete state-space and thus to a varying number of parameters. 
This issue is solved by providing px with an additional sample $\mathcal{D}'^i = \mathcal{D}^i \cup \vect{x}_{max}$ with $\vect{x}_{max} = (max(\mathcal{D}_{\cdot 1}), \ldots, max(\mathcal{D}_{\cdot m}))$, which contains the maximum observed (or discretized) value for each feature.
Since the state-space is created from the maximum value for each feature, this leads to all distributed learners having the same number of parameters.
Afterwards, we simply remove scale back the sufficient statistics such that
\begin{equation}
    \tilde{\vect{\mu}}^t_{m^i} = \frac{(n^t_{m^i} + 1) \cdot \tilde{\vect{\mu}}^{'i}_{m^i} - \phi(\vect{x}_{max})}{n^t_{m^{i}}},
\end{equation}
while keeping the number of parameters equal across all learners.
This way we ensure all models having the same structure and number of parameters even when some configurations have not been observed yet.

The actual parameter estimation is started by calling the train method of PX with the graph and data along with a maximum number of iterations and the progress hooks that shall be called from within PX.
On each gradient descent iteration we monitor the difference between current and previous likelihood and once the difference is sufficiently small we start a grace period of for example $t=100$ iterations after which the training is terminated as long as the difference remains below that threshold.

When using a regulariser on the likelihood we provide PX with an additional regularization hook, that  adds the derivative of the regulariser to the gradient by copying the sum of gradient and regulariser into the memory reserved for the gradient. 
For the $\ell_2$ regularization we for example have 
\begin{equation}
    \begin{split}
    \nabla \ell_R( \vect{\theta}; \mathcal{D}) &= \nabla_{\vect{\theta}} \ell( \vect{\theta}; \mathcal{D}) +  \nabla_{\vect{\theta}}\lambda R(\vect{\theta}) \\
    &=\nabla_{\vect{\theta}} \ell( \vect{\theta}; \mathcal{D}) +  2\lambda \vect{\theta},
    \end{split}
\end{equation} 
where $\lambda$ controls the impact of the regularization term, with $\lambda=0$ equating to an unregularised task.

Once the training terminates by either exceeding the maximum of iterations or by triggering the stop criterion, we store the local model parameters in a shared list which is accessible by the coordinator. 
The coordinator then accesses the parameters for the aggregation phase.


\section{Aggregation Phase}
Upon completion of the run phase the coordinator receives the updated local model parameters, which are used to create an aggregator instance.
The parameters for an arbitrary aggregator object are usually passed as $\mathbb{R}^{d \times k}$ matrix, where, again $d$ is the number of parameters for each model and $k$ is the number of models.


Prior to the aggregation we may optionally choose to sample additional models, i.e., parameter vectors from existing ones taking advantage of the asymptotic normality of the MLE (cf. \ref{ssec:asymp}).
This is especially useful for radon machines when we have close to $ r^h << k < r^{h+1}$ models, but strictly less than required for aggregating $r^{h+1}$ models for an arbitrary $h$.

Parameters of exponential families are asymptotically normal distributed as shown in \sect~\ref{ssec:asymp}.
Hence, we can center the sample distribution around $\vect{\theta}^i$ if we assume this to be the asymptotic result.
We then have $\hat{\vect{\theta}}^i \sim \mathcal{N}(\vect{\theta}^i, \cdot)$, where the covariance matrix is still needs to be determined in case we can not compute or store the full fisher information matrix.
The different choices for correlation matrices are derived from the results in \sect~\ref{ssec:asymp} and \sect~\ref{ssec:bounds}.
%We could use Wasserstein-2 Distance to measure distance between local distribution and true distribution

\subsection{Covariance Generation}
\label{ssec:covgen}

Recall, that we want to sample from $\vect{\theta} \sim \mathcal{N}(\vect{\theta}^*, \cdot)$ for some correlation matrix $\vect{\Sigma}$ under the assumption that $\vect{\theta}^*$ are the true parameters.
In this case we can use either the full covariance matrix $\vect{\Sigma}$, the diagonal $diag(\Sigma)$ or a single value $\sigma^2 I$.
Ideally, we want to compute the fisher information \wrt $\vect{\theta}^*$, but this is not always possible.

Instead, we determine the covariance matrix for each distribution in three different ways, while also having the option to skip sampling altogether.
While the uniform variance and the diagonal are only required to contain positive entries only, the full covariance matrix needs to satisfy some additional properties.

Covariance matrices are always positive semidefinite, symmetric and non-singular. 
Hence sampling all entries at random may almost definitely result in a matrix violating at least one of these constraints.
However, there are some algorithms and techniques that can be used to generate full covariance matrices.
First, we can sample a matrix $A\in \mathbb{R}^{d \times d}$ over an arbitrary distribution and perform a matrix multiplication with its transpose
\begin{equation}
    \Sigma = AA^T, \quad A_{ij}\sim\mathcal{U}(a,b), \; \forall \;0 < i,j \leq n,
\end{equation}
where $\mathcal{U}$ is a uniform distribution with minimum $a$ and maximum $b$.

The result is a positive semidefinite and symmetric matrix, while the matrix may still contain linear dependencies for some rows or columns.

Another approach originally introduced by Davies and Higham \cite{davies2000numerically} uses a set of positive eigenvalues to sample a correlation matrix from.
Given a set of positive eigenvalues $\vect{\lambda}\in \mathbb{R}^d$ where $\sum_{i=1}^n \lambda_i = d$ we can sample a random correlation $\rho$ with these eigenvalues
This can be used to randomly generate a set of positive eigenvalues, then normalizing them such that the sum is equal to the number of dimensions $d$ and use the resulting normalized eigenvalues to sample a fitting correlation matrix. 
We then obtain the covariance matrix by rescaling the components of the correlation matrix with the product of standard deviations from two components 
\begin{equation}
    \begin{split}
            \rho_{ij} &= \frac{\Sigma_{ij}}{\sqrt{\Sigma_{ii}\Sigma_{jj}}} \\
            \Sigma_{ij} &=  \rho_{ij}\sqrt{\Sigma_{ii}\Sigma_{jj}}.
    \end{split}
\end{equation}

While both variants mentioned above result in proper covariance matrices, they to not have an inherent relationship with the actual covariance matrix for the asymptotic distribution.
Recall that the true covariance matrix is the inverse fisher information matrix of the likelihood, i.e., the second order gradient $\nabla^2 A(\vect{\theta})$ of the likelihood \wrt $\vect{\theta}$.
While computation of the full fisher information matrix may not be feasible on the devices or even the coordinator, we can partially compute the components. 
For example the diagonal of the fisher information for some parameters $\vect{\theta}^j$ on a distributed learner is 
\begin{equation}
    diag(i(\vect{\theta}^j)) = \mathbb{E}[\phi(X)] - \mathbb{E}[\phi(X)]^2,
\end{equation}
which for binary random variables is just the difference between the marginal probability of observing a state and its squared value.
For distributions over local models we add an error to the diagonal of the variance, i.e.,
\begin{equation}
    \tilde{\Sigma}=  \frac{1}{\abs{\mathcal{D}} \cdot diag(i(\vect{\theta}^j))} + \epsilon,
\end{equation}
where $\epsilon$ is based on the bounds derived in \sect~\ref{ssec:bounds}.

Moreover, we may compute entries on the diagonal blocks, that is, entries between sufficient statistics from the same clique:
\begin{equation}
    i(\vect{\theta})_{C_{ij}} = 0- \sum_{\vect{x} \in \vect{\mathcal{X}}} \mathbb{P}_{\vect{\theta}}(X= \vect{x}) \phi_C(\vect{x})_i \sum_{\vect{x} \in \vect{\mathcal{X}}} \mathbb{P}_{\vect{\theta}}(X= \vect{x}) \phi_C(\vect{x})_j= - p_i p_j,
\end{equation}
where $ \mathbb{E}[\phi(\vect{X})_i \phi(\vect{X})_j]$ (cf. \eq~\ref{eq:sparsefish}) is always zero as the probability of observing two events simultaneously in a single clique is zero.

Since we are introducing an additional variance between parameter of distributed learners we can quantify this error and add it to the variance terms.
As addition of positive values to the diagonal preserves the properties of a covariance matrix, we may add an additional error term to the diagonal while preserving positive semidefiniteness, symmetry and non-singularity..

Diagonal matrices such as $diag(\Sigma)$ for some $\Sigma_{ii} > 0, \; \forall 1 \leq i \leq d$ are always positive semidefinite and invertible unless $det(\Sigma) = \prod_{i=1}^n \Sigma_{ii} \approx 0$ becomes numerically too small to represent on the available hardware architecture. 
If this is the case we can apply a matrix regularization $diag(\Sigma) + \lambda I$ for a tradeoff in precision with for example , where $I$ is the identity matrix and $\lambda > 0$.

In summary, we present three different ways of generating full covariance matrices $\Sigma$ or simplifications thereof using only the diagonal $diag(\Sigma)$ or a single variance term $\sigma^2$ for all distribution components.
\subsection{Aggregators}

The aggregator interface provides the predefined functions for implementing additional aggregators.
The models to be aggregated are first passed to the constructor along with additional used-defined parameters, that are necessary for the aggregation.
We consider the input models to be either in matrix for or a list of PXPY model objects from which we can extract the parameters.
Then, the aggregate function is called, which aggregates the models and stores the result in a separate variable inside the object instance.
Additional aggregators only require the implementation of the constructor and aggregate function with the predetermined input format.

\paragraph*{Arithmetic Mean}
We provide the models for the arithmetic mean by bringing the model parameters into matrix form, i.e., $\mathbb{R}^{d \times k}$ for d-dimensional parameters and k models.
The aggregate is computed along the row-axis of the matrix according to \eq~\ref{eq:arithmean}

\paragraph*{Weighted Average}
The weighted average is computed as the weighted sum over the matrix rows.
The set of weights $\vect{\lambda}$ is determined by two approaches.
First we compute the likelihood $\log \mathcal{N}(\vect{\theta}^i; \hat{\vect{\theta}}, \vect{\Sigma})$, based on the maximum likelihood estimator for mean and covariance computed from all models and compute the normalized weights based on \eq~\ref{eq:normll}.
This is especially useful when we have additional information about $\mathcal{N}$ such as an improved estimate for the mean. 
Given this prior, we can then better appraise the learner's models.

The second approach is based on dynamically computing mean and variance of some scoring metric using Welford's algorithm.
Distributed learners exchange models and compute the classification score or likelihood on their own observed data given external parameters.
\alg~\ref{alg:welford} outlines these steps, where each vertex first sends its parameters to neighboring nodes and then computes and sends messages based on incoming parameters. 
Each node then updates the average score based on incoming messages.

\begin{algo}{Welford's Algorithm~\cite{welford1962note}}
\begin{algorithm}[H]
    \label{alg:welford}
    \begin{algorithmic}[1]
        \REQUIRE Network Graph $N=(V_N,E_N)$ with models $m^v, v \in V_N$ \\
        \ENSURE Running Average $\tilde{\eta}^i$ and Variance $\tilde{\sigma}^i$ \\
        \COMMENT{Initialization}\\
        \FORALL{$v \in V_N$}
        \STATE{Send $\vect{\theta}^v$ to neighbors $\mathcal{N}(v)$}\\
        \STATE{$\tilde{\eta}^i \leftarrow 0$ }\\
        \STATE{$\tilde{\sigma}^i \leftarrow 0$}\\
        \STATE{$i^v \leftarrow 1$}\\
        \ENDFOR \\
        \COMMENT{Process score and messages for all nodes}
        \FORALL{$\vect{\theta}^{s \rightarrow t},\: s \in \mathcal{N}(t), \: s,t \in V_N$}
        \STATE{$S^{t \rightarrow s} = f(\vect{\theta}^s, \mathcal{D}^t)$}\\
        \ENDFOR
        \FORALL{incoming messages $S^{t \rightarrow s}$}
        \STATE{$i^s \leftarrow i^s + 1$}
        \STATE{$\tilde{\eta}^s,  \tilde{\sigma}^s \leftarrow$ Update($\tilde{\eta}^s, \tilde{\sigma}^s, i^s, S^{t \rightarrow s}$)}\\   
        \ENDFOR\\
        \COMMENT{All nodes send their average and variance score to the coordinator.}\\
        \RETURN{$\tilde{\eta}^s, \:\tilde{\sigma}^s /i^s$}
    \end{algorithmic}
    \caption[Decentralized Weight Estimation]{Weight determination for distributed learners using Welford's Algorithm. All nodes send their parameters to the set of neighbors(ll. 1-6). Then each node computes a score based on the received parameters and the local data (ll. 7-9) and returns the score to the original sender.
    Finally, the nodes monitor incoming messages and dynamically update mean and variance based on Welford's algorithm(ll. 10-13).}
\end{algorithm}
\end{algo}

\begin{algo}{Update}
    \begin{algorithm}[H]
        \label{alg:updWelford}
        \begin{algorithmic}[1]
    \REQUIRE $\tilde{\eta}^s, \tilde{\sigma}^s, i^s, S^{t \rightarrow s}$ \\
    \ENSURE  Updated average $\tilde{\eta}^i$ and variance $\tilde{\sigma}^i$ \\
    \STATE{$\Delta_{m} \leftarrow  S^{t \rightarrow s} - \tilde{\eta}^s$}\\
    \STATE{$\tilde{\eta}^s \leftarrow \tilde{\eta}^s  + \Delta_{m} / i^s$}\\
    \STATE{$\Delta_{v} =  S^{t \rightarrow s} - \tilde{\eta}^s  $}\\
    \STATE{$\tilde{\sigma}^i \leftarrow \tilde{\sigma}^i + \Delta_{m} \cdot \Delta_{v}$}\\
    \RETURN{$\tilde{\eta}^s, \: \tilde{\sigma}^i$}
\end{algorithmic}
\caption[Update procedure for the online mean and variance computation]{For an incoming message $S$ we compute the difference before and after updating the mean. The unnormalized variance is given by the sum of the previous variance and the product of both differences $\Delta_m, \Delta_v$.}
\end{algorithm}
\end{algo}

The coordinator then normalizes the weights (\eq~\ref{eq:scorenorm}) and computes the weighted average as usual.

\paragraph*{Radon Machines}
While the concept of Radon Points allows us to compute the center point for a given set of points solving a system of linear equations we can not obtain a unique solution if the coefficient matrix is not full rank, i.e., is singular.
This is mainly caused by two or more linear dependent columns or rows in the matrix.

As the number of samples seen by each device increases the parameters approach each other to some extent, which in turn increases the likelihood of introducing linear dependencies between parameter vectors.
Since we assume the data to be identically and independently distributed (i.i.d.) all samples are from the same random variable and with a large enough sample size the local parameters vectors are going to equalize.
This becomes an issue when solving systems of linear equations as linear dependencies cause the system to be overdetermined, which then has no unique solution anymore.
With the probability of introducing such dependencies increasing with the amount of data the solution to such a system is not unique anymore.
Instead of solving a system of linear equations we can transform the problem into a least squares minimization, which can be used to find any non-unique solution, or the unique if it exists.

In that case we check if the coefficient matrix from \eq~\ref{eq:radonopt} is singular and if it is we switch to a least squares optimization approach to obtain a valid solution. 

In the non-singular case the system of linear equations with coefficients $A \in \mathbb{R}^{d+2 \times d+2}$, variables and constants $\vect{x}, \vect{b} \in \mathbb{R}^{d+2}$

\begin{equation}
    \label{eq:syseq}
    \begin{split}
        A\vect{x} &= \vect{b} \\
        \vect{x} &= (A^TA)^{-1}A^T \vect{b}
    \end{split}
\end{equation}
has a unique solution.
Instead of directly solving \eq~\ref{eq:syseq} we may obtain a solution by solving

\begin{equation}
    \hat{\vect{x}} = \arg\min_{x} \norm{A\vect{x} - \vect{b}}^2_2,
\end{equation}
instead, which is a least squares minimization \wrt to the local model parameters, which are the columns of $A$ and constants $b$ from constraints introduced in \eq~\ref{eq:radonopt}.

We then reach a unique solution for $\vect{x}$ on non-singular matrices using either approach or, if the matrix is singular, one solution to the problem using the least squares minimization.
We will now move on to the experiments, setup and empirical results evaluating the different aggregation methods.

\paragraph*{Bootstrap Aggregation}

The bootstrap aggregation takes advantage of generative modeling, which allows us to generate new samples from a distribution.
Here, we use Gibbs sampling (\alg~\ref{alg:gibbs}) with 20 iterations to generate new samples.
We sample a total of $\sum_{i=1}^k \abs{\mathcal{D}^i} /2$ from each model, compute the sufficient statistics and train a new model on the coordinator with the combined sample data.

This concludes the implementation chapter.
The implementation is available on \href{https://github.com/Xst4r/Distributed-PGM/tree/master/Code}{GitHub}.
We will now move on to the experiments and results and finally summarize our findings and discuss future work.