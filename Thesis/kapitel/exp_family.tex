% kapitel2.tex
\chapter{Background}
\label{chapter:kap2}

\section{Notation}
\label{sec:nota}
Let us first define our notation. 
Random Variables will be noted as $X$ and vectors will be written in bold letters, e.g. a vector of random variables $\vect{X} = \{X_1, X_2, \ldots, X_m\} \in \mathbb{R}^m$.
The state space of a random variable will be denoted as a cursive letter of that random variable $\mathcal{X}$ and the statespace for  a random vector likewise as $\vect{\mathcal{X}}$.
Parameters of a probability density function in the context of the canocical exponential family will usually be noted as $\vect{\theta} \in \mathbb{R}^n, \quad \mathbb{N}^+$.
Here $\mathbb{N}^+$ is defined as $ \mathbb{N}^+ = \mathbb{N} \setminus \{0\}$.
Model parameters on the i$^{th}$ local device will be noted as $\vect{\theta^i}$, while $\vect{\theta_i}$ is the entry at the i$^{th}$ position of the parameter vector.
A multivariate gaussian normal distribution for example has parameters of the form $\vect{\theta} = (\vect{\mu}, \vect{\Sigma})$, with mean $\vect{\mu} \in \mathbb{R}^{n}$ and covariance matrix $\vect{\Sigma} \in \mathbb{R}^{n \times n}$.
Additional notations for model parameters will be introduced if necesesary.
\todo{More generally any variable denothed with \* is optimal, \^ is global and \~ is local}
In the context of optimization and optimality we assume $\vect{\theta}^*$ to be the true, unknown parameter vector.
The global parameters vectors, i.e., the parameters obtained by a global model, which has access to all available data is denoted as  $\vect{\hat{\theta}}$.
Finally, $\vect{\tilde{\theta}}$ denotes any local parameter vector and $\vect{\tilde{\theta}}^{(i)}$ the i$^{th}$ local parameter vector respectively.

\section{Probabilistic Graphical Models}
\label{sec:pgm}
When dealing with statistical (bayesian) models our goal usually is to estimate a set of parameters of a predetermined distribution that reduces the risk of predicting the wrong outcome (Risk Minimization). 
This is usually done by maximizing the Likelihood of the parameters having generated the data set $\mathcal{D}$ (see \ref{ssec:train}).

Let $\vect{X}$ be a random vector as defined in secition \ref{sec:nota}. 
Each entry in $X_i \in \vect{X}$ corresponds to a single random variable.
The most simple statistical models model each random variable individually while not accounting for dependencies between two or more variables.
This leads to worse estimates as the model can not capture the full scope of the underlying distribution.
With Probabilistic Graphical Models we can capture the underlying dependencies between these random variables in order to create a better estimate for the assumed distribution $\mathcal{D}$ adheres to.

When estimating parameters of a distribution we may consider each feature a random variable and the data generated a result of a stochastic process based on this random variables.
Probabilistic Graphical Models (PGM) are a powerful tool to model conditional independencies between random variables. 
We express each random variable as a node in a graph and each edge between two nodes represents a dependency. 
With the exception of trees and graphs with bounded treewidth performing exact inference in such a Graph is NP-Hard.
This is a result from having to compute the marginal distribution, i.e., summing over all possible states of all maximum cliques inside the graph.
However, methods exist, such as loopy belief propagation to perform approximate inference.
We may also apply a junction tree algorithm to transform the graph into a tree, where exact inference is possible in polynomial time.

When considering probabilistic graphical models we usually utilize parametric distributions, that are part of the exponential family. 
Our goal is to estimate the parameters of such a distribution, which optimizes some criterion e.g. Maximum Likelihood or Maximum Entropy.


After estimating the parameters for each distribution and device, we employ an aggregation mechanic to create a composite model.
Distributed integer probabilistic graphical models \cite{piatkowski2019distributed} have recently been considered by Piatkowski.
In this specific scenario the models were aggregated using the average over all local models.
Other existing aggregation mechanics are discussed in section.
Additionally we need to define a stopping criterion where every device will stop sending messages to other devices, which means we require some kind of convergence.
This is usually the case when most devices have a similar local model.
In the following we will consider existing work on that specific research area and then lay out the necessary steps and challenges for the thesis.

Probabilistic Graphical Models are usually represented as a Graph, i.e., a set of vertices and edges that represent the independency structure of $\vect{X}$.
Each vertex corresponds to a random variable and each edge is a dependency between two variables.
Formally, let $G = (\vect{X},E)$ be a Graph with $\vect{X} =\{X_1 \ldots, X_n\}$, $\vect{\mathcal{X}} = \{\mathcal{X}_1, \ldots, \mathcal{X}_n\}$ and $E \subseteq \vect{X} \times \vect{X}$

\subsection{Markov Random Fields}

A Markov Random Field is a probabilistic graphical model to represent the joint probability of a random vector $\vect{X}$. 
Given a set of special properties, the Markov Properties we can express the joint probability distribution in terms of cliques in the graph.

\subsubsection*{Markov Properties}
Probabilistic graphical models are considered Markov Random Fields if the Graph is undirected and the three markov properties hold for any set of random variables indexed by the vertices of G $\vect{X} = \{X_v\}, v \in V$

\paragraph*{Pairwise Makrov Property}
\paragraph*{Local Markov Property}
Given a random variable $X_v$ it is independent of all other variables given its neighbours 
$X_v \independent X_{V\setminus (v \cup \mathcal{N}(v))} \lvert X_{\mathcal{N}(v)}$
\paragraph*{Global Makrov Property}

\paragraph*{}
\paragraph*{Clique Factorization}

A clique is a subset of nodes $C \subseteq V$ such that, $\forall u,v,  \in C, u \neq v: \exists (u,v) \in E$ there exists and edge in $E$ for all nodes in $C$, i.e., the subset is fully connected.
Now let $\mathcal{C}$ be set set of maximal cliques, i.e., the set of cliques that are not included in any other clique or formally 
\begin{equation}
    \forall C_i, C_j \in \mathcal{C}, i \neq j  : C_i \not\subset C_j, C_j \not\subset C_i, C_i \neq C_j
\end{equation}
there exists no such pair of cliques in $\mathcal{C}$ such that either one is a true subset or the other.
This is not to be confused with the maximum clique, which is the largest clique of the graph $G$, i.e. $\max \mathcal{C} = \max \{\abs{\mathcal{C_1}}, \ldots, \abs{C_n}\}$ the clique with maximum cardinality.

We can then express the joint probability distribution in terms of the set of maximal cliques, by treating each clique as a local probability density function and then factorizing over all maximal cliques. 
We assign a potential function $\psi_C: \mathcal{X}_C \leftarrow \mathbb{R}$ to the clique that maps any state of the clique $\vect{x}_C \in \mathcal{X}_C$ to a positive real valued number.
The clique factorization is then defined as:

\begin{equation}
    \mathbb{P}(\vect{X}=\vect{x}) = \frac{1}{Z} \cdot \prod_{C \in \mathcal{C}} \psi_C(\vect{x}_C)
\end{equation},
where $Z$ is the normalizer of the probability density function, $\mathbb{P}$ is a probability density function, i.e.:

\begin{equation}
    \sum_{\vect{x}\in \mathcal{\vect{X}}}   \frac{1}{Z} \prod_{C \in \mathcal{C}} \psi_C(\vect{x}_C)  = 1.
\end{equation}

The partition function $Z$ is therefore defined as sum over factorizions over all possible states:
\begin{equation}
    Z = \sum_{x\in \mathcal{X}} \prod_{C \in \mathcal{C}} \psi_C(\vect{x}_C)
\end{equation}

\begin{equation}
    \psi_C(\vect{x}_c) = \exp(\sum_{i=1}^n \theta_i \cdot \phi(\vect{x})_i)
\end{equation}


\subsection{Exponential Families}
\label{ssec:expf}

We will now introduce a convenient property of parameters from canonical exponential families, asymptotic normality of the maximum likelihood estimator. 

The canonical exponential family is defined as 
\begin{equation}
    p(\vect{x} \lvert \vect{\theta}) = \frac{1}{Z(\vect{\theta})} \prod_{c \in \mathcal{C}} \psi_c(\vect{x})
\end{equation}

\subsubsection{Asymptotic Normality}
\label{ssec:asymp}
The Maximum Likelihood Estimator of canonical exponential families is consistent, regular and thus asymptotic normal i.e. $\sqrt{n}(\theta - \theta^*) \sim \mathcal{N}(0, i(\theta^*)^{-1})$.
Implying, that the paremeters follow a normal distribution with the  true parameter vector as mean an the inverse fisher information as covariance, with 

\begin{equation}
    i(\theta^*) = \frac{\partial}{\partial^2 \vect{\theta}} p(x \lvert \vect{\theta})
\end{equation}

being the fisher information matrix.

We can interpret this result in such way that a set of parameter vectors obtained from data sets $\{\mathcal{D}_1, \ldots \mathcal{D}_n\}$ is normally distributed around the true parameter vector for a sample size of $n \leftarrow \infty$. 
This in turn implies that we can treat parameter vectors of local devices as samples from such a distribution and use this to estimate the maximum likelihood of the parameter space.
Addtionally we can use some $\vect{\theta}$ and assume this to be $\theta^*$ and use the result for sampling additional models.

\subsection{Structure Learning}
    Determining the Structure for the graph is important as we want to model the actual dependencie
    between random variables. 
    We may decide to drop dependencies in order to simplify the model, which allows for faster and exact training(in case the graph is a tree).
    
    We will use the Chow-Liu algorithm to approximate a independency structure, which always results in a tree.
    The algorithm computes the cross entropy for every pair of nodes and then computing the minimum spanning tree to 
    obtain the edges with the largest correlation.
    This limits the clique size in the graph to two, resulting in a tree, but also limiting the number of nodes that depend on each other.

    In this work the Chow-Liu algorithm is used for all independency structure approximations.
\subsection{Training}
\label{ssec:train}

Exponential Family is family of parametrized densities
Parameters have to be estimated
Maximum Likelihood Estimation
Given Data maximize Likelihood of Data being generated by distribution
Maximization over Distribution Parameters 
Negative Average Log Likelihood to prevent numerical errors
In general no closed form solution
Derivative is average sufficient statistics + expectation of $\vect{X}$
Log Partition is P-Complete, however solvable in poly time for trees.

optimization via Belief Propagation -> Inference
\subsection{Inference}
\label{ssec:inf}

Computing Clique Marginals
Belief Propagation
Exact for Trees 
Loopy Belief Propagation for non-tree Graphs

Since we are using the Chow-Lio algorithm to approximate the independency structure it is sufficient to use Belief Propagation for inference as this is guaranteed to be optimal and deterministic on trees.

\section{Distributed Learning}
There are mainly two different architectural approaches to distributed learning. 
The first approach is a decentralized approach without coordinator.
Communication between nodes is allowed and necessary to transmit some amount of data to reach an overall conclusion to the learning task.
The other approach involves a centralised coordinator node that broadcasts intermediate results or correction terms to local nodes.
The nodes in this approach send their data to the coordinator. 
Hybrid-Methods, where both types of architerure are mixed are also possible.
This work will focus on the second type, while also giving some theoretical insights to the decentralized approach.

Moreover the data may be horizontally or vertically distributed, i.e., either all features are available on each device, but only a small portion of the data or each device has access to all data of a subset of features.
Sensor Networks are an example for the latter, where each sensor may only generate one or more features such as temperature or magnetic field strength.

The key questions we have to ask ourselves in a distributed settting with horizontally distributed data are:
\begin{itemize}
    \item How much data is sufficient to obtain a proper estimate for the model on each device ? 
    \item When, how much and what do we communicate either between local devices or the coordinator ?
    \item What are we allowed to communicate ? (in terms of privacy preservation on user devices)
\end{itemize}

Depending on the task we may have to restrict communication, memory and processor consumption to an absolute minimum.
Here, statistical models have one major advantage over other models, that is they require less disk space to store and process as only certain statistics need to be preserved.
These are generally the parameters of the distribution we provide.
With markov random fields using a probability distribution from the exponential family we do not even have to store the data in memory has we only have to preserve the sufficient statistics for training and inference.

The two major issues we have to deal with are therefore the sample complexity and the model aggregation.
As the main focus of this work is the model aggregation, which will be featured in chapter \ref{chapter:ch3}, we will now briefly discuss sample complexity.

\subsection{Sample Complexity}
One of the key questions of distributed learning is the number of samples required to obtain local models that are sufficiently good, i.e., when to stop learning and either start communication with the coordinator
or to terminate the algorithm and produce a final aggregate model. 
The problem to find the number of samples required is known to be hard \todo{cite} and as such no exacty solution to this problem exists.
However, an upper bound on the sample complexity is given by the Hoefding-Bound. 
We apply the Hoefding-Inequality on the pairwise difference between average sufficient statistics of local models.

The Hoefding-Inequality is given by
\begin{equation}
    P(\vect{\bar{X}} - \mathbb{E}[\vect{\bar{X}}] \geq t ) \leq \exp^{-2\abs{\mathcal{D}]t^2}},
\end{equation}

where for some constant $t$ the probability of the difference between the empirical mean of a random variable and its expectation is begin greater than $t$ is negatively exponential bounded by the amount of data seen.

It can be shown that for $t = \frac{\sqrt{(c+1) \log d}}{2\abs{\mathcal{D}}}$ the difference between two average sufficient statistics $\vect{\mu}^i = \frac{1}{\mathcal{D}_i} \sum_{x \in \mathcal{D}} \vect{\phi(x)}, \quad \vect{\phi(x)} \in \mathbb{R}^d$ of a probabilistic graphical model is bounded by
\begin{equation}
    \norm{\mu^{i} -  \mu^{j}}_\infty \leq 2 \sqrt{
        \frac{(c+1) \log d}
        {2\abs{\mathcal{D^{'}}}}
        } = \epsilon
\end{equation},
with probability of at least $\delta= (1- 2 \exp(-c \log d))$. Here $D^{'} = \min(\abs{\mathcal{D}^i}, \abs{\mathcal{D}^j})$.
The proof is provided by Piatkowski \cite{piatkowski2019distributed}. \todo{Maybe put this in appendix}
Addtionally, as the average sufficient statistics approach each other so do the model parameters to some degree as shown by \todo{CITE}.

As the model parameters approach each other, we can show by Popoviciu's Theorem \cite{popoviciu1935equations} 
\begin{threm}{Upper Bound on the Variance}
    Let $\vect{X}$ be a bounded random variable with $\inf(\vect{X}) = b$ and $\sup(\vect{X}) = a$, 
    then the variance $Var[\vect{X}] = \sigma^2$ is bounded by 
    \begin{equation}
        \sigma^2 \leq \bigg(\frac{a-b}{2}\bigg)^{2}
    \end{equation}
\end{threm}

For the interested reader we refer to Sharma et al. \cite{sharma2010betterbounds} for more information about upper and lower bounds for variances.

\input{kapitel/proofs/popoviciu.tex}

It follows that
\begin{equation*}
    Var[\vect{X}] \leq f(\frac{a-b}{2}) \leq  \frac{1}{4} \cdot \mathbb{E}\bigg[\big((\vect{X} - b) - (\vect{X} - a)\big)^2\bigg]  =  \bigg(\frac{a-b}{2}\bigg)^2 
\end{equation*}\qed

Recall that $\norm{\vect{\mu}^{i} -  \vect{\mu}^{j}}_\infty \leq  \epsilon$, where $\epsilon$ provides the largest difference between two entries of the average sufficient statistics hence let $a - b = \mu^{i}_k - \mu^{j} = epsilon$.
This means that there exists no larger difference between the two vectors resulting in an upper bound for the variance of the average sufficient statistics with $a - b = \epsilon$ 
\begin{equation*}
    Var[\vect{\mu}^{i}_k -  \vect{\mu}^{j}_k] \leq \frac{\epsilon^2}{4} \quad \forall i,j,k
\end{equation*}.
Given two average sufficient statistics we can employ the Hoefding Bound to guarantee the distance between these two to be smaller than some $\epsilon$ with probability $\delta$


As the amount of data on each device increases we can also measure how close the sufficient statistics are with probability $\delta$ by rearranging the above equation. 
This is the most common scenario in distributed learning. As time goes on the amount of data increases allowing us to continuously measure the distance and variance between the sufficient statistics.
Once we reach our predetermined goal on a device, i.e. $\abs{\mathcal{D}}^^i$ is sufficiently large to satisfy our bound we stop collecting more data send the parameter vector to the coordinator node.

\todo{Equation}

We see that as $\abs{\mathcal{D}}$ increases, for a fixed $\delta$ the distance $\epsilon$ decreases and so does the variance. 

\todo{Maybe Plot eps and delta in 3D}
\subsection{Convergence}
\section{Sampling}
\subsection{Perturb and MAP}
\subsection{Gibbs Sampling}
