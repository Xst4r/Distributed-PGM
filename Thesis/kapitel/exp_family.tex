% kapitel2.tex
\chapter{Background}
\label{chapter:kap2}
In this chapter we are going to introduce the necesesary background in in generative modelling, which in this case includes canonical exponential family distributions, markov random fields and the parameter estimations for these types of models.
Additionally, we will introduce the general notion of distributed learning and algorithms to sample from generative models.

As it is our goal to employ model aggregation on models obtain from a variety of devices, we first have to clarify how we actually obtain the models from observed data. 
Then we will set this result into the context of distributed learning as a mandatory step towards a unified aggregate, obtained from local devices.

However, let us first define some basic concepts and notation that is going to be used in this work.
\section{Notation}
    \label{sec:nota}
    Let us first define our notation we will be using throughout this work.
    First, let us introduce tools and notations frequently encountered in probability theory and machine learning.
    

    \begin{definition}{Random Variable}
        \label{def:randvar}
        Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, where $\Omega$ is the domain, $\Omega$ is a $\sigma$-Algebra contaning all possible outcomes and $\mathbb{P}: \mathcal{F} \rightarrow [0,1]$ is a probability measure defined on all possible outcomes.
        A Random Variable $X: \Omega \rightarrow \mathcal{X}$ is a measurable function defined over all possible outcomes of the probability space that maps these to the domain $\mathcal{X}$.

        Parametrized probability measures will be denoted $\mathbb{P}_{\vect{\theta}} (\vect{X} = \vect{x})$, i.e., the proabability of $\vect{X}$ assuming some value $\vect{x}$.
    \end{definition}

    Random vectors will be written in bold letters $\vect{X} = (X_1, X_2, \ldots, X_m)$.
    The state space of a random variable will be denoted as a cursive letter of that random variable $\mathcal{X}$ and the domain of a random vector likewise as $\vect{\mathcal{X}} = (\mathcal{X}_1, \mathcal{X}_2, \ldots, \mathcal{X}_m)$.
    Samples from $\vect{\mathcal{X}}$ are usually written as lower case letters e.g. $\vect{x} \in \mathcal{X}^m$ for an m-dimensional state space $\vect{\mathcal{X}}$.
    We usually express each element of a random variable in terms of a subscript, e.g., $\vect{X}_i$ or $\vect{X}_A$, where $A \subset\{1, \ldots, m\} $ selects a subset of the dimensions.

    Then, let $\mathcal{D} = \{(\vect{x}^1, y^1), \ldots, (\vect{x}^n, y^n)\} \subset \mathbb{R}^m \times \mathbb{N}$ be a data set containing samples $\vect{x}^i \in \mathcal{X}^m$ from a multi-dimensional random variable $\vect{X}$ and label domain $y^i \in \mathcal{Y}$.
    Furthermore let $\mathcal{D}_A$ be a subset of samples for any indexing set $A$ and $\mathcal{D}_{\cdot A}$ be all available samples from a subset of features indexed by $A$.  

    Parameters of a parametrized probability density function in the context of the canocical exponential family will usually be noted as $\vect{\theta} \in \mathbb{R}^n$.
    Given a set of model parameters $\mathcal{M} = \{\vect{\theta}^1,  \ldots, \vect{\theta}^k\}$, each parameter vector $\vect{\theta}^i$ denotes the $i^{th}$ entry in the set, while $\vect{\theta_i}$ is the entry at position i$^{th}$ of a parameter vector.
    A multivariate gaussian normal distribution for example has parameters of the form $\vect{\theta} = (\vect{\mu}, \vect{\Sigma})$, with mean $\vect{\mu} \in \mathbb{R}^{n}$ and covariance matrix $\vect{\Sigma} \in \mathbb{R}^{n \times n}$.
    Additional notations for model parameters will be introduced if necesesary.
    In the context of optimization and optimality we assume $\vect{\theta}^*$ to be the true, unknown parameter vector.
    The global parameters vectors, i.e., the parameters obtained by a global model, which has access to all available data is denoted as  $\vect{\hat{\theta}}$.
    Finally, $\vect{\tilde{\theta}}$ denotes any local parameter vector and $\vect{\tilde{\theta}}^{(i)}$ the i$^{th}$ local parameter vector respectively.

    Now that we have defined so basic notation let us move on to probabilistic graphical models, exponential families and statistical inference.
\section{Probabilistic Graphical Models}
\label{sec:pgm}
When dealing with statistical models our goal is to estimate a set of parameters $\vect{\theta}$ of a probability distribution $\mathbb{P}_{\vect{\theta}}$, which optimize a function such as the risk or likelihood in order to perform tasks associated with that model.

%Conditional Independency
While in the case of statistical independency between all elements of a random variable, i.e.
\begin{equation}
    \mathbb{P}_{\vect{\theta}}(\vect{X} = \vect{x}) = \prod_{i=1}^m \mathbb{P}_{\vect{\theta}}(\vect{X}_i = \vect{x}_i)
\end{equation}
factorization over the marginal probabilities for each element is sufficient to obtain the joint probability, the assumption of statistical independency generally does not hold.
Hence, it is important to capture dependencies between random variables as accurate as possible, which in turn increases the complexity of a model.
In the general case no closed form solution for such models exists, but there are some exceptions, e.g. the random variables are only pairwise independent of each other.

\begin{definition}{Conditional Independency}
    Let $\vect{X}, \vect{Y}$ be two random variables. Two random variables are independent,  $\vect{X} \independent \vect{Y}$, iff
    \begin{equation}
        \begin{split}
        \forall \vect{x} \in \mathcal{X}, \vect{y} \in \mathcal{Y}:  \; \mathbb{P}(\vect{X} = x \lvert \vect{Y} = y) &= \frac{\mathbb{P}(\vect{X} = x , \vect{Y} = y)}{\mathbb{P}(\vect{Y} = \vect{y})} \\
        &=  \frac{\mathbb{P}(\vect{X} = x) \mathbb{P}(\vect{Y} = \vect{y})}{\mathbb{P}(\vect{Y} = \vect{y})}.  \\
        \end{split}
    \end{equation} 
    Meaning, that the joint probability is equal to the product of the individual probabilities for all elements of either random variable.
    We extend this definition by the notion of conditional independency. 
    Two random variables are called conditionally independent if the factorization is independent given a third random variable $\vect{Z}$, that is
    \begin{equation}
        \forall \vect{x} \in \mathcal{X}, \vect{y} \in \mathcal{Y}: \mathbb{P}(\vect{X} = x , \vect{Y} = y \lvert \vect{Z} = \vect{z}) =   \mathbb{P}(\vect{X} = x \lvert \vect{Z} = \vect{z}) \mathbb{P}(\vect{Y} = y \lvert \vect{Z} = \vect{z})
    \end{equation}
    which will be written as $\vect{X} \independent \vect{Y} \lvert \vect{Z} $
\end{definition}

% Capturing Dependencies
Let $\vect{X}$ be a random vector as defined in \sect \ref{sec:nota}. 
Each element $\vect{X}_i \in \vect{X}$ corresponds to a dimension of the random variable.
The most simple statistical models model each random variable individually while not accounting for dependencies between two or more variables.
This leads to worse estimates as the model can not capture the full scope of the underlying distribution.
With Probabilistic Graphical Models we can capture the underlying dependencies between these random variables in order to create a better estimate for the assumed distribution $\mathcal{D}$ adheres to.

%Graph representation
When estimating parameters of a distribution we may consider each feature a random variable and the data generated a result of a stochastic process based on this random variables.
Probabilistic Graphical Models (PGM) are a powerful tool to model conditional independencies between random variables. 
We express each random variable as a node in a graph and each edge between two nodes represents a dependency. 
With the exception of trees and graphs with bounded treewidth performing exact inference in such a Graph is NP-Hard~\cite{cooper1990computational}.
This is a result from having to compute the marginal distribution, i.e., summing over all possible states of all maximum cliques inside the graph.
However, methods such as loopy belief propagation exist to perform approximate inference in polynomial time.
Other approaches such as the junction tree approch firstt ransform the graph into a tree, allowing exact inference in polynomial time.

% Probability Dist. Factorization
Probability distributions from the exponential family can be expressed in a general form as product of a normalizer and some non-negative function $\psi: \mathcal{X}^m \rightarrow \mathbb{R}_+$ usually denoted as potential function:
\begin{equation}
    \label{eq:probpot}
    \mathbb{P}(\vect{X} = \vect{x}) = \frac{1}{Z} \psi(\vect{x}),
\end{equation}
where $Z$, the partition function ensures that the probability distribution properly sums to one.
If all entries in $\vect{X}$ are independent from each other we can write $\mathbb{P}(\vect{X} = \vect{x}) = \frac{1}{Z} \prod_{i=1}^m \psi_i(x_i)$ as the probability of a set of independent random variables is the product of each individiual probability.

\begin{example}{Normal Distribution}
    For a multivariate normal distribution with mean $\vect{\mu}$ and covariance matrix $\vect{\Sigma}$
    \begin{equation}
        \mathcal{N}(\vect{x}; \vect{\mu}, \vect{\Sigma}) = \frac{1}{\sqrt{2 \pi \abs{\Sigma}}} \cdot \exp\bigg(-\frac{1}{2} (\vect{x} - \vect{\mu})^T \Sigma^{-1}(\vect{x} - \vect{\mu}))\bigg)
    \end{equation}
    we can express the normal distribution in terms of partition and potential function as follows
    \begin{equation}
        Z = \sqrt{2 \pi \abs{\Sigma}}, \quad \psi(\vect{x}) = \exp\bigg(-\frac{1}{2} (\vect{x} - \vect{\mu})^T \Sigma^{-1}(\vect{x} - \vect{\mu}))\bigg).
    \end{equation}
    Here the partition $Z$ only depends on the covariance matrix and the potential function is expressed in terms of the model parameters $\vect{\mu}, \vect{\Sigma}$
\end{example}
%After estimating the parameters for each distribution and device, we employ an aggregation mechanic to create a composite model.
%Distributed integer probabilistic graphical models \cite{piatkowski2019distributed} have recently been considered by Piatkowski.
%In this specific scenario the models were aggregated using the average over all local models.
%Other existing aggregation mechanics are discussed in section.
%Additionally we need to define a stopping criterion where every device will stop sending messages to other devices, which means we require some kind of convergence.
%This is usually the case when most devices have a similar local model.
%In the following we will consider existing work on that specific research area and then lay out the necessary steps and challenges for the thesis.

%Capturing Statistical Independencies
However, in practice random variables are rarely fully independent of each other, while they also may not be completely dependent on all other random variables.
This leads to the notion of conditional independency, i.e., which variables are independent of each other if a certain set of variables is observed.
Conditional independencies between random variables can be represented in a convenient and interpretable form.
%TODO Rewrite
For a graph $G=(V,E)$ the components $\vect{X}_v$ of a random variable $\vect{X}$ are modeled as a node and conditional independencies are edges between these nodes.
Each vertex corresponds to a random variable and each edge is a dependency between two variables.
Formally, let $G = (\vect{X},E)$ be a Graph with $\vect{X} =\{X_1 \ldots, X_n\}$, $\vect{\mathcal{X}} = \{\mathcal{X}_1, \ldots, \mathcal{X}_n\}$ and $E \subseteq \vect{X} \times \vect{X}$

This leads to the formulation of Markov Random Fields, which is a Graph structure that represents the conditional independencies between random variables.

\subsection{Markov Random Fields}
%TODO Graph Representation
%MRF Defintion
Markov Random Fields are undirected graphs used to express conditional independencies between random variables. 
Each vertex of the graph is associated with a single random variable from $\vect{X}$ and connections, edges, between vertices represent the conditional independencies. 
Formally, let $G = (V,E)$ be a graph with vertices $V=\{1, \ldots m\}$ and $E \subseteq V \times V$. An undirected graph is a graph where vertices are connected in both directions $(u,v) \in E \Leftrightarrow (v,u) \in E$ if that edge exists.
Markov Random Fields are probabilistic graphical models, that represent the joint probability of a random variable $(\vect{X}_v), \; v \in V$ \wrt to the vertices and their conditional independencies through edges of the graph. 

For $\vect{X}$ to be considered a Markov Random Field \wrt G we following properties, called Markov Properties have to hold:

%MRF Properties
\subsubsection*{Markov Properties}
Probabilistic graphical models are considered Markov Random Fields if the Graph is undirected and the three markov properties hold for any set of random variables indexed by the vertices of G, $\vect{X} = \{X_v\}, v \in V$

\paragraph*{Pairwise Makrov Property}
Two random variables $X_u, X_v$ are independent if they are not adjacent i.e. $\not\exists (u,v) \in E$ and given all other random variables:
\begin{equation}
    X_u \independent X_v \lvert X_{V \setminus\{u,v\}}
\end{equation}

\paragraph*{Local Markov Property}
Given a random variable $X_v$ it is independent of all other variables given its neighbours 
\begin{equation}
    X_v \independent X_{V\setminus (v \cup \mathcal{N}(v))} \lvert X_{\mathcal{N}(v)},   
\end{equation}
where $\mathcal{N}: V \rightarrow V$ is the Markov Blanket of a node $v \in V$, i.e., the neighbors of $v$ such that $u \in \mathcal{N}(v) \Leftrightarrow (v,u) \vee (u,v) \in E$.

\paragraph*{Global Makrov Property}
Let $A, B \subset V, A \cap B = \emptyset$ be two subsets of vertices from G. 
These subsets are independent if, given a third subset $C$, there exists no path from $A$ to $B$ that contains unobserved variables, i.e., all paths contain at least one variable from $C$:

\begin{equation}
    X_A \independent X_B \lvert X_C
\end{equation}

Instead of expressing the probability in terms of a single probability  it can be shown that if $\vect{X}$ satisfies the conditions of a Markov Random Field we can break \eq~\ref{eq:probpot} into a factorization, i.e., a joint probability over cliques potentials in the graph.

\paragraph*{Clique Factorization}

A clique is a subset of nodes $C \subseteq V$ such that, $\forall u,v,  \in C, u \neq v: \exists (u,v) \in E$ there exists and edge in $E$ for all nodes in $C$, i.e., the subset is fully connected.
Now let $\mathcal{C}$ be set set of maximal cliques, i.e., the set of cliques that are not included in any other clique or formally 
\begin{equation}
    \forall C_i, C_j \in \mathcal{C}, i \neq j  : C_i \not\subset C_j, C_j \not\subset C_i, C_i \neq C_j
\end{equation}
there exists no such pair of cliques in $\mathcal{C}$ such that either one is a true subset or the other.
This is not to be confused with the maximum clique, which is the largest clique of the graph $G$, i.e. $\max \mathcal{C} = \max \{\abs{\mathcal{C}_1}, \ldots, \abs{C_n}\}$ the clique with maximum cardinality.

We can then express the joint probability distribution in terms of the set of maximal cliques, by treating each clique as a local probability density function and then factorizing over all maximal cliques.
Instead of using the full potential function from \eq~\ref{eq:probpot}, we can express the potential function as a factorization over clique potentials.
This allows us to break up the potentials into smaller distribitions and compute them individiually. 
We assign a potential function $\psi_C: \mathcal{X}_C \rightarrow \mathbb{R}$ to every clique in the graph.

\begin{equation}
        \mathbb{P}(\vect{X}=\vect{x}) = \frac{1}{Z} \cdot \psi(\vect{x})= \frac{1}{Z} \cdot \prod_{C \in \mathcal{C}} \psi_C(\vect{x}_C)
\end{equation},
where $Z$ is the normalizer of the probability density function, such that $\mathbb{P}$ is a proper probability distribution, i.e.

\begin{equation}
    \frac{1}{Z} \sum_{\vect{x}\in \mathcal{\vect{X}}} \prod_{C \in \mathcal{C}} \psi_C(\vect{x}_C) = 1 
\end{equation}

The partition function $Z$ is therefore defined as sum of all potentials
\begin{equation}
    Z = \sum_{x\in \mathcal{X}} \prod_{C \in \mathcal{C}} \psi_C(\vect{x}_C)
\end{equation}

\begin{equation}
    \psi_C(\vect{x}_c) = \exp(\inner{\vect{\theta}_C}{\phi_C(\vect{x})})
\end{equation}

\paragraph*{Sufficiency}
    For a function $\phi$ it is said to be a sufficient statistic of random variable $\vect{X}$ if the data is conditionally independet of the model parameters given $\phi$, that is iff $\vect{\theta}\independent \mathcal{D} \lvert \phi(\mathcal{D})$
    The choice of $\phi: \vect{\mathcal{X}} \rightarrow \mathbb{R}^d$ directly determines the shape of  $\mathbb{E}_{\mathbb{P}}[\phi(x)]$.
    Let $C \in \mathcal{C}(G)$, be a clique in the set of maximum cliques of a graph G and let further
    $\phi_C: \vect{\mathcal{X}_C} \rightarrow \mathbb{R}^{\abs{\mathcal{X}_C}}$ be the sufficient statistic for that clique.
    Then if the target variable is binary, i.e., $\phi : \mathcal{X}_C \rightarrow \{0,1\}^{\abs{\mathcal{X}_C}}$ it is easy to see that  $\mathbb{E}_{\mathbb{P}}[\phi_C(x_C)_i] = P(\phi_C(\vect{x_C})_i = 1)$, since
    \begin{equation}
        \mathbb{E}_{\mathbb{P}}[\vect{X}] = 0 \cdot \mathbb{P}(\vect{X} = 0) + 1 \cdot \mathbb{P}(\vect{X}= 1) = \mathbb{P}(\vect{X} = 1), \quad \mathcal{X} \in \{0,1\}
    \end{equation}.
    Thus, the expectation is equal to the probability that a clique is in state $i$.
    
    \subsection{Exponential Families}
    \label{ssec:expf}
    One important result is the definition of the canonical exponential family as the distribution best suited to fit the observed data.
    Using the Shannon Entropy we find that

    \begin{equation}
        P(X) = \exp^{(\inner{\vect{\theta}}{\vect{\phi_i(x)}} - A(\theta))},
    \end{equation}
    is the distribution that maximizes the entropy.

    Let us start with the definition of Shannon Entropy and why it is necessary to introduce some notion of optimality when it comes to finding a distribution that fits our observed data $\mathcal{D}$.
    The approach presented here follows the intuition featured by Wainwright and Jordan \cite{wainwright2008graphical}.

    We want to find the distribution $P$ from the space of all probability distributions $\mathcal{P}$, that fits the data  $\mathcal{D}$ the most.
    However, the problem is that there are many such distributions $P \in \mathcal{P}$ that satisfy the following condition. Let $\phi: \vect{\mathcal{X}}  \rightarrow \mathbb{R}^d$ be some function that maps observations to some real-valued vector, we then want to find a distribution such that:
    \begin{equation}
        \label{eq:expecval}
        \mathbb{E}_P[\phi_d(X)] = \tilde{\mathbb{E}}_{\mathcal{D}}[\phi_d(X)], 
    \end{equation}
    for every feature, i.e., entry in $\vect{\mathcal{X}}$ the expected value of the probability distribution matched the expected value obtained from our observed data.

    We can find the best distribution \wrt the entropy by maximizing the Shannon Entropy with \eq\ref{eq:expecval} as constraints on a constrained optimization problem:
    \begin{equation}
        \label{eq:entropy}
        \begin{split}
            \max_{P\in \mathcal{P}} \quad & H(P) = - \sum_{x\in\mathcal{X}} P(x) \log P(x) \\
            s.t. \quad & \mathbb{E}_P[\phi_d(X)] = \tilde{\mu}_d  \; \forall d\\
            & \sum_{x\in \mathcal{X}} p(x) = 1.
        \end{split}
    \end{equation}
    For brevities sake we omit the inequality constraints for $p(x) \geq 0$ as it turns out that these constraints are redundant. 
    Moreover we introduce lagrange multipliers to express equation \ref{eq:entropy} as an unconstrained optimization problem and turn the optimization into a minimization by multiplying the objective function $\mathcal{L}$ by minus one.
    \begin{equation}
        \min_{P\in \mathcal{P}, \vect{\theta}, \vect{\theta_0} } \mathcal{L}(P, \vect{\theta}, \theta_0) = \sum_{x\in\mathcal{X}} P(x) \log P(x) + \sum_{i=1}^d \theta_i (\tilde{\mu}_d - \sum_{x \in \mathcal{x}} p(x) \phi_i(x)) + \theta_0 \sum_{x\in\mathcal{X}} p(x) -1.
    \end{equation}
    One Lagrange multplier is introduced for each constraint resulting in a total of $d+1$ Lagrange multipliers.
    We then compute the derivative of \eq\ref{eq:entropy}\wrt $P(x)$ :
    \begin{equation}
        \begin{split}
        \frac{\partial \mathcal{L}}{\partial P} =  \frac{\partial}{\partial P} \sum_{x\in\mathcal{X}} P(x) \log P(x) &+ \sum_{i=1}^d \theta_i (\tilde{\mu}_i - \sum_{x \in \mathcal{X}} p(x) \phi_i(x)) + \theta_0 \sum_{x\in\mathcal{X}} p(x) -1 \\ 
        &= 1 + \log P(X) - \sum_{i=1}^d \theta_i  \phi_i(x) + \theta_0 = 0 \\
        \log P(X) &=  \sum_{i=1}^d \theta_i  \phi_i(x) - \theta_0 - 1 \\
        P(X) &= \exp^{(\langle\vect{\theta}, \vect{\phi_i(x)} \rangle - \theta_0 - 1)} \\
    \end{split}
    \end{equation}

    and it follows that the objective function reaches its optimum when 
    \begin{equation}   
    P(x) = \exp^{(\inner{\vect{\theta}}{\vect{\phi_i(x)}} - \theta_0 - 1)}
    \end{equation} 
    and with setting $\theta_0 = -1 + A(\theta) $ we obtain the form of the canocical exponential family

    \begin{equation}
        P(X) = \exp^{(\langle\vect{\theta}, \vect{\phi_i(x)} \rangle - A(\theta))},
    \end{equation}
    where $A(\theta) = \log \sum_{\vect{x}\in \vect{\mathcal{X}}} \exp(\inner{\vect{\theta}}{\phi(\vect{x})})$ is the log partition function, the normalizer of the probability distribution, ensuring that the second set of constraints from \eq\ref{eq:entropy}is not violated.

    \subsection{Training}
    \label{ssec:train}

    Now that we know, which probability distribution maximizes the entropy we have to determine the parameters $\vect{\theta} \in \Theta$, where $\Theta = \{\vect{\theta} \;\lvert A(\vect{\theta}) < \infty\} \subset \mathbb{R}^d$ is the set of feasible solutions.
    Additionally we have to compute the partition function $A$, which depends on $\vect{\theta}$.
    Using a first order gradient descent method, we can find the optimum by iteratively updating the parameters of a function $\ell: \mathbb{R}^d \rightarrow \mathbb{R}$, i.e., taking one step in the direction of the derivative, the gradient 
    \begin{equation}
        \label{eq:gradesc}
        \vect{\theta}^{t+1} = \vect{\theta}^{t} + \alpha \nabla \ell(\vect{\theta}^{t}).
    \end{equation}

    Note that in \eq\ref{eq:gradesc} the $t+1$ does refer to an update of $\vect{\theta}$ as opposed to the usual notation, i.e., the $t+1^{th}$ model. 
    The choice of $f$ is usually the likelihood function
    \begin{equation}
        \label{eq:likelihood}
        \mathcal{L}(\vect{\theta} ; \mathcal{D}) = \arg\max_{\vect{\theta}}\prod_{\vect{x} \in \mathcal{D}}  \mathbb{P}_{\vect{\theta}}(\vect{X} = \vect{x}),
    \end{equation}
    which maximizes the probability of the data being generated by a probability distribution conditioned on $\vect{\theta}$.

    Due to numerical instabilities we usually choose to minimize the negative log likelihood instead,
    \begin{equation}
        \label{eq:likelihood:log}
        \ell(\vect{\theta} ; \mathcal{D}) = \arg\min_{\vect{\theta}} \; - \sum_{\vect{x} \in \mathcal{D}}  \log \mathbb{P}_{\vect{\theta}}(\vect{X} = \vect{x}).
    \end{equation}

    Since the logarithm is a monotone operator, this transformation does not affect the solution.
    Additionally, when optimizing via first order methods e.g. gradient descent, it is advantageous to use the negative average log likelihood
    \begin{equation}
        \label{eq:likelihood:expfam}
        \begin{split}
        \ell(\vect{\theta}, \mathcal{D}) &= - \frac{1}{\abs{\mathcal{D}}} \sum_{\vect{x} \in \mathcal{D}} \log \mathbb{P}_{\vect{\theta}}(x)\\
        &= - \frac{1}{\abs{\mathcal{D}}} \sum_{\vect{x} \in \mathcal{D}} \inner{\vect{\theta}}{\phi(x)} - A(\vect{\theta})
    \end{split}
    \end{equation}
    instead.

    Optimizers usually minimize a function and thus we just use the negative log likelihood as this turns out to be equivalent to maximizing the likelihood.
    The reason for taking the average becomes appararent when using first order methods to find the optimum, i.e., the gradient of a function instead of second order methods, which use for example the second derivative, the Hessian.

    Taking the derivative \wrt to $\vect{\theta}$ we get:
    \begin{equation}
        \label{eq:likelihood_der}
        \begin{split}
            \frac{\partial \ell(\vect{\theta}, \mathcal{D})}{\partial \vect{\theta}} &=  \frac{\partial}{\partial \vect{\theta}}  - \frac{1}{\abs{\mathcal{D}}} \sum_{\vect{x} \in \mathcal{D}} \inner{\vect{\theta}}{\phi(x)} + A(\vect{\theta})\\
            &= - \frac{1}{\abs{\mathcal{D}}}  \sum_{\vect{x} \in \mathcal{D}} \phi(x) +  \frac{\partial}{\partial \vect{\theta}}  A(\vect{\theta}) \\
            &= - \frac{1}{\abs{\mathcal{D}}}  \sum_{\vect{x} \in \mathcal{D}} \phi(x) +  \frac{\partial}{\partial \vect{\theta}}  \log \sum_{\vect{x} \in \vect{\mathcal{X}}} \exp(\inner{\vect{\theta}}{\phi(\vect{x})})\\
            &= - \frac{1}{\abs{\mathcal{D}}}  \sum_{\vect{x} \in \mathcal{D}} \phi(x) +  \frac{1}{Z(\vect{\theta})}\frac{\partial}{\partial \vect{\theta}}  \sum_{\vect{x} \in \vect{\mathcal{X}}} \exp(\inner{\vect{\theta}}{\phi(\vect{x})})\\
            &= - \frac{1}{\abs{\mathcal{D}}}  \sum_{\vect{x} \in \mathcal{D}} \phi(x) +  \frac{1}{Z(\vect{\theta})} \sum_{\vect{x} \in \vect{\mathcal{X}}}  \frac{\partial}{\partial \vect{\theta}}  \exp(\inner{\vect{\theta}}{\phi(\vect{x})})\\
            &= - \frac{1}{\abs{\mathcal{D}}}  \sum_{\vect{x} \in \mathcal{D}} \phi(x) +  \frac{1}{Z(\vect{\theta})} \sum_{\vect{x} \in \vect{\mathcal{X}}} \exp(\inner{\vect{\theta}}{\phi(\vect{x})}) \frac{\partial}{\partial \vect{\theta}}  \inner{\vect{\theta}}{\phi(\vect{x})}\\
            &= - \frac{1}{\abs{\mathcal{D}}}  \sum_{\vect{x} \in \mathcal{D}} \phi(x) +  \frac{1}{Z(\vect{\theta})} \sum_{\vect{x} \in \vect{\mathcal{X}}} \exp(\inner{\vect{\theta}}{\phi(\vect{x})}) \phi(\vect{x})\\
            &= - \frac{1}{\abs{\mathcal{D}}}  \sum_{\vect{x} \in \mathcal{D}} \phi(x) + \sum_{\vect{x} \in \vect{\mathcal{X}}} \exp(\inner{\vect{\theta}}{\phi(\vect{x})} - A(\vect{\theta})) \phi(\vect{x})\\
            &= - \frac{1}{\abs{\mathcal{D}}}  \sum_{\vect{x} \in \mathcal{D}} \phi(x) + \sum_{\vect{x} \in \vect{\mathcal{X}}} \mathbb{P}_{\vect{\theta}}(\vect{X} = \vect{x}) \phi(\vect{x})\\
            &=  - \tilde{\mathbb{E}}_{\mathcal{D}}[\phi(x)] + \mathbb{E}_{\mathbb{P}}[\phi(x)]
        \end{split}
    \end{equation}
    which is the difference between the expectation of the probability distribution $\mathbb{P}$ and the empirical expectation, i.e., the average sufficient statistics implying that the optimum is located where the empirical expectation matches the distribution's expectation.

    Unfortunately the computation of the gradient requires the computation of the log partition function, which is \#P-Complete.
    As such there exists no computationally tractable solution to the computation of the log partition in the general case.
    However, it is possible to find an exact solution to this problem given some structural constraints on the Graph G, such as a limited treewidth or if G is a tree.

    Learning the model parameters requires us to compute the gradient of the likelihood and this in turn requires us to compute the log partition function. 
    This problem is known as inference, which has no exact solution in the general case, but can be exactly determined in the case where G is a tree.

    \paragraph*{Log Partition Function}
    For second order methods such as Newton's Method the second derivative of A \wrt $\vect{\theta}$ may also be of interest.
    It can be shown (see for example \cite{piatkowski2018exponential}), that the second derivative of the log partition function is the Fisher Information matrix, that is:

    \begin{equation}
        \label{eq:sparsefish}
        \begin{split}
        \frac{\partial^2 A(\vect{\theta})}{\partial \vect{\theta}_i \partial \vect{\theta}_j} &= \mathbb{E}[\phi(\vect{X})_i \phi(\vect{X})_j] - \mathbb{E}[\phi(\vect{X})_i] \mathbb{E}[\phi(\vect{X})_j], \\
        &= \sum_{\vect{x} \in \mathcal{X}} \phi(x)_i \phi(x)_j \mathbb{P}_{\vect{\theta}}(\vect{x}) - \sum_{\vect{x} \in \mathcal{X}} \phi(x)_i \mathbb{P}_{\vect{\theta}} \sum_{\vect{x} \in \mathcal{X}} \phi(x)_i  \mathbb{P}_{\vect{\theta}} \\
        &= Cov[\phi(\vect{X})_i, \phi(\vect{X})_j].
        \end{split}
    \end{equation}

    However, in terms of distributed learning, especially on resource constrained systems, second order methods or the fisher information are of little use as the memory requirement for storing the matrix is quadratical in the number of model parameters.
    Even for small models the memory requirement is not negligible.

    \subsection{Asymptotic Normality}
        \label{ssec:asymp}
        Parametrized families of densities can be attributed certain properties if they satisfy some regularity conditions.
        These properties allow us to investigate the estimator's behaviour as the number of models increases. 
        One such property is the asymptotic normality of the maximum likelihood estimator.
        Given a number of samples $n = \abs{\mathcal{D}}$ drawn i.i.d. from a random variable $\vect{X}$, we can show that in the limit of $n \rightarrow \infty$ the parameter vectors follows a normal distribution $\mathcal{N}(\vect{\mu}, \vect{\Sigma})$ for some mean and variance.

        %The diagonal representation only requires storing an array with $d$ elements and in sparse matrix representation $2d$ elements, one for the value itself and another for the position in the matrix. We do not require both row and column indices as they are equal in this case.
        %Additionally, for elements $i,j$ beloning to the same clique the left hand side of \eq~\ref{eq:sparsefish} is zero as the probability of observing both $\phi_C(\vect{x})_i$ and $\phi_C(\vect{x})_j$ is by definition zero, i.e., each clique can only assume one state at a single time.

        \begin{threm}{Berk's Theorem of Asymptotic Normality\cite{berk1972consistency}}
            \label{theorem:berk}
            Let $\vect{\theta^*} \in \mathbb{R}^d$ denote the true parameter vector and $\hat{\vect{\theta}} \in \vect{\Theta}$ some MLE with $\ell'(\hat{\vect{\theta}}, \mathcal{D}) = 0$.
            Furthermore let $i(\vect{\theta}^*) \in \mathbb{R}^{d \times d}$ be the fisher information matrix.

            Then for $n \rightarrow \infty$ the MLE $\vect{\theta}$ is asymptotically normal, i.e.

            \begin{equation}
                \sqrt{n}(\vect{\theta} - \vect{\theta}^*) \dot{\sim} \mathcal{N}(0, i(\vect{\theta}^*)^{-1}),
            \end{equation}
            which can be rearranged according to Slutsky's Theorem\cite{casella2002statistical} to

            \begin{equation}
                \vect{\theta} \dot{\sim} \mathcal{N}(\vect{\theta}, i(\vect{\theta}^*)^{-1}/n).
            \end{equation}
        \end{threm}

        The Fisher Information $i(\vect{\theta}) \in \mathbb{R}^{d \times d}$, sometimes called precision matrix
        \begin{equation*}
            i(\vect{\theta}) = \mathbb{E}\bigg[- \frac{\partial^2}{\partial \vect{\theta}_i \partial \theta_j} \log f(\vect{X}; \vect{\theta})\bigg] ,
        \end{equation*} 
        is the negative hessian of the log likelihood for some probability density function $f$ evaluated at $\vect{\theta}$.
        The Fisher Information can be interpreted as a measure of precision for a random variable $\vect{X}$, i.e., the information random samples from a distribution contain about the parameters.
        A random variable having low precision is equates to high variance, that is less information conveyed about the unknown parameters for each sample.

        For the stationary MLE $\vect{\theta}^*$  and $n \rightarrow \infty$, the inverse fisher information $i(\vect{\theta}^*)^{-1}/ n = \frac{1}{n} \vect{\Sigma}$ is an estimator for the covariance matrix, implying that as $n$ increases the covariance decreases and the precision increases in terms of the sample size.

        Since $\vect{\theta}$ is normally distributed around the true parameter vector, for any sample $\mathcal{D}$ drawn i.i.d. from $\vect{X}$ the MLE $\vect{\theta}$ obtained by minimizing the negative average log likelihood $\ell(\vect{\theta}; \mathcal{D})$ can be treated as a sample from a normal distribution.

        Furthermore, the MLE for the mean of a normal distribution $\mathcal{N}(\vect{\theta}^*, i(\vect{\theta}^*)^{-1}/n)$ with mode $\vect{\theta}^*$ is
        \begin{equation}
            \vect{\theta}^* = \frac{1}{n} \sum_{i=1}^n \vect{\theta}^{i}.
        \end{equation}
        Implying, that in the limit, the arithmetic average of all samples drawn from $\mathcal{N}$ are the true MLE for the normal distribution and thus the true parameter vector for $\mathbb{P}_{\vect{\theta}}(x)$ for a canonical exponential family distribution.
        
        Additionally if $\mathcal{N}$ is known or assumed to be know we can sample $\epsilon \sim \mathcal{N}(0, i(\vect{\theta}^*)^{-1})$ to generate new samples, i.e., new models without sampling additional data from $\vect{X}$.
        If $ i(\vect{\theta}^*)^{-1}$ we may find a proper substitute, that approximates the fisher information matrix or some other covariance matrix.

        This property can be helpful when sampling additional parameter vectors in a perturb and MAP fashion. We will talk about this in \sect \ref{ssec:pmap}.
        
        \paragraph*{Conditions}
        There a certain conditions associated with a distribution in order for Berk's Theorem to hold.
        Distributions from the canocical exponential family satisfy all of these conditions

        The Maximum Likelihood Estimator has to be consistent.

        All PDFS in the model have the same support

        The likelihood has to be differentiable up to third order order higher.

        The true estimator has to be an interior point of $\vect{\Theta}$.

        The MLE has to be unique and solve $\ell(\vect{\theta}) = 0$

        If this holds the MLE is consistent and asymptotically normal

        \paragraph*{Proof of Berk's Theorem}
        Let $\hat{\vect{\theta}}$ be the optimum of the maximum likelihood estimation and $\vect{\theta}^*$ the true parameter vector for some likelihood function $\ell(\vect{\theta}; \mathcal{D})$.
        The estimator for the canonical exponential family is consistent~\cite{berk1972consistency}  in  $\hat{\vect{\theta}} = \arg\max_{\vect{\theta}} \ell(\vect{\theta}; \mathcal{D})$.
        For a large enough sample size $\abs{\mathcal{D}} = n$ the maximum likelihood estimator$\hat{\vect{\theta}} - \vect{\theta}^* \approx 0$  approaches the true estimator, i.e. it is consistent. 
        Consistency allows us to use the first-order Taylor Expansion around $\vect{\theta}^*$ in order to obtain:
        \begin{equation}
            \label{eq:ll_general}
            \begin{split}
                 \ell^{'}(\vect{\theta}^*; \mathcal{D}) +(\hat{\vect{\theta}} - \vect{\theta}^*) \ell^{''}(\vect{\theta}^*; \mathcal{D}) &\approx 0 \\
            - (\hat{\vect{\theta}} - \vect{\theta}^*) \ell^{''}(\vect{\theta}^*; \mathcal{D})&  \approx \ell^{'}(\vect{\theta}^*) \\
            (\hat{\vect{\theta}} - \vect{\theta}^*)  &\approx - \frac{ \ell^{'}(\vect{\theta}^*; \mathcal{D}) }{ \ell^{''}(\vect{\theta}^*; \mathcal{D})} \\
            \sqrt{n}(\hat{\vect{\theta}} - \vect{\theta}^*)  &\approx - \sqrt{n} \frac{ \ell^{'}(\vect{\theta}^*; \mathcal{D}) }{ \ell^{''}(\vect{\theta}^*; \mathcal{D})}, \\
            \end{split}
        \end{equation}
        where $\ell^{'}$ is the first order partial derivative \wrt $\vect{\theta}^*$ and $\ell^{''}$ the second order derivative.
        %With Delt Operator instead of ''
        %\begin{equation}
            %\begin{split}
            %0 &\approx \nabla_{\vect{\theta}^*}\ell(\vect{\theta}^*; \mathcal{D}) +(\hat{\vect{\theta}} - \vect{\theta}^*) \nabla^2_{\vect{\theta}^*}\ell(\vect{\theta}^*; \mathcal{D}) \\
            %- (\hat{\vect{\theta}} - \vect{\theta}^*) \nabla^2_{\vect{\theta}^*}\ell(\vect{\theta}^*; \mathcal{D})&  \approx \nabla_{\vect{\theta}^*}\ell(\vect{\theta}^*) \\
            %(\hat{\vect{\theta}} - \vect{\theta}^*)  &\approx - \frac{ \nabla_{\vect{\theta}^*}\ell(\vect{\theta}^*; \mathcal{D}) }{ \nabla^2_{\vect{\theta}^*}\ell(\vect{\theta}^*; \mathcal{D})} \\
            %\sqrt{n}(\hat{\vect{\theta}} - \vect{\theta}^*)  &\approx - \sqrt{n} \frac{ \nabla_{\vect{\theta}^*}\ell(\vect{\theta}^*; \mathcal{D}) }{ \nabla^2_{\vect{\theta}^*}\ell(\vect{\theta}^*; \mathcal{D})} \\
        %    \end{split}
        %\end{equation}
      Then substituting the derivatives of the likelihood with the canonical exponential family from \eq~\ref{eq:sparsefish} we obtain for the denominator of \eq~\ref{eq:ll_general}
      \begin{equation}
        \begin{split}
        \frac{1}{n}\ell^{''}(\vect{\theta}^*; \mathcal{D}) &= \frac{1}{n} \sum_{i=1}^n \frac{\partial}{\partial \vect{\theta}^2} \ell (\vect{\theta}^*; \vect{x}^i)\\
        &= \frac{1}{n} \sum_{i=1}^n \frac{\partial}{\partial \vect{\theta}^2} A(\vect{\theta}^*)\\
        & = \frac{\partial}{\partial \vect{\theta}^2}  A(\vect{\theta}^*)  \\
        &\stackrel{\ref{eq:sparsefish}}{=} \mathbb{E}[\phi(\vect{X}) \phi(\vect{X})] - \mathbb{E}[\phi(\vect{X})] \mathbb{E}[\phi(\vect{X})] = - i(\vect{\theta}^*)
        \end{split}
      \end{equation}
      the negative fisher information for the true model parameters.
      In combination with the first order derivative obtained from in \eq~\ref{eq:likelihood_der} we get 
      \begin{equation}
        \label{eq:asympnorm}
          \begin{split}
            \sqrt{n}(\hat{\vect{\theta}} - \vect{\theta}^*)  &\approx - \frac{\frac{1}{\sqrt{n}}( -\tilde{\mathbb{E}}_{\mathcal{D}}[\phi(x)] + \mathbb{E}_{\mathbb{P})}[\phi(x)])}{\frac{1}{n} \frac{\partial}{\partial \vect{\theta}^2} \log } \\
            \sqrt{n}(\hat{\vect{\theta}} - \vect{\theta}^*)  &\approx - \frac{\frac{1}{\sqrt{n}}( -\tilde{\mathbb{E}}_{\mathcal{D}}[\phi(x)] + \mathbb{E}_{\mathbb{P})}[\phi(x)])}{\frac{1}{n} \mathbb{E}[\phi(\vect{X}) \phi(\vect{X})] - \mathbb{E}[\phi(\vect{X})] \mathbb{E}[\phi(\vect{X})]} \\
            \sqrt{n}(\hat{\vect{\theta}} - \vect{\theta}^*)   &\approx - \frac{\frac{1}{\sqrt{n}}( -\tilde{\mathbb{E}}_{\mathcal{D}}[\phi(x)] + \mathbb{E}_{\mathbb{P})}[\phi(x)])}{- i(\vect{\theta}^*)} 
          \end{split}
      \end{equation}

      Applying the Central Limit Theorem to an estimator with certain regularity conditions \eq~\ref{eq:asympnorm} 
      we find that 
        \begin{equation}
            \label{eq:asympnomi}
            \frac{1}{\sqrt{n}} ( -\tilde{\mathbb{E}}_{\mathcal{D}}[\phi(x)] + \mathbb{E}_{\mathbb{P}}[\phi(x)]) \rightarrow \mathcal{N}(0, i(\vect{\theta}^*))
        \end{equation}
        has expected value of zero with variance $i(\vect{\theta}^*)$, the fisher information.
        Additionally we already know that the denominator in \eq~\ref{eq:asympnorm} converges in probability to the fisher information and thus according to Slutky's Theorem, which states:

        \begin{threm}{Slutky's Theorem \cite{casella2002statistical}}
            Given two random elements $\vect{X}_n, \vect{Y}_n$, if $\vect{X}_n$ converges via Central Limit Theorem to a random variable $\vect{X}$ with some stationary distribution and $\vect{Y}$ converges in probability to a constant $c$ then 
            \begin{equation}
                \frac{\vect{X}_n}{\vect{Y}_n} \dot{\rightarrow} \frac{\vect{X}}{c}
            \end{equation}
        \end{threm}

        Combining \eq~\ref{eq:asympnorm} and \ref{eq:asympnomi} with Slutsky's Theorem we obtain 
        \begin{equation}
            \sqrt{n}(\hat{\vect{\theta}} - \vect{\theta}^*)  \rightarrow \frac{1}{ i(\vect{\theta}^*)} \mathcal{N}(0, i(\vect{\theta}^*) = \mathcal{N}(0, i(\vect{\theta}^*)^{-1}) 
        \end{equation}

        Thus, the Maximum Likelihood Estimator of canonical exponential families is consistent, regular and thus asymptotic normal i.e. $\sqrt{n}(\hat{\theta} - \theta^*) \sim \mathcal{N}(0, i(\theta^*)^{-1})$.
        Implying, that the paremeters follow a normal distribution with the  true parameter vector as mean an the inverse fisher information as covariance.
        Implying, that the inverse fisher infomration is an estimator for the covariance matrix and vice versa:
        
        \begin{equation}
            \vect{\Sigma} =  i(\theta^*)^{-1}
        \end{equation}

        We can interpret this result in such way that a set of parameter vectors obtained from data sets $\{\mathcal{D}_1, \ldots \mathcal{D}_n\}$ is normally distributed around the true parameter vector for a sample size of $n \leftarrow \infty$. 
        This in turn implies that we can treat parameter vectors of local devices as samples from such a distribution and use this to estimate the maximum likelihood of the parameter space.
        Addtionally we can use some $\vect{\theta}$ and assume this to be $\theta^*$ and use the result for sampling additional models.
        

\subsection{Inference}
\label{ssec:inf}
Evaluating a statistical model in some way is usually referred to as inference.
There are several tasks associated with inference such as finding the probability for a given observation or finding the mode of a distribution, that is, the observation with the largest probability.
Additionally, finding the model parameters $\vect{\theta}$ also requires us to perform inference for each iteration as we have to compute the log partition function as it is dependent on the model parameters, which are updated every iteration.

Inference can be broadly categorized into the following four areas, which all include some way or another marginalization and finding the partition function.

\paragraph*{Likelihood Computation}
Given a single observation $\vect{x} \in \vect{\mathcal{X}}$ we want to find its probability on $\mathbb{P}_{\vect{\theta}}$ or the likelihood, which is the log probability.
\begin{equation}
    \ell(\vect{\theta}; \vect{x}) = \inner{\vect{\theta}}{\vect{x}} - A(\vect{\theta})
\end{equation}

\paragraph*{Marginal Distribution}
Here our goal is the obtain the probability that a subset of states $A \subset V$ is in a specified configuration.
Hence, we have to marginalize over all other nodes not included in $A$. 
This is the more general case for the Likelihood Computation, where we obtain the probability $A = V$, an individual, fully observed sample.
    \begin{equation}
        \mathbb{P}_{\vect{\theta}}(\vect{X}_A = x_A) = \sum_{\vect{x}_B \in V \setminus\{A\}} \mathbb{P}_{\vect{\theta}}(x_B , x_A), \; A \cap B = \emptyset, A \cup B \subset V
    \end{equation}

\paragraph*{Conditional Probability}
Obtaining the conditional probability for an observed subset $B \subset V$ on another disjoint subset $A \subset V$ requires two steps. First we have to marginalize over the joint probability of both sets followed by the probability of observing $B$ on its own.
    \begin{equation}
        \mathbb{P}_{\vect{\theta}}(\vect{X}_A = \vect{x}_A \lvert \vect{X}_B = \vect{x}_B) = \frac{\mathbb{P}_{\vect{\theta}}(\vect{X}_A = \vect{x}_A , \vect{X}_B = \vect{x}_B)}{\mathbb{P}_{\vect{\theta}}(\vect{X}_B = \vect{x}_B)}, \;\; A \cap B = \emptyset, A \cup B \subset V
    \end{equation}

\paragraph*{Maximum a-Posteriori}
Instead of marginalization, this task requires maximization as we want to obtain the single sample with the highest probability for a distribution. 
Instead of using the sum-product message passing algorithm we use the max-product message passing instead.
\begin{equation}
    \label{eq:map}
    \hat{\vect{x}} = \arg\max_{\vect{x} \in \vect{\mathcal{X}}} \mathbb{P}_{\vect{\theta}}(\vect{X} = \vect{x}) = \arg\max_{\vect{x} \in \vect{\mathcal{X}}} \exp(\inner{\vect{\theta}}{\phi(\vect{x})} - A(\vect{\theta}))
\end{equation}

\paragraph*{Belief Propagation}
Even for a small number of nodes and states per node the computation of the partition function and thus performing any type of inference becomes computationally intractable.
Consider a graph with 100 nodes representing a random variable with binary states.
In the worst case, i.e., a fully conected graph we have to iterate over $2^{100} = 1.26\cdot e^{30}$ states
While we can not obtain the exact solution for the marginalization for a given Graph with arbitrary structe, it is however possible to find an exact solution on trees or an approximate solution on general graphs.


The algorithm to solve this problem was introduced as Belief Propagation or Sum-Product message passing by Pearl \cite{pearl1982reverend} and was later extended to be exact on polytrees as well \cite{kim1983computational}.
In case the graph is not a tree we can still find an approxiate solution via loopy belief propagation, which was empirically studied by Murphy et al. \cite{murphy2013loopy}

Belief propagation takes advantage of the distributivity of potentials computed on each node. 
Instead of computing the same terms over and over we can simply pass them between nodes of the graph in a dynamic programming fashion.
This allows us to obtain the exact solution for all relevant marginals without having to enumerate over all possible states $\vect{\mathcal{X}}$, that can then be used to calculate the partition function.

When $G=(V,E)$ is a tree, the clique factorization reduces to a product over nodes and edges
\begin{equation}
    \label{eq:edgepot}
    \mathbb{P}_{\vect{\theta}}(\vect{X} = \vect{x}) = \frac{1}{Z(\vect{\theta})} \prod_{u\in V} \psi_u(x_u) \prod_{u,v \in E} \psi_{uv}(x_{uv}).
\end{equation}
This is the case as the set of maximal cliques in a tree is just the set of all nodes and edges.
The sum-product algorithm is capable of computing the marginal distribution for every node in the graph.
The marginal distribution for a single node is defined as
\begin{equation}
    \mathbb{P}_{\vect{\theta}}(X_v = x_v) = \sum_{\vect{x}' \in \{\vect{x}' \lvert x_v\}} p(\vect{x}'),
\end{equation}
which is the sum over all probabilities, where $x_v$ appears.

The sum-product algorithm takes advantage of the property that for any graph G, if G is a tree, any subgraph $G' \subset G$ is also a tree. 
This way we can break down the factorization from \eq\ref{eq:edgepot} into subproblems again requiring the factorization of a smaller tree.
This divide an conquer approach can be used to compute all node marginals simultaneously and in parallel by passing theses messages between nodes.


Consider an arbitrary vertex $u \in V$ of G, where $\mathcal{N}(s)$ is the set of neighbors of $u$.
We can then write the factorization in terms of the subgraph $G'=(V', E')$ induced by each neighbor of $u$. 
Let one of these neighbors be the node $v$ we then have 
\begin{equation}
    \mathbb{P}_{\vect{\theta}}(\vect{x}_{V'}\lvert G') \propto \prod_{v \in V'} \psi_v(x_v) \prod_{vw \in E'} \psi_{vw}(x_{vw})
\end{equation}

This can be done for every subtree of $v$, which allows us to obtain the marginal distribution for $x_v$
\begin{equation}
    \label{eq:normbp}
    \begin{split}
    \mathbb{P}_{\vect{\theta}}(X_v = x_v) &= \frac{1}{Z} \cdot \psi_v(x_v) \prod_{t \in \mathcal{N}(v)} m_{t \rightarrow v}(x_v),  \\
    & \text{with}\\
    m_{t \rightarrow v}(x_v) &= \sum_{x'_{V_t}} \psi_{vt}(x_v, x_t') p(x'_{V_t} \lvert G_t) \\
    & = \sum_{x'_{V_t}} \psi_{vt}(x_v, x_t')  \prod_{s\in \mathcal{N}(t) \setminus \{v\}} m_{s \rightarrow t}(x_t),
    \end{split}
\end{equation}
such that the input for a subtree is, again, a subtree.

This gives rise to a 2-pass algorithm in case the graph is a tree.
In the first phase, "Converge", we start by choosing an arbitrary root, wher all messages converge.
Then, we start computing messages on the leaves \wrt to the root as they do not depend on additonal messages.
Every root of a subtree waits for all messages to arrive and the sends its message towards the parent until the messages have propagated to the root.
In the second pass, the "Broadcast", the root sends messages to each neighbor, distributing the messages that have arrived from different subtrees.

In the general case, when G is not a tree, we can employ a similar algorithm usually referred to as Loopy Belief Propagation(\alg~\ref{alg:lbp}).
Instead of using two passes to reach convergence we compute new messages on each node by sending  old messages back and forth between nodes.
This is done by computing messages in both directions for each edge i.e., 
\begin{equation}
    \begin{split}
        \label{eq:mespas}
        m_{t \rightarrow v}(x_v) &= \sum_{x'_{V_t}} \psi_{vt}(x_v, x_t') \prod_{s\in \mathcal{N}(t) \setminus \{v\}} m_{s \rightarrow t}(x_t) \\
        m_{v \rightarrow t}(x_t) &= \sum_{x'_{V_v}} \psi_{tv}(x_t, x_v') \prod_{u\in \mathcal{N}(v) \setminus \{t\}} m_{u \rightarrow v}(x_v). \\
    \end{split}
\end{equation}

We may then express the partition function in terms of messages\cite{piatkowski2018exponential} at any node as 
\begin{equation}
    Z = \sum_{\vect{x} \in \mathcal{X}_v} \prod_{u \in \mathcal{N}(v)} m_{u \rightarrow v}(\vect{x}_v).
\end{equation} 


\begin{algo}{Loopy Belief Propagation}
    \begin{algorithm}[H]
        \caption{Loopy Belief Propagation}
        \begin{algorithmic}
            \label{alg:lbp}
            \REQUIRE Graph G=(V,E), Model Parameters $\vect{\theta} \in \mathbb{R}^d$, Threshold~$\epsilon > 0$, Number of Iterations $n$\\
            \ENSURE  Node and Edge Marginals $\hat{\mu} \in [0,1]^{d + \abs{V}}$ \\
            \STATE{$\vect{m}^{new} \leftarrow 0, \vect{m}^{old} \leftarrow \infty$}
            \FOR{i=1 ...n}
            \STATE{$\vect{m}^{new} \leftarrow \vect{m}^{old}$} 
                \FORALL{$u,v \in E$}
                \STATE{Update messages $m_{u \rightarrow v}(\vect{x}_v)$ and $m_{v \rightarrow u}(\vect{x}_u)$ (\eq~\ref{eq:mespas})}\\
                \ENDFOR
                \IF{$\epsilon \geq  \abs{\vect{m}^{new} - \vect{m}^{old}}$}
                \STATE{$\vect{m}^{new} \leftarrow \vect{m}^{old}$}\\
                \STATE{BREAK}\\
                \ENDIF
            \ENDFOR
            \RETURN {Normalized messages $\hat{\mu}$~(\eq~\ref{eq:normbp})}
        \end{algorithmic}
    \end{algorithm}
\end{algo}
\subsection{Structure Learning}

So far we have talked about estimating the parameters of exponential family models and performing inference, which is a necessary step for training as well. 
Given some data $\mathcal{D} \subset \mathbb{R}^m$  we consider each dimension of $\mathcal{D}$ to be generated by univariate random variables $\vect{X} = (\vect{X}_1, \ldots \vect{X}_m)$.
Unless we already have some information about the underlying independency structure of $\vect{X}$, it is necessary to somehow create this independency structure based on the observed samples $\vect{X} \sim \mathbb{P}_{\vect{\theta}}$.
In terms of the graph $G=(V,E)$, we have knowledge about the shape of the vertices $V$, but usually none about the edges $E$.

In case the independency structure is given it may still be beneficial to employ a technique to determine a different independency structure, e.g., to reduce complexity of the original structure via a junction tree algorith \todo{cite}.
Additionally the provided structure may be too complex, which would result in increase run-times and non-exact solutions.
Here, we may estimate a tree based on the provided structure or simply to verify the structure, o.e., if the structure is plausible.

Finding conditional independencies between radom variables can be achieved by various approaches such as the Chow-Liu algorithm or \todo{more dependency}. 
We can obtain more refined structures by using dedicated tools such as Chordalysis \todo{cite}.

For this work we will focus on the Chow-Liu algrithm, which uses a combination of Cross-Correlation and Minimum Spanning Tree in order to find the edges, with the hightes correlation.
The 

\paragraph*{Chow-Liu Algorithm}
Chow and Liu introduced an algorithm\cite{chow1968approximating} to obtain a first order dependency structures using second order terms.
The tree is constructed by finding the edges that minimize the Kullback Lebiler divergence between the actual distribution and the distribution induced by the set of edges we obtain by computing the maximum spanning tree of the mutual information between all random variables.

Formally:
\begin{definition}{Weighted Undirected Graph}
    Let $G=(V,E)$ be a graph, where $E = V \times V$ and let further $c: V \times V \rightarrow \mathbb{R}$ be a cost function associated with edges of G with $c(u,v) = c(v,u) \forall u,v \in V$.
\end{definition}

    The cost function $c(u,v)$ is given by the mutual information between two random variables, i.e. 
    \begin{equation}
        c(\vect{X}_v, \vect{X}_u) = I(X_v, X_u) = \sum_{\vect{x}_v \in \mathcal{X}_v} \sum_{\vect{x}_u \in \mathcal{X}_u} \mathbb{P}(\vect{x}_v,\vect{x}_u) \log \frac{\mathbb{P}(\vect{x}_v,\vect{x}_u) }{\mathbb{P}(\vect{x}_v)  \mathbb{P}(\vect{x}_u)},
    \end{equation}
    where the probabilities are usually the relative frequencies of the observed events from $\mathcal{D}$.
    Then, we obtain the minimum Kullback-Leibler divergence by computing the maximum spanning tree on the  connected graph.
    \begin{equation}
        \min_{E} KL(\mathbb{P}^*, \mathbb{P}_{E}) = \max_{E} \sum_{(u,v) \in E} c(\vect{X}_v, \vect{X}_u), \; s.t. \; \abs{E} = \abs{V}-1 .
    \end{equation}
    Here connected implies, that there exists a path between each pair of vertices in G. Such a tree can be found with well known algorithms such as Prim's Algorithm or Kruskal's Algorithm.

\section{Distributed Learning}
Our next goal is to apply the training and evaluation of probabilistic graphical models to a distributed setting.
Therefore, we need to indentify the task and environment we want to apply distributed learning on.
First, the available data may either be distributed onto different devices or stored at some central server.
Given the data $\mathcal{D} = \{\vect{x}^1, \ldots, \vect{x}^n\} \subseteq \vect{\mathcal{X}}^m$ is already distributed on some set of devices, this raises the question on how the data is distributed.
Horizontally distributed data is a subset of the data, i.e. $\mathcal{D}_{A} = \{\vect{x}^1, \ldots, \vect{x}^A \lvert A \leq n\}$ from domain $\mathcal{X}^m$. while vertically distributed data is a subset of features $\mathcal{D}_{A} = \{\vect{x}^1, \ldots, \vect{x}^n \} \subseteq \vect{\mathcal{X}}^A, A < m$ from domain $\mathcal{X}^A$, but contains all data available.
Briefly, the horizontal approach has a restricted sample size and the vertical approach a restricted domain.

Furthermore, there are two different architectural approaches to distributed learning. 
The first approach is a decentralized approach without a central device that stores and manages data and models.
Network connections between devices are used for communication and coordinator.
In case of wireless networks the connectivity between devices may change rapidly, which leads to a more dynamic approach with frequently changing network structure.
The second approach uses a centralized server, the coordinator, as an intermediary between nodes. 
Most if not all communication is managed by the coordinator, while data may still be kept on the local devices only.
In theory it is also possible to combine both approaches, i.e., to allow communication between local devices to some degree to achieve a better local solution.
Lastly, in the fully centralized approach all data is kept on the server is then partitioned into chunks and sent to local devices, that then execute some task, which is for example the case in a Map\&Reduce framework.

Bringing statistical models to local devices is useful when the data is horizontally distributed and can not be sent to a central server.
This may be the case when we would either incur relative heavy communication load (e.g. wireless sensor networks with limited bandwidth) or when privacy is of concern.
Instead, we may move the statistical model onto these devices and communicate the estimated distribution $\mathbb{P}_{\vect{\theta}^i}$ with a coordinator.

Depending on the task we may have to restrict communication, memory and processor consumption to an absolute minimum.
Here, statistical models have one major advantage over other models, that is they require less disk space to store and process as only certain statistics need to be preserved.
These are generally the parameters of the distribution we provide.
With markov random fields using a probability distribution from the exponential family we do not even have to store the data in memory has we only have to preserve the sufficient statistics for training and inference.

The two major issues we have to deal with are therefore the sample complexity and the model aggregation.
As the main focus of this work is the model aggregation, which will be featured in chapter \ref{chapter:ch3}, we will now briefly discuss convergence and sample complexity.

\subsection{Convergence}
In a distributed environment it is necessary to define some notion of convergence in order for local models to stop computation and possibly to stop gathering data altogether.
This becomes even more critical when memory, processing capabilities and energy consumption are limited.
We differentiate between two types of convergence criteria, static and dynamic.
While static convergence may be determined at the start of each session, dynamic convergence criteria may change over time depending on local model properties.
Dynamic criteria usually require some sort of node communication either between each other or between the coordinator. 
Then, depending on some properties of each model, we may call for convergence on each device.
Static criteria on the other hand are determined beforehand and may involve number of samples observed, number of iterations or something else enteriely, that does not require direct communication.

\paragraph*{Local Convergence}
Local models are trained using gradient descent to minimize the likelihood. 
Recall that the likelihood is convex, thus the minimizer always moves towards the global minimum.
As $\vect{\theta}$ moves towards the global minimum the difference between two subsequent objective function values decreases as well.
Hence, we may use this difference as a method to determine convergence, given some threshold $\varepsilon$ we stop the optimization if the different between two likelihoods falls below this threshold, i.e.:
\begin{equation}
    \abs{\ell(\vect{\theta}^{(t+1)} ; \mathcal{D}) - \ell(\vect{\theta}^{(t)} ; \mathcal{D})} \leq \varepsilon,
\end{equation}
where the choice of $\varepsilon$ determines how close we are to the actual optimum when stopping.
Note, that $\theta^t$ here denotes any parameter vector at time $t$ instead of the parameters on some local device.
Additionally, when the gradient is close to zero, we may also terminate the optimization as the optimum is located at $\nabla_{\vect{\theta}} \ell(\vect{\theta}^{*} ; \mathcal{D}) = 0$ and once the gradient
\begin{equation}
    \norm{\nabla_{\vect{\theta}} \ell(\vect{\theta}^{(t)} ; \mathcal{D})}_{\infty} \leq \epsilon
\end{equation}
is sufficiently close to the optimum we set terminate the optimization. 


\paragraph*{Global Convergence}
Since data on each device is dynamically accumulated we aquire new samples in an online streaming fashion, the previous results may become obsolote in the next iteration.
Here, we simplify this approach and consider a synchronized method, where we express the steps necessary to obtain a result in terms of rounds or iterations. 
At each iteration step, we extend the original average sufficient statistics by the newly aquired data. 
This can be done by keeping a record of the sufficient statistics and the number of samples observed.
The new sufficient statistics and round $t+1$ are then the weighted average of the old average sufficient statistics and time $t$ and new observations 
\begin{equation}
    \vect{\tilde{\mu}}^{t+1} = \frac{\abs{\mathcal{D}^{t}}\vect{\tilde{\mu}}^t + \sum_{\vect{x} \in \mathcal{D}^{new}} \phi(\vect{x})}{\abs{\mathcal{D}^{t}} + \abs{\mathcal{D}^{new}}}.
\end{equation}

However, the already observed data is still incorporated in the sufficient statistics and only changes depending in the amount of new samples discovered.
If the sufficient statistics do not change much e.g. we already have large number of samples and only observe a small amount of new data, the optimum is not likely to change too drastically. 
Instead of simply reiterating the training we can check if the old parameters are still sufficientely close to the optimum in terms of the new data i.e. 

\begin{equation}
    \norm{\nabla_{\vect{\theta}^t} \ell(\vect{\theta}^{t} ; \mathcal{D}_i^{t+1})}_{\infty} \leq \epsilon,
\end{equation}
then choose to skip a training step if the gradient is still $\epsilon$-close to zero.

The total memory requirement is linear  in the number of parameters, that is, the d-dimensional space that $\phi$ maps to, thus each model only requires $\mathcal{O}(d)$ memory.
Using this iterative approach we can keep track of the number of samples observed and the current state of the model.
Now, we need to determine a stopping criterion, which allows us to terminate the optimization and data gathering process, if so desired. 
Otherwise, we may also keep the optimization and data aquisition running indefinitely.

We need to devise some evaluation mechanism that allows us to define a stopping criterion based on for example the amount of data observed or some pairwise distance measure between local model parameters or sufficient statistics.

\subsection{Bounds on Sample Complexity and Variance}
One of the key questions of distributed learning is the number of samples required to obtain local models that are sufficiently good, i.e., when to stop gathering new data.
This also implies having knowledge of when the model is good enough to stop training.
The problem to find the number of samples required cannote be solved exactly, but some bounds exists that allow an estimate of the model quality.
One such bound on the sample complexity is the Hoefding-Bound, which provides an upper bound on the distance between sufficient statistics.
 
We apply the Hoefding-Inequality on the pairwise difference between average sufficient statistics of local models.
Furthermore, we are going to demonstrate that the variance on entries in the sufficient statistics and between the parameters of different models is bounded in the amount of samples observed.

First, the Hoefding-Inequality is given by
\begin{equation}
    P(\vect{\bar{X}} - \mathbb{E}[\vect{\bar{X}}] \geq t ) \leq \exp^{-2\abs{\mathcal{D}]t^2}},
\end{equation}

where for some constant $t$ the probability of the difference between the empirical mean of a random variable and its expectation is begin greater than $t$ is negatively exponential bounded by the amount of data seen.

Piatkowski~\cite{piatkowski2019distributed} has shown that for $t = \frac{\sqrt{(c+1) \log d}}{2\abs{\mathcal{D}}}$ the difference between two average sufficient statistics $\vect{\mu}^i = \frac{1}{\mathcal{D}_i} \sum_{x \in \mathcal{D}} \vect{\phi(x)}, \; \vect{\phi(x)} \in \mathbb{R}^d$ of a probabilistic graphical model is bounded by
\begin{equation}
    \label{eq:hoefd}
    \norm{\mu^{i} -  \mu^{j}}_\infty \leq 2 \sqrt{
        \frac{(c+1) \log d}
        {2\abs{\mathcal{D^{'}}}}
        } = \epsilon,
\end{equation}

with probability of at least $\delta= (1- 2 \exp(-c \log d))$. Here $D^{'} = \min(\abs{\mathcal{D}^i}, \abs{\mathcal{D}^j})$.

As the average sufficient statistics approach each other, we can show by Popoviciu's Theorem \cite{popoviciu1935equations} that the variance $Var[\norm{\vect{\mu}^{i} -  \vect{\mu}^{j}}_\infty]$ is bounded \wrt the max-norm $\norm{\cdot}_{\infty}$.
\begin{threm}{Upper Bound on the Variance}
    Let $\vect{X}$ be a bounded random variable with $\inf(\vect{X}) = b$ and $\sup(\vect{X}) = a$, 
    then the variance $Var[\vect{X}] = \sigma^2$ is bounded by 
    \begin{equation}
        \sigma^2 \leq \bigg(\frac{a-b}{2}\bigg)^{2}
    \end{equation}
\end{threm}

Results by Sharma et al. \cite{sharma2010betterbounds} suggest a slighty improved bound by using the third central moment.
However, we assume the upper bound defined by Popoviciu to be sufficiently good. 

\input{kapitel/proofs/popoviciu.tex}

Recall that $\norm{\vect{\mu}^{i} -  \vect{\mu}^{j}}_\infty \leq  \epsilon$, where $\epsilon$ is the bound on the largest absolute difference between two entries of the average sufficient statistics. 
Then for some $k$ that satisfies the bound with equality we can write $\mu^{i}_k - \mu^{j}_k = a - b  = \epsilon$.
This means that there exists no larger difference between entries of the two vectors resulting in an upper bound for the variance of the average sufficient statistics with $a - b = \epsilon$ and thus
\begin{equation*}
    Var[\vect{\mu}^{i}_k -  \vect{\mu}^{j}_k] \leq \frac{\epsilon^2}{4} = \sigma^2 \quad \forall i,j,k.
\end{equation*}
Given two average sufficient statistics we can employ the Hoefding Bound to guarantee the distance and variance between these two to be smaller than some $\epsilon$ with probability $\delta$

Likewise, there is an explicit relationship between the sufficient statistics, sample size and the model parameters, which is shown by Bradley et al. \cite{bradley2012sample}, who consider the MLE bound \wrt to the sample size. 
Let $C_min \geq 0$ be a bound on the smallest eigenvalue of the fisher information obtained from the true parameters $\vect{\theta}^*$ and $\phi_{max}$ be the maximum magnitude of the sufficient statistics. 
Bradley et al. have shown that \wrt to these paramaters the $\ell_1$-norm between the true parameters and some estimate is bounded by 

\begin{equation}
    \label{eq:pac-bound}
    \norm{\vect{\hat{\theta}}- \vect{\theta}^*}_1 \leq \frac{C_{min}}{4d\phi^3_{max}} \abs{\mathcal{D}}^{-\alpha/2},
\end{equation}
where $\alpha$ is a hyperparameter $0 < \alpha \leq 1$ the controls the convergence rate in trade-off for the probability that this bound holds. 
Rearranging \eq~\ref{eq:hoefd} we substitute $\abs{\mathcal{D}}$ from \eq~\ref{eq:pac-bound} and can show that the distance between the estimate and true parameters is related to the hoefding bound on sufficient statistics
\begin{equation}
    \norm{\vect{\hat{\theta}}- \vect{\theta}^*}_1 \leq \frac{C_{min}}{4d\phi^3_{max}} \frac{\sqrt{2\sigma^2}}{\sqrt{((c+1) \log d)}},
\end{equation}
for $\alpha =1$.

Applying the triangle inequality to the parameters we obtain for any two parmater vectors $i,j$
\begin{equation}
    \norm{\vect{\theta}^i- \vect{\theta}^j}_1 \leq  \norm{\vect{\theta}^i - \vect{\theta}^*}_1  +  \norm{\vect{\theta}^j- \vect{\theta}^*}_1 \leq 2 \frac{C_{min}}{4d\phi^3_{max}} \frac{\sqrt{2\sigma^2}}{\sqrt{((c+1) \log d)}}
\end{equation}
and by the decreasing monotonicity of p-norms \cite{raissouli2010various} we have

\begin{equation}
    \norm{\vect{\theta}^i- \vect{\theta}^j}_{\infty}  \leq \norm{\vect{\theta}^i- \vect{\theta}^j}_1.
\end{equation}

As the amount of data on each device increases we can also measure how close the sufficient statistics are with probability $\delta$ by rearranging the above equation. 
This is the most common scenario in distributed learning. As time goes on the amount of data increases allowing us to continuously measure the distance and variance between the sufficient statistics.
Once we reach our predetermined goal on a device, i.e. $\abs{\mathcal{D}}^^i$ is sufficiently large to satisfy our bound we stop collecting more data send the parameter vector to the coordinator node.

For a given probability $\delta$ and with variance threshold $\sigma^2 = (\epsilon/2)^2$ we rearrange \eq~\ref{eq:hoefd} to obtain 
\begin{equation}
    \abs{\mathcal{D}} = \frac{(c+1) \log d}{2 \sigma^2},
\end{equation}

which is the amount of data necessary to have variance  of at most $\sigma^2$ with probability $\delta$.

We see that as $\abs{\mathcal{D}}$ increases, for a fixed $\delta$ the distance $\epsilon$ decreases and so does the variance. 
In conclusion we have seen that as the sample size increases the variance for model parameters and sufficient statistics decreases.
This allows us to choose a variance or distance threshold and obtain the necessary amount of data to reach that threshold.

Next we will introduce methods for sampling from a generative model, which is one of the main advantages these types of models have over discriminative ones.

\section{Sampling}
Statistical models such as probabilistic graphical models belong to the class of generavtive models.
These types of models allow not only for solving discriminative tasks such as classification, but also have the ability to freely sample new data.
Furthermore, we may have some device that only generates partial data with missing entries.
We may then use the model to fill in the gaps in order to obtain complete observations.
This is usually based on the configuration with the highest likelihood.

How do we sample from such a distribution? There are several different ways to sample from such a distribution, two of those are the perturb and MAP approach and Gibbs sampling.


\subsection{Perturb and MAP}
\label{ssec:pmap}
Recall from \eq\ref{eq:map} that sample that maximizes the posterior probability of probability distribution is the sample that maximizes the probability.
Perturb and MAP (\alg\ref{alg:pmap}) presents a technique to obtain samples using the maximum a-posteriori probability of a distribition.
The core idea is to add noise $\epsilon$ to the model parameters $\vect{\theta}$ effectively changing the mode of the distribution by small amount and thus obtaining a different maximizer for the a-posteriori probability. 
\begin{equation}
    \hat{\vect{x}}  = \arg\max_{\vect{x} \in \vect{\mathcal{X}}} \mathbb{P}_{\vect{\theta}}(\vect{x})
\end{equation}

However, we can \textbf{perturb} the model parameters with an additional term to obtain a different MAP state such that 
\begin{equation}
    \hat{\vect{x}}  = \arg\max_{\vect{x} \in \vect{\mathcal{X}}} \mathbb{P}_{\vect{\theta} + \epsilon}(\vect{x}), \quad \epsilon \sim f(x),
\end{equation}
where $\epsilon$ is a sample from another probability density function $f$.
The perturbation term may follow a uniform or normal distribution or some other, possibly unknown, distribution.

Now consider the asymptotic normality of exponential families as well as the bound on variances. 
We can use both in such way that $\epsilon$ is a sample from a normal distribution with variance based on the upper bound of the variance for the Hoefding-Bound.
This way we can sample from different MAP-States, while still remaining inside the Hoefding-Bound.

\begin{algo}{Perturb and MAP~\cite{papandreou2011perturb}}
    \begin{algorithm}[H]
        \caption{Perturb and MAP}
        \begin{algorithmic}
            \label{alg:pmap}
            \REQUIRE Probability Distribution $\mathbb{Q}_{\vect{\lambda}}(\vect{x})$
            \ENSURE  MAP-State $\hat{\vect{x}}$ from $P_{\vect{\theta} + \epsilon}(\vect{x})$ \\
            \STATE{Sample $\epsilon \sim \mathbb{Q}_{\vect{\lambda}} $}\\
            \STATE{$\hat{\vect{x}} \leftarrow \arg\max_{\vect{x} \in \vect{\mathcal{X}}} P_{\vect{\theta} + \epsilon}(\vect{x})$}\\
            \RETURN {$\hat{\vect{x}}$}
        \end{algorithmic}
    \end{algorithm}
\end{algo}

\subsection{Gibbs Sampling}
The Gibbs Sampling algorithm as presented in \alg\ref{alg:gibbs} is another method of obtaining samples $\vect{x} \in \vect{\mathcal{X}}$ from a random variable $\vect{X}$. 
Roughly, we first sample from a uniform distribution on $[0,1]$ and then proceed to compute the marginal prbabiltieis for a single entry, finding the most probable entry.
This is repeated for randomly chosen elements and it can be shown that the initially uniform sample approaches the distribution asymptotically.
However, as this may be computationally inhibitive due to the iterative nature of this method we may stop after a certain number of iterations, obtaining a sample which is roughly from $\vect{X}$.

\begin{algo}{Gibbs Sampling~\cite{yildirim2012bayesian}}
    \begin{algorithm}[H]
        \caption{Gibbs Sampling}
        \begin{algorithmic}
            \label{alg:gibbs}
            \REQUIRE $\vect{x} \sim f(\tau) \in \mathbb{R}^m$, Graph G=(V,E)
            \ENSURE  $\vect{x} \sim P_{\vect{\theta}}\in \mathbb{R}^m$ \\
            \FOR{i=1 ...n}
                \FORALL{$j \in V$}
                \STATE{$x_j \leftarrow x_j \sim P_{\vect{\theta}}(\vect{X}_j  \lvert \mathcal{N}(\vect{X}_j) )$}
                \ENDFOR
            \ENDFOR
            \RETURN {$\vect{x}$}
        \end{algorithmic}
    \end{algorithm}
\end{algo}
