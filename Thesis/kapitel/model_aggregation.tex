% Model_Aggregation.tex

\chapter{Model Aggregation}
\label{chapter:ch3}

\section{Averaging}
The most intuitive approach to model aggregation is done by simply computing the unbiased average of all model parameters.
\begin{equation}
    \frac{1}{n} \sum_{i=1}^n \theta^i
\end{equation}

Recall that in section \ref{ssec:asymp} we showed that the maximum likelihood estimator for canonical exponential families is asymptotically normal.
Hence, each $\vect{\theta}^i$ is one sample from the distribution $\vect{\theta}^i \dot{\sim} \mathcal{N}(\vect{\theta}^*, i(\theta)^{-1})$.
While we have no direct knowledge of $\vect{\theta^*}$, we can use the maximum likelihood estimators for mean and covariance for normal distributions which are

\begin{tcolorbox}
    Given a data set $\mathcal{D}$ with an underlying multivariate normal distribution and let $\vect{x}^i \in \mathcal{D}$ be a single sample from $\mathcal{D}$.
    The maximum likelihood estimators for mean and covariance of a multivariate normal distribution are
    \begin{equation}
        \begin{split}
            &\vect{\mu}_{MLE} = \frac{1}{n} \sum_{i=1}^n \vect{x}^i \\
        &\vect{\Sigma}_{MLE} = \frac{1}{n} \sum_{i=1}^n (\vect{x}^i - \vect{\mu}) \cdot  (\vect{x}^i - \vect{\mu})^T.
        \end{split}
    \end{equation}
\end{tcolorbox}

For $\vect{\theta}^i$ we then have $\vect{\mu}_{MLE} = \frac{1}{n} \sum_{i=1}^n \vect{\theta}^i$, which is exactly the average of all local model parameters.
Thus the arithmetic mean maximizes the likelihood.
\begin{tcolorbox}
    Proof maybe 
\end{tcolorbox}


\section{Weighted Averaging}
The first step towards a more refined approach to model aggregation is to turn the uninformative prior, that is, the equally weighted aggreagtes into an informed prior by some means.
\begin{equation}
     \sum_{i=1}^n \lambda_i \theta^i
\end{equation}

While this sounds quite easy in theory, the research on informative priors and prior-based learning is still prevalent and actively pursued.
One method involves weighting the local model parameters by its likelihood given probability distribution over these parameters.
Since the Maximum Likelihood Estimator for canonical exponential families is asymptotically normal we may use the likelihood of a gaussian normal as weights:
\begin{equation}
    \lambda_i = \frac{\log p(\theta \lvert \psi)}{\sum_{x\in \mathcal{X}} \log p(\theta \lvert \psi)}
\end{equation}
where the denominator is the normalizer for the probability distribution used, either discrete or continuous.
In case of a normal distribution we then have
\begin{equation}
    p(\theta \lvert \psi) = \mathcal{N}(\theta \lvert \vect{\mu}, \vect{\Sigma})
\end{equation}

\begin{equation}
    \lambda_i = \log \frac{1}{\sqrt{2\pi\lvert \vect{\Sigma} \rvert}} \cdot \exp^{-\frac{1}{2} \cdot (\vect{\theta} - \vect{\mu})^T\vect{\Sigma}^{-1}(\vect{\theta} - \vect{\mu}}
\end{equation}
\section{Radon Machines}
A more robust method to solve the issue of model aggregation is the computation of the geometric median.
While the geometric median is well defined, there exists no algorithm to exactly compute the geometric median in $\mathbb{R}^n$ dimensions.
However Kamp et al. \cite{kamp2017effective} introduced Radon Machines as method for model ensembling.
Computing the radon point of a set of points is a variant of the geometric median and closely related to the tukey depth. 
The solution to the equation is obtained by solving a system of linear equations.

\paragraph*{Radon Points}
Given a pair of convex hulls defined by two sets of points definined the hull $S_1, S_2$, a radon point is a point that is contained within the intersection of the two convex hulls of $S_1, S_2$. 
Radon's Theorem was originally proposed by Radon in 1921 \cite{radon1921mengen}:

\begin{threm}{Radon's Theorem}
    Given a set of of points in euclidean space
    \begin{equation}
        S = \{\vect{x}_1, \ldots \vect{x}_{d+2}\} \subset \mathbb{R}^d,
    \end{equation}
   there exists a partition of S into two subsets $S_1, S_2$ such that the intersection of the convex hulls spanned by both sets is not empty, that is
    
    \begin{equation}
        \exists S_1, S_2, \;\; S_1 \cup S_2 = S, \; S_1 \cap S_2 = \emptyset: Conv(S_1) \cap Conv(S_2) \neq \emptyset,
    \end{equation}

    where $Conv(\cdot)$ is the Convex Hull for a given set.
    Any point contained  in the intersection between the two convex hulls is a radon point.
\end{threm}

\paragraph*{Computation of Radon Points}

The problem of finding such as point can be solved by formulating a system of linear equations, where the radon point is defined by the equality of the convex combinations for the two resulting subsets $S_1, S_2$.
A radon point can be found by solving the following optimization problem

\begin{equation}
    \begin{split}
        \min_{\vect{\lambda}} \quad &1 \\
        s.t. \;\; &\sum_{i=1}^{d+2} \lambda_i \vect{x}_i = \vect{0}\\
             \;\; &\sum_{i=1}^{d+2} \lambda_i = 0 \\
             \; \; & \lambda_1 = 1
    \end{split}
\end{equation}

Since we have $d+2$ $\mathbb{R}^d$-dimensional points, we have to add an additional constraint by fixing one arbritary variable to a constant. 
This allows, in absence of linear dependencies, finding a unique solution to the problem.
However, any solution to the above system of linear equations is a radon point and there may exist multiple solutions.
One particular issue that we may run into is that the maximum likelihood estimators estimated from samples of the same random variable approch each other in the limit of $n \rightarrow \infty$, which means we may introduce linear dependencies as the amount of data on each device increases.
While this is an issue for the sytem of linear equation, as the solution may not be unique anymore, we can simply use the optimization problem to minimize the sum of squares to find any one solution.
As all points that satisfy the following condition are considered to be radon points:
\begin{tcolorbox}
    Condition for Radon Points    
\end{tcolorbox}

\paragraph*{Limitations}
However, this requires the computation of the radon point, which in euclidean space has $\mathbb{R}^n + 2$ dimensions.
This requires us to have at least $\mathbb{R}^n + 2$ models readily available and with further aggregation $\mathbb{R}^{r^n}$ as each hierachichal step increases the number of models required exponentially.

\section{Bootstrap Aggregagation}
With generative modelling we can easily sample additional data from local models by transferring the local model parameters to the coordinator node.
Sampling can be realized in various different ways such as Gibbs Sampling or Perturb and Map.
We then create a new Dataset from our samples and use this to train a new global model. 
This has been shown to work in CITE and for canonical expoential families this approach is analogous to computing the expected average sufficient statistics:
TODO PROOF:


\section{Performance-Weighted-Averaging}
Instead of using a federated approach we can also rely on a federated-decentralized hybrid approach. 
That is, we generate weights vor a weighted average based on some measure of local performance.
While this this does not gurantee good results for local performance of a single model, we can attempt to measure performance of local models on other local devices.
This approach essentially incorporates the idea of decentralized thresholding as featured in CITE RAN WOLFF.
Local Nodes communicate with each other and in this specific case we excchange parameter vectors to perform some kind of performance measure on the local data of each device.
This method still gurantees minimal computation cost, while also guranteeing the local data to remain private.
We then formulate some performance measure such as average accuracy on all other local data and use this to compute the weighted average on the coordinator.
