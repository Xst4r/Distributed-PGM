  \section{Introduction}
  With an increasing amount of data gathered on distributed devices, e.g. mobile phones, sending this data to a central processing unit in order to perform machine learning tasks becomes increasingly time-consuming.
  The amount of communication necessary to transfer the data increases as more data is available on these devices.
  Furthermore, due to privacy concerns we may not want to transmit and thus share possibly personal or sensitive data.
  Hence we are looking for alternatives that alleviate these issues.
  One possible solution is performing machine learning tasks "on edge", i.e. at the data source (e.g. mobile phones).
  Distributed machine learning has been successfully applied in a variety of research areas such as Astrophysics \cite{panousopoulou2017distributed}, Medicine \cite{deist2017infrastructure} or Economics \cite{kreitlein2015green}.

  \paragraph{Distributed Learning}

  Transitioning from a central computing device to a set of distributed devices requires careful planning and execution, even moreso when considering mobile devices, as limited processing power and power conservation have to be taken into account.
  Depending on the data distribution type, either horizontally or vertically, we need to approach this task differently. 
  Consider for example a distributed network of sensors.
  Here we would assume the data to be vertically distributed as each sensor gathers data associated with a certain subset of features.
  Instead, for this work we assume the former, which implies that all features are available on each device, with each device only containing a small subset of the data.
  For each device we perform the intended machine learning task with respect to the underlying problem.
  We will then aggregate these models using different mechanisms to create a more accurate and robust local or global model, which performs as well as or as close as possible to the model obtained by a central computing node that has access to the complete data.
  Here local model refers to an individual model for each device.
  We measure model performance by defining suitable statistics such as classification accuracy, robustness, generalisation capability or some error function.
  While one can argue, that with a sufficient amount of data on each device, we are able to create a reasonably well performing model, we usually do not have the required amount of data available to achieve a sufficiently well performing model.
    
  Performing the task on a set of devices allows us to significantly reduce the communication cost as we are not required to transfer data.
  Instead, we are only required to send the model parameters or certain statistics, which are then aggregated to create a composite model.
  Depending on the overall structure we may only be able create these composite models on the local device, i.e. we are limited to aggregating models individually by receiving information from connected (neighbouring) devices.
  For this work we will focus on distributed probabilistic graphical models as our structure of choice for the distributed environment.
  %Processor or increase in processing capability is gained from threading i.e. parallel computing. Processing power of a single core did not improve significantly over the past few years.
  \paragraph{Probabilistic Graphical Models}  


  When estimating parameters of a distribution we may consider each feature a random variable and the data generated a result of a stochastic process based on this random variables.
  Probabilistic Graphical Models (PGM) are a powerful tool to model conditional independencies between random variables. 
  We express each random variable as a node in a graph and each edge between two nodes represents a dependency. 
  With the exception of trees and graphs with bounded treewidth performing exact inference in such a Graph is NP-Hard.
  This is a result from having to compute the marginal distribution, i.e., summing over all possible states of all maximum cliques inside the graph.
  However, methods exist, such as loopy belief propagation to perform approximate inference.
  We may also apply a junction tree algorithm to transform the graph into a tree, where exact inference is possible in polynomial time.
    
  When considering probabilistic graphical models we usually utilize parametric distributions, that are part of the exponential family. 
  Our goal is to estimate the parameters of such a distribution, which optimizes some criterion e.g. Maximum Likelihood or Maximum Entropy.
  

  After estimating the parameters for each distribution and device, we employ an aggregation mechanic to create a composite model.
  Existing aggregation mechanics are discussed in section \ref{sec:ew}.
  Additionally we need to define a stopping criterion where every device will stop sending messages to other devices, which means we require some kind of convergence.
  This is usually the case when most devices have a similar local model.
  In the following we will consider existing work on that specific research area and then lay out the necessary steps and challenges for the thesis.

    \section{Existing Work}
    \label{sec:ew}
    Several research publications have dealt with distributed model aggregation and communication between devices. 
    The topic remains prominent in current literature and many articles expand upon or introduce new techniques that deal with the issues that arise in a distributed environment.
    They attempt to deal with issues such as reducing communication overhead, computing some function value based on distributed computational results or a way to more efficiently create local models based on messages from neighbouring nodes. 

    \subsection{Aggregation Mechanics}

    When locally aggregating models using techniques such as a weighted average or radon points we additionally have to consider the complexity of these techniques and the benefit they provide compared to the increased complexity, which may be introduced to the system.
    Several aggregation mechanics have been proposed to increase local model performance.

    \paragraph{Undirected Integer Exponential Family Models}
    Piatkowski \cite{piatkowskidistributed} considers distributed undirected probabilistic graphical models with integer parameters. 
    Using the average sufficient statistics for each node to estimate the model parameters already yields a promising model, which has a sublinear communication cost with respect to the data and achieves a competitive accuracy compared to the global model.
    %We propose to use this work as a baseline and plan to improve the model by using weighted averages, estimating the weights with an additional maximum likelihood step.
    
    \paragraph{Radon Machines}
    Radon Machines and Radon Points as presented by Kamp et.al \cite{kamp2017effective} are another way to create a single global model aggregated from local models. New hypotheses (models) are generated by computing the radon points of hypotheses' subsets. Where radon-points are obtained by computing the non-empty intersection between two convex hulls spanned by the subsets, where the intersection between bots sets is empty.
    We are guaranteed to obtain a non-empty intersection between the convex hulls, as the number of hypotheses necessary for each set is directly tied to the radon number. 
    The radon number indicates the smallest amount of vectors necessary, such that the intersection between every two equally sized subsets is empty and the intersection of their convex hulls is not.
    
    \paragraph{Wasserstein-Barycenter}
      Using Wasserstein Barycenters, as shown by Dognin et. al. \cite{dognin2019wasserstein} is another possible approach to model ensembling or aggregation. When aggregating the models we additionally include semantic information about for example class topology to create model better suited for the task.
   
    %\subsection{Stopping Criterion}
    %Where are we sending messages ? 
    %What are we sending between the nodes ? 
    %Why and when to we have to stop sending messages ?
    %When computing models on distributed devices we have to define some stopping mechanism.
    %Once this mechanism holds for all devices we have obtained a global solution that all nodes agree on, with respect the the criterion.

    \subsection{Distributed Thresholding} 
    When aggregating models on a local level we have to define some stopping criterion where every node stops sending its messages to its neighbours.
    This is usually done when all nodes or all neighbors agree with a node on the solution.
    This can be efficiently done for network graphs without cycles.
    However, when considering networks containing cycles the local solution on every node may be erroneous.
    Wolff introduced a local thresholding technique  \cite{wolff2013local}, that solves this issue allowing us to use graphs containing cycles.
    Several other methods such as geometric thresholding \cite{sharfman2007geometric} \cite{keren2011shape} for distributed environments have been proposed.


    
   %\subsection{Tools}
   %A variety of tools are available to realize or simulate a distributed environment. These tools allow us to create several instances of the same or even different problems and pass messages between them.
   %This allows us pass data or statistics between models.
   % \paragraph{Open MPI}
   %Open Message Passing Interface (OpenMPI \cite{gabriel04:_open_mpi}) is a tool, that allows us to create such distributed processes and enables us to easily send messages between two or more processes. 

   \section{Challenges}
  Distributed and parallel computing in and of itself are already difficult tasks, that need to be carefully approached.
  Depending on the application, a distributed framework may not lead to the desired results if the communication overhead between devices is greather than the benefit by distributing the tasks.

  Applying probabilistic graphical model theory to a distributed environment and aggregating models poses challenges such as the question for convergence, optimality (either locally or globally) and time-complexity especially compared to a single model.
  When considering general network graphs, i.e. with cycles and possibly large cliques, the task is even more challenging.

  Furthermore, the different techniques presented in section \ref{sec:ew} may not be directly suited for use with probabilistic graphical models.
  Here, it is required to modify these mechanics at be applicable to our proposed structure and model.

  Depending on the use-case communication overhead and power consumption are of great concern. 
  Therefore, creating a framework that reduces these factors as much as possible, while also performing reasonably well, is desired.
  Furthermore, for comparison between different aggregation and thresholding mechanics it will be beneficial to keep the framework as flexible as possible.
  This will allow the effortless comparison between aggregation and thresholding mechanics. 

   
  \section{Thesis Structure}
    We propose the following structure.
    
  \subsection{Introduction and Related Work}
    First we will introduce the problem, its challenges and possible application in real life e.g. medicine, economics or genetics. We will then, based on state-of-the-art methods present related and existing work that may be useful for creating a framework or other methods that may be used in comparison to our method.

  \subsection{Theoretical Foundation}
    We will introduce the general theory of probabilistic graphical models as well as the differences in a distributed system. 
    Furthermore, we will introduce several aggregation mechanisms, which will be compared to each other based on their performance.
    Additionally, the thresholding mechanism necessary for the  stopping criterion will be discussed. 

  \subsection{Model Aggregation methods and algorithms for distributed PGMs}
    Here, we propose an algorithmic framework, which can be applied to distributed systems.
    This includes model parameter estimation on each device as well as communication between devices, aggregation mechanics and the stopping criterion.

  \subsection{Experiments}
  Based on the algorithms introduced we will run experiments on several available data sets using the proposed aggregation and thresholding mechanics.

  \subsection{Results and Conclusion}
  Here, we will evaluate and compare experimental results based on the different aggregation mechanics. 
  We will use the state-of-the-art performance measures to compare the local models.
  Finally, we will present the mechanics best suited for probabilistic graphical models on distributed networks.

  \section{Roadmap}
  \begin{enumerate}
       \item Researching the theoretical approaches of different aggregation mechanics and stopping criteria obtaining an overview of the state-of-the-art techniques used to perform this tasks.
       \item Researching tools and frameworks for message passing interfaces, parallelization and distributed computation such as OpenMPI\cite{gabriel04:_open_mpi} and OpenMP.
       \item Preparation and creation of a framework capable of distributed computation for parametric distributions based on incoming data and communication between devices employ model aggregation mechanisms.
       \item Performing experiments for different aggregation techniques and stopping criteria, which includes data preprocessing, model estimation and aggregation.
       \item Comparing and evaluating experimental results in terms of performance (accuracy generalization) and speed and finding the mechanics best suited for probabilistic graphical models and exponential families.
    \end{enumerate}