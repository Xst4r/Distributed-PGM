   
  \section{Introduction}
  With an increasing amount of data gathered on distributed devices, e.g. mobile phones, sending this data to a central processing unit in order to perform machine learning tasks becomes increasingly time-consuming.
  The amount of communication necessary to transfer the data increases as more data is available on these devices.
  Furthermore, due to privacy concerns we may not want to transmit and thus share possibly personal or sensitive data.
  Hence we are looking for alternatives that alleviate these issues.
  One possible solution is performing machine learning tasks "on edge", i.e. at the data source (e.g. mobile phones).
  Distributed machine learning has been successfully applied to a variety of research areas such as Astrophysics \cite{panousopoulou2017distributed}, Medicine \cite{deist2017infrastructure} or Economics \cite{kreitlein2015green}.

  \paragraph{Distributed Learning}

  Transitioning from a central computing node to a set of distributed distributed nodes requires careful planning and execution, even moreso when considering mobile devices, as limited processing power and power conservation have to be considered.
  Depending on the data distribution type, either horizontally or vertically, we need to approach this task differently. 
  Consider for example a dsitributed network of sensors.
  Here we would assume the data to be vertically distributed as each sensor gathers data associated with a certain subset of features.
  Instead, for this work we assume the former, that is all features associated with the data are available on each device, with each device only containing a small subset of the data.
  For each device we peform the intended machine learning task with respect to the underlying problem.
  We will then aggregate these models using different mechanisms to create a more accurate and robust model, which performs as well as or as close as possible to the model obtained by a central computing node.
  We measure model performance by defining suitable statistics such as accuracy, robustness and generalisation capability.
  While one can argue, that with a sufficient amount of data on each device, we are able to create a reasonably well performing model, we usually do not have the required amount of data available to achieve a sufficiently well performing model.
    
  Performing the task on a set of devices allows us to significantly reduce the communication cost as we are not required to transfer data.
  Instead, we are only required to send the model parameters or certain statistics, which are then aggregated to create a composite model.
  Depending on the model's runtime scaling with respect to the data we may also reduce time-complexity.
  For this work we will focus on distributed probabilistic graphical models as our structure of choice for the distributed environment.
  %Processor or increase in processing capability is gained from threading i.e. parallel computing. Processing power of a single core did not improve significantly over the past few years.
  \paragraph{Probabilistic Graphical Models}  


  When estimating parameters of a distribution we may consider each feature a random variable and the data generated a result of a stochastic process.
  Probabilistic Graphical Models (PGM) are a powerful tool to model conditional independencies between random variables. 
  We usually express each random variable as a node in a graph and each edge between two nodes is a dependency. 
  With the exception of trees and graphs with bounded treewidth performing exact inference in such a Graph is NP-Hard.
  This is a result from having to compute the marginal distribution, i.e., summing over all possible states of all maximum cliques inside the graph.
  However, methods exist, such as loopy belief propagation to perform approximate inference.
  We may also apply a junction tree algorithm to transform the graph into a tree, where exact inference is possible in polynomial time.
    
  When considering probabilistic graphical models we usually utilize parametric distributions, that are part of the exponential family. 
  Our goal is to estimate the parameters of such a distribution, which optimizes some criterion e.g. Maximum Likelihood or Maximum Entropy.
  

  Having obtained the parameters (the hypotheses) for each distribution on each device we employ an aggregation mechanic to create a composite model.
  Existing aggregation techniques are discussed in section \ref{sec:ew}.
  Additionally we need to define a stopping criterion where every device will stop sending messages to other devices, i.e., a convergences criterion.
  This is usually the case when most devices "agree" on their parameters.
  In the following we will consider existing work on that specific research area and then lay out the necessary steps and challenges for the proposed thesis.

    \section{Existing Work}
    \label{sec:ew}
    Several research articles have dealt with distributed model aggregation and communication. 
    The topic remains prominent in current literature and many articles expand upon or introduce new techniques that deal with the issues that arise in a distributed environment.
    Most of these attempt to deal with issues such as reducing communication overhead, computing some global function value based on distributed computational results or a way to more efficiently create a global model from local models.

    \subsection{Aggregation Mechanics}

    Several aggregation mechanics have been proposed to increase the performance of the aggregated model. 

    \paragraph{Undirected Integer Exponential Family Models}
    Piatkowski \cite{piatkowskidistributed} considers distributed probabilistic graphical models with integer parameters. 
    Using a simple averaging method computed over all distributed models already yields a promising model, which has a sublinear communication cost with respect to teh data and achieves a competetive accuracy compared to the global model.
    We propose to use this work as a baseline and plan to improve the model by using weighted averages, estimating the weights with an additional maximum likelihood step.
    
    \paragraph{Radon Machines}
    Radon Machines and Radon Points as presented by Kamp et.al \cite{kamp2017effective} are another way to create a single global model aggregated from local models. New hypotheses (models) are generated by computing the radon points of hypotheses' subsets. Where radon-points are obtained by computing the non-empty intersection between two convex hulls spanned by the subsets, where the intersection between bots sets is empty.
    We are guaranteed to obtain a non-empty intesection between the convex hulls, as the number of hypotheses necessary for each set is directly tied to the radon number. 
    The radon number indicates the smallest amount of vectors necessary, such that the intersection between every two equally sized subsets is empty and the intersection of their convex hulls is not.
    
    \paragraph{Wasserstein-Barycenter}
      Using Wasserstein Barycenters, as shown by Dognin et. al. \cite{dognin2019wasserstein} is another possible approch to model ensembling or aggregation. When aggregating the models we additionally include semantic information about for exmaple class topology to create model better suited for the task.
   
    %\subsection{Stopping Criterion}
    %Where are we sending messages ? 
    %What are we sending between the nodes ? 
    %Why and when to we have to stop sending messages ?
    %When computing models on distributed devices we have to define some stopping mechanism.
    %Once this mechanism holds for all devices we have obtained a global solution that all nodes agree on, with respect the the criterion.

    %\paragraph{Distributed Thresholding} 
    %Distributed Thresholding featured by Wolff \cite{wolff2013local} allows us to compute thresholds for a global thresholding functions by using local function values only. Propagating the average between nodes we are able to reliably compute the global average, without being too heavy on communication.
    %This allows to define a stopping criterion, which holds when the local solution does not deviate too far from the current global solution. 
    %As long as this holds the device does not send additional messages.
    %Devices that do not fulfill the criterion keep sending messages until every device, that is every node agrees on the solution to a certain extent.

    
   %\subsection{Tools}
   %A variety of tools are available to realize or simulate a distributed environment. These tools allow us to create several instances of the same or even different problems and pass messages between them.
   %This allows us pass data or statistics between models.
   % \paragraph{Open MPI}
   %Open Message Passing Interface (OpenMPI \cite{gabriel04:_open_mpi}) is a tool, that allows us to create such distributed processes and enables us to easily send messages between two or more processes. 

    \section{Challenges}
   Researching different aggregation mechanics involves research of different methods and incorporating them in a framework suitable for distributed learning.

   When researching these techniques we have to find the approaches presented in publications, that are ... 

   Creating a suitable framework ....

   
    \section{Thesis Structure}
    We propose the following thesis structure.
    
    \subsection{Introduction and Related Work}
    First we will introduce the problem, its challenges and possible application in real life e.g. industry. We will then based on state-of-the-art methods present related and existing work that may be useful for creating a framework or other methods that may be used to compare it to our method.

    \subsection{Theoretical Baseline}
    We will introduce the general theory of probabilistic graphical models as well as differences when using a distributed system. Furthermore we will introduce sevral aggregation mechanisms, which will be used to create the composite model.
    Furhtermore we introduce stopping criteria as presented in section \ref{sec:ew}.

    \subsection{Model Aggregation methods and algorithm for distributed PGMs}
    Here we will introduce algorithms for distributed learning, aggregation techniques and stopping criteria.

    \subsection{Experiments}
    Experiments, where we estimate the parameters of the exponential family distributions and employ the model aggregation mechanics to create a global model. 
    \subsection{Results and Conclusion}
    Evaluation of experimental results 
    \section{Roadmap}
    \begin{enumerate}
       \item Researching the theoretical approaches of different aggregation techniques and stopping criteria obtaining an overview of the state-of-the-art techniques used to perform this tasks.
       \item Reading up on modules and framworks for message passing interfaces, parallelisation and distributed computation such as OpenMPI, OpenMP and CUDA.
       \item Preparation of a framework capable of distributed computation of parametric distributions based on incoming data and communication between devices (threads) to employ model aggregation mechanisms.
       \item Experiments for different aggregation techniques and stopping criteria, which includes data preprocessing, model estimation and aggregation.
       \item Comparing and evaluating experimental results in terms of performance (accuracy generalisation) and speed.
    \end{enumerate}