  \section{Introduction}
  With an increasing amount of data gathered on distributed devices, e.g. mobile phones, sending this data to a central processing unit in order to perform machine learning tasks becomes increasingly time-consuming.
  Furthermore, due to privacy concerns we may not want to transmit and thus share possibly personal or sensitive data.
  Hence we are looking for alternatives that alleviate these issues.
  One possible solution is performing machine learning tasks "on edge", i.e. at the data source (e.g. mobile phones).
  Distributed machine learning has been successfully applied in a variety of research areas such as Astrophysics \cite{panousopoulou2017distributed}, Medicine \cite{deist2017infrastructure} or Economics \cite{kreitlein2015green}.

  \paragraph{Distributed Learning}

  Transitioning from a central computing device to a set of distributed devices requires careful planning and execution.
  We assume all devices holding a small subset of the data, with each device knowing all features.
  For each device we perform the intended machine learning task with respect to the underlying problem.
  We will then aggregate these models using different mechanisms to create a more accurate and robust local or global model, which performs as well as or as close as possible to the model obtained by a central computing node that has access to the complete data.
  Here local model refers to an individual model for each device.
  We measure model performance by defining suitable statistics such as classification accuracy, robustness, generalisation capability or some error function.

  Performing the task on a set of devices allows us to significantly reduce the communication cost as we are not required to transfer data.
  Instead, we are only required to send the model parameters or certain statistics, which are then aggregated to create a composite model.
  For this work we will focus on distributed probabilistic graphical models as our structure of choice for the distributed environment.
  %Processor or increase in processing capability is gained from threading i.e. parallel computing. Processing power of a single core did not improve significantly over the past few years.
  \paragraph{Probabilistic Graphical Models}  


  When estimating parameters of a distribution we may consider each feature a random variable and the data generated a result of a stochastic process based on this random variables.
  Probabilistic Graphical Models (PGM) are a powerful tool to model conditional independencies between random variables. 
  We express each random variable as a node in a graph and each edge between two nodes represents a dependency. 
  With the exception of trees and graphs with bounded treewidth performing exact inference in such a Graph is NP-Hard.
  This is a result from having to compute the marginal distribution, i.e., summing over all possible states of all maximum cliques inside the graph.
  However, methods exist, such as loopy belief propagation to perform approximate inference.
  We may also apply a junction tree algorithm to transform the graph into a tree, where exact inference is possible in polynomial time.
    
  When considering probabilistic graphical models we usually utilize parametric distributions, that are part of the exponential family. 
  Our goal is to estimate the parameters of such a distribution, which optimizes some criterion e.g. Maximum Likelihood or Maximum Entropy.
  

  After estimating the parameters for each distribution and device, we employ an aggregation mechanic to create a composite model.
  Distributed integer probabilistic graphical models \cite{piatkowskidistributed} have recently been considered by Piatkowski.
  In this specific scenario the model was aggregated using the average over all local models.
  Other existing aggregation mechanics are discussed in the next section.

    \section{Existing Work}
    \label{sec:ew}
    Several research publications feature  distributed model aggregation.
    The topic remains prominent in current literature and many articles expand upon or introduce new techniques that deal with the issues that arise in a distributed environment.
    They attempt to deal with issues such as reducing communication overhead, computing some function value based on distributed computational results or a way to more efficiently create local models based on messages from neighbouring devices. 

    \subsection{Model Averaging and Bayesian Models}
    When considering local models we may obtain an aggregate by simply averaging over all models. We then obtain a model where each local model contributes with equal weight (one divided by the number of models) to the aggregate. 
    Instead of having equal weights we may consider the weights for the (weighted) average unknown. This introduced an intermediate step where we are required to estimate the weights for each local model first.
    Generally for Bayesian Models, various aggregation mechanics have been explored and introduced.
    Given the model priors as described in \cite{hoeting1999bayesian} we may sum over all viable models to create an aggregate model.
    %We propose to use this work as a baseline and plan to improve the model by using weighted averages, estimating the weights with an additional maximum likelihood step.
    
    \subsection{Radon Machines}
    Radon Machines and Radon Points as presented by Kamp et.al \cite{kamp2017effective} are another way to create a single global model aggregated from local models. New hypotheses (models) are generated by computing the radon points of hypotheses' subsets. Where radon-points are obtained by computing the non-empty intersection between two convex hulls spanned by the subsets, where the intersection between bots sets is empty.
    We are guaranteed to obtain a non-empty intersection between the convex hulls, as the number of hypotheses necessary for each set is directly tied to the radon number. 
    The radon number indicates the smallest amount of vectors necessary, such that the intersection between every two equally sized subsets is empty and the intersection of their convex hulls is not.
    
    \subsection{Wasserstein-Barycenter}
      Using Wasserstein Barycenters, as shown by Dognin et. al. \cite{dognin2019wasserstein} is another possible approach to model ensembling or aggregation. When aggregating the models we additionally include semantic information about for example class topology to create model better suited for the task.
   
    %\subsection{Stopping Criterion}
    %Where are we sending messages ? 
    %What are we sending between the nodes ? 
    %Why and when to we have to stop sending messages ?
    %When computing models on distributed devices we have to define some stopping mechanism.
    %Once this mechanism holds for all devices we have obtained a global solution that all nodes agree on, with respect the the criterion.

    \subsection{Distributed Thresholding} 
    When aggregating models on a local level we have to define some stopping criterion where every node stops sending its messages to its neighbours.
    This is usually done when all nodes or all neighbors agree with a node on the solution.
    This can be efficiently done for network graphs without cycles.
    However, when considering networks containing cycles the local solution on every node may be erroneous.
    Wolff introduced a local thresholding technique  \cite{wolff2013local}, that solves this issue allowing us to use graphs containing cycles.
    Several other methods such as geometric thresholding \cite{sharfman2007geometric} \cite{keren2011shape} for distributed environments have been proposed.


    
   %\subsection{Tools}
   %A variety of tools are available to realize or simulate a distributed environment. These tools allow us to create several instances of the same or even different problems and pass messages between them.
   %This allows us pass data or statistics between models.
   % \paragraph{Open MPI}
   %Open Message Passing Interface (OpenMPI \cite{gabriel04:_open_mpi}) is a tool, that allows us to create such distributed processes and enables us to easily send messages between two or more processes. 

   \section{Challenges}
  Distributed and parallel computing in and of itself are already difficult tasks, that need to be carefully approached.
  Depending on the application, a distributed framework may not lead to the desired results if the communication overhead between devices is greather than the benefit by distributing the tasks.

  Applying probabilistic graphical model theory to a distributed environment and aggregating models poses challenges such as the question for convergence, optimality (either locally or globally) and time-complexity especially compared to a single model.
  When considering general network graphs, i.e. with cycles and possibly large cliques, the task is even more challenging.

  Depending on the use-case communication overhead and power consumption are of great concern. 
  Therefore, creating a framework that reduces these factors as much as possible, while also performing reasonably well, is desired.
  Furthermore, for comparison between different aggregation and thresholding mechanics it will be beneficial to keep the framework as flexible as possible.
  This will allow the effortless comparison between aggregation and thresholding mechanics. 

   
  \section{Thesis Structure}
    We propose the following structure.
    
  \subsection{Introduction and Related Work}
    First we will introduce the problem, its challenges and possible application in real life e.g. medicine, economics or genetics. We will then, based on state-of-the-art methods present related and existing work that may be useful for creating a framework or other methods that may be used in comparison to our method.

  \subsection{Theoretical Foundation}
    We will introduce the general theory of probabilistic graphical models as well as the differences in a distributed system. 
    Furthermore, we will introduce several aggregation mechanisms, which will be compared to each other based on their performance.
    Additionally, the thresholding mechanism necessary for the  stopping criterion will be discussed. 

  \subsection{Model Aggregation methods and algorithms for distributed PGMs}
    Here, we propose an algorithmic framework, which can be applied to distributed systems.
    This includes model parameter estimation on each device as well as communication between devices, aggregation mechanics and the stopping criterion.

  \subsection{Experiments}
  Based on the algorithms introduced we will run experiments on several available data sets using the proposed aggregation and thresholding mechanics.
  We will compare the mechanics by evaluating the amount of communication required to create the aggregate. Additionally we will investigate model performance, e.g. classification accuracy, compared to the global model or other techniques such as model ensembling.

  \subsection{Results and Conclusion}
  Here, we will evaluate and compare experimental results based on the different aggregation mechanics. 
  We will use the state-of-the-art performance measures to compare the local models.
  Finally, we will present the mechanics best suited for probabilistic graphical models on a network of distributed devices.

  \section{Roadmap}
  \begin{enumerate}
       \item Researching the theoretical approaches of different aggregation mechanics and stopping criteria obtaining an overview of the state-of-the-art techniques used to perform this tasks.
       \item Researching tools and frameworks for message passing interfaces, parallelization and distributed computation such as OpenMPI\cite{gabriel04:_open_mpi} and OpenMP.
       \item Preparation and creation of a framework capable of distributed computation for parametric distributions based on incoming data and communication between devices employ model aggregation mechanisms.
       \item Performing experiments for different aggregation techniques and stopping criteria, which includes data preprocessing, model estimation and aggregation.
       \item Comparing and evaluating experimental results in terms of performance (accuracy generalization) and speed and finding the mechanics best suited for probabilistic graphical models and exponential families.
    \end{enumerate}